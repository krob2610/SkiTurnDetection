{
  "checkpoints": [
    "{\n  \"stub\": false,\n  \"trainable_name\": \"lambda\",\n  \"trial_id\": \"dc3ad0dc\",\n  \"sync_config\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059593000000000000008c0f7261792e74756e652e73796e636572948c0a53796e63436f6e6669679493942981947d94288c0a75706c6f61645f646972944e8c0673796e636572948c046175746f948c0b73796e635f706572696f64944d2c018c0c73796e635f74696d656f7574944d08078c0e73796e635f61727469666163747394888c1273796e635f6f6e5f636865636b706f696e74948875622e\"\n  },\n  \"_orig_experiment_path\": \"/Users/kuba/ijcai_ski/ray_results/test\",\n  \"_orig_experiment_dir_name\": \"test\",\n  \"_local_experiment_path\": \"/Users/kuba/ijcai_ski/ray_results/test\",\n  \"_remote_experiment_path\": null,\n  \"config\": {\n    \"Gradient_LR\": 0.051829973183880884,\n    \"Gradient_Momentum\": 0.9092124464828797,\n    \"Gradient_Steps\": 420,\n    \"Threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308060000000000000094869452942e\"\n    },\n    \"merge_threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308130000000000000094869452942e\"\n    },\n    \"number_of_points\": 1000,\n    \"outliers_multiplier\": 2.3375674387733074\n  },\n  \"_Trial__unresolved_config\": {\n    \"Gradient_LR\": 0.051829973183880884,\n    \"Gradient_Momentum\": 0.9092124464828797,\n    \"Gradient_Steps\": 420,\n    \"Threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308060000000000000094869452942e\"\n    },\n    \"merge_threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308130000000000000094869452942e\"\n    },\n    \"number_of_points\": 1000,\n    \"outliers_multiplier\": 2.3375674387733074\n  },\n  \"evaluated_params\": {\n    \"Gradient_LR\": 0.051829973183880884,\n    \"Gradient_Momentum\": 0.9092124464828797,\n    \"Gradient_Steps\": 420,\n    \"Threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308060000000000000094869452942e\"\n    },\n    \"merge_threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308130000000000000094869452942e\"\n    },\n    \"number_of_points\": 1000,\n    \"outliers_multiplier\": 2.3375674387733074\n  },\n  \"experiment_tag\": \"1_Gradient_LR=0.0518,Gradient_Momentum=0.9092,Gradient_Steps=420,Threshold=6,merge_threshold=19,number_of_points=1000,outliers_multiplier=2.3376\",\n  \"location\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059546000000000000008c197261792e74756e652e6578706572696d656e742e747269616c948c095f4c6f636174696f6e9493942981947d94288c08686f73746e616d65944e8c03706964944e75622e\"\n  },\n  \"stopping_criterion\": {},\n  \"_setup_default_resource\": true,\n  \"_default_placement_group_factory\": \"800595aa000000000000008c237261792e74756e652e657865637574696f6e2e706c6163656d656e745f67726f757073948c15506c6163656d656e7447726f7570466163746f72799493942981947d94288c085f62756e646c6573945d947d948c0343505594473ff000000000000073618c155f686561645f62756e646c655f69735f656d70747994898c095f7374726174656779948c045041434b948c055f6172677394298c075f6b7761726773947d9475622e\",\n  \"placement_group_factory\": \"800595aa000000000000008c237261792e74756e652e657865637574696f6e2e706c6163656d656e745f67726f757073948c15506c6163656d656e7447726f7570466163746f72799493942981947d94288c085f62756e646c6573945d947d948c0343505594473ff000000000000073618c155f686561645f62756e646c655f69735f656d70747994898c095f7374726174656779948c045041434b948c055f6172677394298c075f6b7761726773947d9475622e\",\n  \"log_to_file\": [\n    null,\n    null\n  ],\n  \"max_failures\": 0,\n  \"_last_result\": {\n    \"score\": 0.8610290823778323,\n    \"time_this_iter_s\": 10.911266088485718,\n    \"done\": true,\n    \"training_iteration\": 1,\n    \"trial_id\": \"dc3ad0dc\",\n    \"date\": \"2025-01-23_02-39-37\",\n    \"timestamp\": 1737596377,\n    \"time_total_s\": 10.911266088485718,\n    \"pid\": 30487,\n    \"hostname\": \"MacBook-Pro.ds19.agh.edu.pl\",\n    \"node_ip\": \"127.0.0.1\",\n    \"config\": {\n      \"Gradient_LR\": 0.051829973183880884,\n      \"Gradient_Momentum\": 0.9092124464828797,\n      \"Gradient_Steps\": 420,\n      \"Threshold\": 6,\n      \"merge_threshold\": 19,\n      \"number_of_points\": 1000,\n      \"outliers_multiplier\": 2.3375674387733074\n    },\n    \"time_since_restore\": 10.911266088485718,\n    \"iterations_since_restore\": 1,\n    \"experiment_tag\": \"1_Gradient_LR=0.0518,Gradient_Momentum=0.9092,Gradient_Steps=420,Threshold=6,merge_threshold=19,number_of_points=1000,outliers_multiplier=2.3376\"\n  },\n  \"_default_result_or_future\": null,\n  \"last_update_time\": 1737596377.243582,\n  \"metric_analysis\": {\n    \"score\": {\n      \"max\": 0.8610290823778323,\n      \"min\": 0.8610290823778323,\n      \"avg\": 0.8610290823778323,\n      \"last\": 0.8610290823778323,\n      \"last-5-avg\": 0.8610290823778323,\n      \"last-10-avg\": 0.8610290823778323\n    },\n    \"time_this_iter_s\": {\n      \"max\": 10.911266088485718,\n      \"min\": 10.911266088485718,\n      \"avg\": 10.911266088485718,\n      \"last\": 10.911266088485718,\n      \"last-5-avg\": 10.911266088485718,\n      \"last-10-avg\": 10.911266088485718\n    },\n    \"done\": {\n      \"max\": false,\n      \"min\": false,\n      \"avg\": false,\n      \"last\": false,\n      \"last-5-avg\": false,\n      \"last-10-avg\": false\n    },\n    \"training_iteration\": {\n      \"max\": 1,\n      \"min\": 1,\n      \"avg\": 1,\n      \"last\": 1,\n      \"last-5-avg\": 1,\n      \"last-10-avg\": 1\n    },\n    \"time_total_s\": {\n      \"max\": 10.911266088485718,\n      \"min\": 10.911266088485718,\n      \"avg\": 10.911266088485718,\n      \"last\": 10.911266088485718,\n      \"last-5-avg\": 10.911266088485718,\n      \"last-10-avg\": 10.911266088485718\n    },\n    \"time_since_restore\": {\n      \"max\": 10.911266088485718,\n      \"min\": 10.911266088485718,\n      \"avg\": 10.911266088485718,\n      \"last\": 10.911266088485718,\n      \"last-5-avg\": 10.911266088485718,\n      \"last-10-avg\": 10.911266088485718\n    },\n    \"iterations_since_restore\": {\n      \"max\": 1,\n      \"min\": 1,\n      \"avg\": 1,\n      \"last\": 1,\n      \"last-5-avg\": 1,\n      \"last-10-avg\": 1\n    }\n  },\n  \"n_steps\": [\n    5,\n    10\n  ],\n  \"metric_n_steps\": {\n    \"score\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059589000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452948c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02663894898887945294284b038c013c944e4e4e4affffffff4affffffff4b0074946243083bf7b6dc8c8deb3f9486945294612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059589000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452948c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02663894898887945294284b038c013c944e4e4e4affffffff4affffffff4b0074946243083bf7b6dc8c8deb3f9486945294612e\"\n      }\n    },\n    \"time_this_iter_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294474025d29178000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294474025d29178000000612e\"\n      }\n    },\n    \"done\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059522000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529489612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059522000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529489612e\"\n      }\n    },\n    \"training_iteration\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b01612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b01612e\"\n      }\n    },\n    \"time_total_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294474025d29178000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294474025d29178000000612e\"\n      }\n    },\n    \"time_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294474025d29178000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294474025d29178000000612e\"\n      }\n    },\n    \"iterations_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b01612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b01612e\"\n      }\n    }\n  },\n  \"export_formats\": [],\n  \"status\": \"TERMINATED\",\n  \"start_time\": 1737596366.319438,\n  \"relative_logdir\": \"lambda_dc3ad0dc_1_Gradient_LR=0.0518,Gradient_Momentum=0.9092,Gradient_Steps=420,Threshold=6,merge_threshold=19,number_of_points=1_2025-01-23_02-39-23\",\n  \"runner\": null,\n  \"last_debug\": 0,\n  \"error_filename\": null,\n  \"pickled_error_filename\": null,\n  \"trial_name_creator\": null,\n  \"trial_dirname_creator\": null,\n  \"custom_trial_name\": null,\n  \"custom_dirname\": null,\n  \"experiment_dir_name\": \"test\",\n  \"saving_to\": null,\n  \"checkpoint_config\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"800595ff000000000000008c0e7261792e6169722e636f6e666967948c10436865636b706f696e74436f6e6669679493942981947d94288c0b6e756d5f746f5f6b656570944b038c1a636865636b706f696e745f73636f72655f617474726962757465948c12747261696e696e675f697465726174696f6e948c16636865636b706f696e745f73636f72655f6f72646572948c036d6178948c14636865636b706f696e745f6672657175656e6379944b008c11636865636b706f696e745f61745f656e64944e8c1a5f636865636b706f696e745f6b6565705f616c6c5f72616e6b7394898c1f5f636865636b706f696e745f75706c6f61645f66726f6d5f776f726b657273948975622e\"\n  },\n  \"checkpoint_manager\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"800595d2020000000000008c257261792e74756e652e657865637574696f6e2e636865636b706f696e745f6d616e61676572948c125f436865636b706f696e744d616e616765729493942981947d94288c145f636865636b706f696e745f7374726174656779948c0e7261792e6169722e636f6e666967948c10436865636b706f696e74436f6e6669679493942981947d94288c0b6e756d5f746f5f6b656570944b038c1a636865636b706f696e745f73636f72655f617474726962757465948c12747261696e696e675f697465726174696f6e948c16636865636b706f696e745f73636f72655f6f72646572948c036d6178948c14636865636b706f696e745f6672657175656e6379944b008c11636865636b706f696e745f61745f656e64944e8c1a5f636865636b706f696e745f6b6565705f616c6c5f72616e6b7394898c1f5f636865636b706f696e745f75706c6f61645f66726f6d5f776f726b657273948975628c155f6c61746573745f636865636b706f696e745f6964944b008c1a5f746f705f7065727369737465645f636865636b706f696e7473945d948c1a5f626573745f7065727369737465645f636865636b706f696e74944e8c1c5f6c61746573745f7065727369737465645f636865636b706f696e74944e8c195f6c61746573745f6d656d6f72795f636865636b706f696e74948c247261792e6169722e5f696e7465726e616c2e636865636b706f696e745f6d616e61676572948c125f547261636b6564436865636b706f696e749493942981947d94288c0b6469725f6f725f64617461944e8c026964944affffffff8c0c73746f726167655f6d6f646594681a8c11436865636b706f696e7453746f726167659493944b01859452948c0472616e6b944b008c076d657472696373947d948c076e6f64655f6970944e8c155f6d6f76655f696e73746561645f6f665f636f7079948875628c185f636865636b706f696e74735f746f5f636c65616e5f7570948f9475622e\"\n  },\n  \"restore_path\": null,\n  \"restoring_from\": null,\n  \"num_failures\": 0,\n  \"num_restore_failures\": 0,\n  \"results\": \"80054e2e\",\n  \"best_result\": \"80054e2e\",\n  \"param_config\": \"80054e2e\",\n  \"extra_arg\": \"80054e2e\",\n  \"_state_json\": null,\n  \"_state_valid\": false,\n  \"_resources\": \"80054e2e\"\n}",
    "{\n  \"stub\": false,\n  \"trainable_name\": \"lambda\",\n  \"trial_id\": \"d653496c\",\n  \"sync_config\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059593000000000000008c0f7261792e74756e652e73796e636572948c0a53796e63436f6e6669679493942981947d94288c0a75706c6f61645f646972944e8c0673796e636572948c046175746f948c0b73796e635f706572696f64944d2c018c0c73796e635f74696d656f7574944d08078c0e73796e635f61727469666163747394888c1273796e635f6f6e5f636865636b706f696e74948875622e\"\n  },\n  \"_orig_experiment_path\": \"/Users/kuba/ijcai_ski/ray_results/test\",\n  \"_orig_experiment_dir_name\": \"test\",\n  \"_local_experiment_path\": \"/Users/kuba/ijcai_ski/ray_results/test\",\n  \"_remote_experiment_path\": null,\n  \"config\": {\n    \"Gradient_LR\": 0.01490695153440607,\n    \"Gradient_Momentum\": 0.9168974430285683,\n    \"Gradient_Steps\": 200,\n    \"Threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b0074946243080e0000000000000094869452942e\"\n    },\n    \"merge_threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b0074946243080b0000000000000094869452942e\"\n    },\n    \"number_of_points\": 500,\n    \"outliers_multiplier\": 3.431298649889307\n  },\n  \"_Trial__unresolved_config\": {\n    \"Gradient_LR\": 0.01490695153440607,\n    \"Gradient_Momentum\": 0.9168974430285683,\n    \"Gradient_Steps\": 200,\n    \"Threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b0074946243080e0000000000000094869452942e\"\n    },\n    \"merge_threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b0074946243080b0000000000000094869452942e\"\n    },\n    \"number_of_points\": 500,\n    \"outliers_multiplier\": 3.431298649889307\n  },\n  \"evaluated_params\": {\n    \"Gradient_LR\": 0.01490695153440607,\n    \"Gradient_Momentum\": 0.9168974430285683,\n    \"Gradient_Steps\": 200,\n    \"Threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b0074946243080e0000000000000094869452942e\"\n    },\n    \"merge_threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b0074946243080b0000000000000094869452942e\"\n    },\n    \"number_of_points\": 500,\n    \"outliers_multiplier\": 3.431298649889307\n  },\n  \"experiment_tag\": \"6_Gradient_LR=0.0149,Gradient_Momentum=0.9169,Gradient_Steps=200,Threshold=14,merge_threshold=11,number_of_points=500,outliers_multiplier=3.4313\",\n  \"location\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059546000000000000008c197261792e74756e652e6578706572696d656e742e747269616c948c095f4c6f636174696f6e9493942981947d94288c08686f73746e616d65944e8c03706964944e75622e\"\n  },\n  \"stopping_criterion\": {},\n  \"_setup_default_resource\": true,\n  \"_default_placement_group_factory\": \"800595aa000000000000008c237261792e74756e652e657865637574696f6e2e706c6163656d656e745f67726f757073948c15506c6163656d656e7447726f7570466163746f72799493942981947d94288c085f62756e646c6573945d947d948c0343505594473ff000000000000073618c155f686561645f62756e646c655f69735f656d70747994898c095f7374726174656779948c045041434b948c055f6172677394298c075f6b7761726773947d9475622e\",\n  \"placement_group_factory\": \"800595aa000000000000008c237261792e74756e652e657865637574696f6e2e706c6163656d656e745f67726f757073948c15506c6163656d656e7447726f7570466163746f72799493942981947d94288c085f62756e646c6573945d947d948c0343505594473ff000000000000073618c155f686561645f62756e646c655f69735f656d70747994898c095f7374726174656779948c045041434b948c055f6172677394298c075f6b7761726773947d9475622e\",\n  \"log_to_file\": [\n    null,\n    null\n  ],\n  \"max_failures\": 0,\n  \"_last_result\": {\n    \"score\": 0.8523570370219542,\n    \"time_this_iter_s\": 11.10339093208313,\n    \"done\": true,\n    \"training_iteration\": 1,\n    \"trial_id\": \"d653496c\",\n    \"date\": \"2025-01-23_02-39-46\",\n    \"timestamp\": 1737596386,\n    \"time_total_s\": 11.10339093208313,\n    \"pid\": 30619,\n    \"hostname\": \"MacBook-Pro.ds19.agh.edu.pl\",\n    \"node_ip\": \"127.0.0.1\",\n    \"config\": {\n      \"Gradient_LR\": 0.01490695153440607,\n      \"Gradient_Momentum\": 0.9168974430285683,\n      \"Gradient_Steps\": 200,\n      \"Threshold\": 14,\n      \"merge_threshold\": 11,\n      \"number_of_points\": 500,\n      \"outliers_multiplier\": 3.431298649889307\n    },\n    \"time_since_restore\": 11.10339093208313,\n    \"iterations_since_restore\": 1,\n    \"experiment_tag\": \"6_Gradient_LR=0.0149,Gradient_Momentum=0.9169,Gradient_Steps=200,Threshold=14,merge_threshold=11,number_of_points=500,outliers_multiplier=3.4313\"\n  },\n  \"_default_result_or_future\": null,\n  \"last_update_time\": 1737596386.958126,\n  \"metric_analysis\": {\n    \"score\": {\n      \"max\": 0.8523570370219542,\n      \"min\": 0.8523570370219542,\n      \"avg\": 0.8523570370219542,\n      \"last\": 0.8523570370219542,\n      \"last-5-avg\": 0.8523570370219542,\n      \"last-10-avg\": 0.8523570370219542\n    },\n    \"time_this_iter_s\": {\n      \"max\": 11.10339093208313,\n      \"min\": 11.10339093208313,\n      \"avg\": 11.10339093208313,\n      \"last\": 11.10339093208313,\n      \"last-5-avg\": 11.10339093208313,\n      \"last-10-avg\": 11.10339093208313\n    },\n    \"done\": {\n      \"max\": false,\n      \"min\": false,\n      \"avg\": false,\n      \"last\": false,\n      \"last-5-avg\": false,\n      \"last-10-avg\": false\n    },\n    \"training_iteration\": {\n      \"max\": 1,\n      \"min\": 1,\n      \"avg\": 1,\n      \"last\": 1,\n      \"last-5-avg\": 1,\n      \"last-10-avg\": 1\n    },\n    \"time_total_s\": {\n      \"max\": 11.10339093208313,\n      \"min\": 11.10339093208313,\n      \"avg\": 11.10339093208313,\n      \"last\": 11.10339093208313,\n      \"last-5-avg\": 11.10339093208313,\n      \"last-10-avg\": 11.10339093208313\n    },\n    \"time_since_restore\": {\n      \"max\": 11.10339093208313,\n      \"min\": 11.10339093208313,\n      \"avg\": 11.10339093208313,\n      \"last\": 11.10339093208313,\n      \"last-5-avg\": 11.10339093208313,\n      \"last-10-avg\": 11.10339093208313\n    },\n    \"iterations_since_restore\": {\n      \"max\": 1,\n      \"min\": 1,\n      \"avg\": 1,\n      \"last\": 1,\n      \"last-5-avg\": 1,\n      \"last-10-avg\": 1\n    }\n  },\n  \"n_steps\": [\n    5,\n    10\n  ],\n  \"metric_n_steps\": {\n    \"score\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059589000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452948c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02663894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308cacad0438246eb3f9486945294612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059589000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452948c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02663894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308cacad0438246eb3f9486945294612e\"\n      }\n    },\n    \"time_this_iter_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529447402634efa8000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529447402634efa8000000612e\"\n      }\n    },\n    \"done\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059522000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529489612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059522000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529489612e\"\n      }\n    },\n    \"training_iteration\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b01612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b01612e\"\n      }\n    },\n    \"time_total_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529447402634efa8000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529447402634efa8000000612e\"\n      }\n    },\n    \"time_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529447402634efa8000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529447402634efa8000000612e\"\n      }\n    },\n    \"iterations_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b01612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b01612e\"\n      }\n    }\n  },\n  \"export_formats\": [],\n  \"status\": \"TERMINATED\",\n  \"start_time\": 1737596375.845751,\n  \"relative_logdir\": \"lambda_d653496c_6_Gradient_LR=0.0149,Gradient_Momentum=0.9169,Gradient_Steps=200,Threshold=14,merge_threshold=11,number_of_points=_2025-01-23_02-39-33\",\n  \"runner\": null,\n  \"last_debug\": 0,\n  \"error_filename\": null,\n  \"pickled_error_filename\": null,\n  \"trial_name_creator\": null,\n  \"trial_dirname_creator\": null,\n  \"custom_trial_name\": null,\n  \"custom_dirname\": null,\n  \"experiment_dir_name\": \"test\",\n  \"saving_to\": null,\n  \"checkpoint_config\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"800595ff000000000000008c0e7261792e6169722e636f6e666967948c10436865636b706f696e74436f6e6669679493942981947d94288c0b6e756d5f746f5f6b656570944b038c1a636865636b706f696e745f73636f72655f617474726962757465948c12747261696e696e675f697465726174696f6e948c16636865636b706f696e745f73636f72655f6f72646572948c036d6178948c14636865636b706f696e745f6672657175656e6379944b008c11636865636b706f696e745f61745f656e64944e8c1a5f636865636b706f696e745f6b6565705f616c6c5f72616e6b7394898c1f5f636865636b706f696e745f75706c6f61645f66726f6d5f776f726b657273948975622e\"\n  },\n  \"checkpoint_manager\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"800595d2020000000000008c257261792e74756e652e657865637574696f6e2e636865636b706f696e745f6d616e61676572948c125f436865636b706f696e744d616e616765729493942981947d94288c145f636865636b706f696e745f7374726174656779948c0e7261792e6169722e636f6e666967948c10436865636b706f696e74436f6e6669679493942981947d94288c0b6e756d5f746f5f6b656570944b038c1a636865636b706f696e745f73636f72655f617474726962757465948c12747261696e696e675f697465726174696f6e948c16636865636b706f696e745f73636f72655f6f72646572948c036d6178948c14636865636b706f696e745f6672657175656e6379944b008c11636865636b706f696e745f61745f656e64944e8c1a5f636865636b706f696e745f6b6565705f616c6c5f72616e6b7394898c1f5f636865636b706f696e745f75706c6f61645f66726f6d5f776f726b657273948975628c155f6c61746573745f636865636b706f696e745f6964944b008c1a5f746f705f7065727369737465645f636865636b706f696e7473945d948c1a5f626573745f7065727369737465645f636865636b706f696e74944e8c1c5f6c61746573745f7065727369737465645f636865636b706f696e74944e8c195f6c61746573745f6d656d6f72795f636865636b706f696e74948c247261792e6169722e5f696e7465726e616c2e636865636b706f696e745f6d616e61676572948c125f547261636b6564436865636b706f696e749493942981947d94288c0b6469725f6f725f64617461944e8c026964944affffffff8c0c73746f726167655f6d6f646594681a8c11436865636b706f696e7453746f726167659493944b01859452948c0472616e6b944b008c076d657472696373947d948c076e6f64655f6970944e8c155f6d6f76655f696e73746561645f6f665f636f7079948875628c185f636865636b706f696e74735f746f5f636c65616e5f7570948f9475622e\"\n  },\n  \"restore_path\": null,\n  \"restoring_from\": null,\n  \"num_failures\": 0,\n  \"num_restore_failures\": 0,\n  \"results\": \"80054e2e\",\n  \"best_result\": \"80054e2e\",\n  \"param_config\": \"80054e2e\",\n  \"extra_arg\": \"80054e2e\",\n  \"_state_json\": null,\n  \"_state_valid\": false,\n  \"_resources\": \"80054e2e\"\n}",
    "{\n  \"stub\": false,\n  \"trainable_name\": \"lambda\",\n  \"trial_id\": \"6bdfc7c7\",\n  \"sync_config\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059593000000000000008c0f7261792e74756e652e73796e636572948c0a53796e63436f6e6669679493942981947d94288c0a75706c6f61645f646972944e8c0673796e636572948c046175746f948c0b73796e635f706572696f64944d2c018c0c73796e635f74696d656f7574944d08078c0e73796e635f61727469666163747394888c1273796e635f6f6e5f636865636b706f696e74948875622e\"\n  },\n  \"_orig_experiment_path\": \"/Users/kuba/ijcai_ski/ray_results/test\",\n  \"_orig_experiment_dir_name\": \"test\",\n  \"_local_experiment_path\": \"/Users/kuba/ijcai_ski/ray_results/test\",\n  \"_remote_experiment_path\": null,\n  \"config\": {\n    \"Gradient_LR\": 0.025517939018637352,\n    \"Gradient_Momentum\": 0.9127268714803948,\n    \"Gradient_Steps\": 110,\n    \"Threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308050000000000000094869452942e\"\n    },\n    \"merge_threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b0074946243080c0000000000000094869452942e\"\n    },\n    \"number_of_points\": 100,\n    \"outliers_multiplier\": 2.0766851891516707\n  },\n  \"_Trial__unresolved_config\": {\n    \"Gradient_LR\": 0.025517939018637352,\n    \"Gradient_Momentum\": 0.9127268714803948,\n    \"Gradient_Steps\": 110,\n    \"Threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308050000000000000094869452942e\"\n    },\n    \"merge_threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b0074946243080c0000000000000094869452942e\"\n    },\n    \"number_of_points\": 100,\n    \"outliers_multiplier\": 2.0766851891516707\n  },\n  \"evaluated_params\": {\n    \"Gradient_LR\": 0.025517939018637352,\n    \"Gradient_Momentum\": 0.9127268714803948,\n    \"Gradient_Steps\": 110,\n    \"Threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308050000000000000094869452942e\"\n    },\n    \"merge_threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b0074946243080c0000000000000094869452942e\"\n    },\n    \"number_of_points\": 100,\n    \"outliers_multiplier\": 2.0766851891516707\n  },\n  \"experiment_tag\": \"3_Gradient_LR=0.0255,Gradient_Momentum=0.9127,Gradient_Steps=110,Threshold=5,merge_threshold=12,number_of_points=100,outliers_multiplier=2.0767\",\n  \"location\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059546000000000000008c197261792e74756e652e6578706572696d656e742e747269616c948c095f4c6f636174696f6e9493942981947d94288c08686f73746e616d65944e8c03706964944e75622e\"\n  },\n  \"stopping_criterion\": {},\n  \"_setup_default_resource\": true,\n  \"_default_placement_group_factory\": \"800595aa000000000000008c237261792e74756e652e657865637574696f6e2e706c6163656d656e745f67726f757073948c15506c6163656d656e7447726f7570466163746f72799493942981947d94288c085f62756e646c6573945d947d948c0343505594473ff000000000000073618c155f686561645f62756e646c655f69735f656d70747994898c095f7374726174656779948c045041434b948c055f6172677394298c075f6b7761726773947d9475622e\",\n  \"placement_group_factory\": \"800595aa000000000000008c237261792e74756e652e657865637574696f6e2e706c6163656d656e745f67726f757073948c15506c6163656d656e7447726f7570466163746f72799493942981947d94288c085f62756e646c6573945d947d948c0343505594473ff000000000000073618c155f686561645f62756e646c655f69735f656d70747994898c095f7374726174656779948c045041434b948c055f6172677394298c075f6b7761726773947d9475622e\",\n  \"log_to_file\": [\n    null,\n    null\n  ],\n  \"max_failures\": 0,\n  \"_last_result\": {\n    \"score\": 0.8809064686511823,\n    \"time_this_iter_s\": 6.891242980957031,\n    \"done\": true,\n    \"training_iteration\": 1,\n    \"trial_id\": \"6bdfc7c7\",\n    \"date\": \"2025-01-23_02-39-36\",\n    \"timestamp\": 1737596376,\n    \"time_total_s\": 6.891242980957031,\n    \"pid\": 30525,\n    \"hostname\": \"MacBook-Pro.ds19.agh.edu.pl\",\n    \"node_ip\": \"127.0.0.1\",\n    \"config\": {\n      \"Gradient_LR\": 0.025517939018637352,\n      \"Gradient_Momentum\": 0.9127268714803948,\n      \"Gradient_Steps\": 110,\n      \"Threshold\": 5,\n      \"merge_threshold\": 12,\n      \"number_of_points\": 100,\n      \"outliers_multiplier\": 2.0766851891516707\n    },\n    \"time_since_restore\": 6.891242980957031,\n    \"iterations_since_restore\": 1,\n    \"experiment_tag\": \"3_Gradient_LR=0.0255,Gradient_Momentum=0.9127,Gradient_Steps=110,Threshold=5,merge_threshold=12,number_of_points=100,outliers_multiplier=2.0767\"\n  },\n  \"_default_result_or_future\": null,\n  \"last_update_time\": 1737596376.528953,\n  \"metric_analysis\": {\n    \"score\": {\n      \"max\": 0.8809064686511823,\n      \"min\": 0.8809064686511823,\n      \"avg\": 0.8809064686511823,\n      \"last\": 0.8809064686511823,\n      \"last-5-avg\": 0.8809064686511823,\n      \"last-10-avg\": 0.8809064686511823\n    },\n    \"time_this_iter_s\": {\n      \"max\": 6.891242980957031,\n      \"min\": 6.891242980957031,\n      \"avg\": 6.891242980957031,\n      \"last\": 6.891242980957031,\n      \"last-5-avg\": 6.891242980957031,\n      \"last-10-avg\": 6.891242980957031\n    },\n    \"done\": {\n      \"max\": false,\n      \"min\": false,\n      \"avg\": false,\n      \"last\": false,\n      \"last-5-avg\": false,\n      \"last-10-avg\": false\n    },\n    \"training_iteration\": {\n      \"max\": 1,\n      \"min\": 1,\n      \"avg\": 1,\n      \"last\": 1,\n      \"last-5-avg\": 1,\n      \"last-10-avg\": 1\n    },\n    \"time_total_s\": {\n      \"max\": 6.891242980957031,\n      \"min\": 6.891242980957031,\n      \"avg\": 6.891242980957031,\n      \"last\": 6.891242980957031,\n      \"last-5-avg\": 6.891242980957031,\n      \"last-10-avg\": 6.891242980957031\n    },\n    \"time_since_restore\": {\n      \"max\": 6.891242980957031,\n      \"min\": 6.891242980957031,\n      \"avg\": 6.891242980957031,\n      \"last\": 6.891242980957031,\n      \"last-5-avg\": 6.891242980957031,\n      \"last-10-avg\": 6.891242980957031\n    },\n    \"iterations_since_restore\": {\n      \"max\": 1,\n      \"min\": 1,\n      \"avg\": 1,\n      \"last\": 1,\n      \"last-5-avg\": 1,\n      \"last-10-avg\": 1\n    }\n  },\n  \"n_steps\": [\n    5,\n    10\n  ],\n  \"metric_n_steps\": {\n    \"score\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059589000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452948c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02663894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308382236c36230ec3f9486945294612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059589000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452948c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02663894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308382236c36230ec3f9486945294612e\"\n      }\n    },\n    \"time_this_iter_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529447401b90a200000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529447401b90a200000000612e\"\n      }\n    },\n    \"done\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059522000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529489612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059522000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529489612e\"\n      }\n    },\n    \"training_iteration\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b01612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b01612e\"\n      }\n    },\n    \"time_total_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529447401b90a200000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529447401b90a200000000612e\"\n      }\n    },\n    \"time_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529447401b90a200000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529447401b90a200000000612e\"\n      }\n    },\n    \"iterations_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b01612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b01612e\"\n      }\n    }\n  },\n  \"export_formats\": [],\n  \"status\": \"TERMINATED\",\n  \"start_time\": 1737596369.6329432,\n  \"relative_logdir\": \"lambda_6bdfc7c7_3_Gradient_LR=0.0255,Gradient_Momentum=0.9127,Gradient_Steps=110,Threshold=5,merge_threshold=12,number_of_points=1_2025-01-23_02-39-27\",\n  \"runner\": null,\n  \"last_debug\": 0,\n  \"error_filename\": null,\n  \"pickled_error_filename\": null,\n  \"trial_name_creator\": null,\n  \"trial_dirname_creator\": null,\n  \"custom_trial_name\": null,\n  \"custom_dirname\": null,\n  \"experiment_dir_name\": \"test\",\n  \"saving_to\": null,\n  \"checkpoint_config\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"800595ff000000000000008c0e7261792e6169722e636f6e666967948c10436865636b706f696e74436f6e6669679493942981947d94288c0b6e756d5f746f5f6b656570944b038c1a636865636b706f696e745f73636f72655f617474726962757465948c12747261696e696e675f697465726174696f6e948c16636865636b706f696e745f73636f72655f6f72646572948c036d6178948c14636865636b706f696e745f6672657175656e6379944b008c11636865636b706f696e745f61745f656e64944e8c1a5f636865636b706f696e745f6b6565705f616c6c5f72616e6b7394898c1f5f636865636b706f696e745f75706c6f61645f66726f6d5f776f726b657273948975622e\"\n  },\n  \"checkpoint_manager\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"800595d2020000000000008c257261792e74756e652e657865637574696f6e2e636865636b706f696e745f6d616e61676572948c125f436865636b706f696e744d616e616765729493942981947d94288c145f636865636b706f696e745f7374726174656779948c0e7261792e6169722e636f6e666967948c10436865636b706f696e74436f6e6669679493942981947d94288c0b6e756d5f746f5f6b656570944b038c1a636865636b706f696e745f73636f72655f617474726962757465948c12747261696e696e675f697465726174696f6e948c16636865636b706f696e745f73636f72655f6f72646572948c036d6178948c14636865636b706f696e745f6672657175656e6379944b008c11636865636b706f696e745f61745f656e64944e8c1a5f636865636b706f696e745f6b6565705f616c6c5f72616e6b7394898c1f5f636865636b706f696e745f75706c6f61645f66726f6d5f776f726b657273948975628c155f6c61746573745f636865636b706f696e745f6964944b008c1a5f746f705f7065727369737465645f636865636b706f696e7473945d948c1a5f626573745f7065727369737465645f636865636b706f696e74944e8c1c5f6c61746573745f7065727369737465645f636865636b706f696e74944e8c195f6c61746573745f6d656d6f72795f636865636b706f696e74948c247261792e6169722e5f696e7465726e616c2e636865636b706f696e745f6d616e61676572948c125f547261636b6564436865636b706f696e749493942981947d94288c0b6469725f6f725f64617461944e8c026964944affffffff8c0c73746f726167655f6d6f646594681a8c11436865636b706f696e7453746f726167659493944b01859452948c0472616e6b944b008c076d657472696373947d948c076e6f64655f6970944e8c155f6d6f76655f696e73746561645f6f665f636f7079948875628c185f636865636b706f696e74735f746f5f636c65616e5f7570948f9475622e\"\n  },\n  \"restore_path\": null,\n  \"restoring_from\": null,\n  \"num_failures\": 0,\n  \"num_restore_failures\": 0,\n  \"results\": \"80054e2e\",\n  \"best_result\": \"80054e2e\",\n  \"param_config\": \"80054e2e\",\n  \"extra_arg\": \"80054e2e\",\n  \"_state_json\": null,\n  \"_state_valid\": false,\n  \"_resources\": \"80054e2e\"\n}",
    "{\n  \"stub\": false,\n  \"trainable_name\": \"lambda\",\n  \"trial_id\": \"d4609241\",\n  \"sync_config\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059593000000000000008c0f7261792e74756e652e73796e636572948c0a53796e63436f6e6669679493942981947d94288c0a75706c6f61645f646972944e8c0673796e636572948c046175746f948c0b73796e635f706572696f64944d2c018c0c73796e635f74696d656f7574944d08078c0e73796e635f61727469666163747394888c1273796e635f6f6e5f636865636b706f696e74948875622e\"\n  },\n  \"_orig_experiment_path\": \"/Users/kuba/ijcai_ski/ray_results/test\",\n  \"_orig_experiment_dir_name\": \"test\",\n  \"_local_experiment_path\": \"/Users/kuba/ijcai_ski/ray_results/test\",\n  \"_remote_experiment_path\": null,\n  \"config\": {\n    \"Gradient_LR\": 0.027408257693298785,\n    \"Gradient_Momentum\": 0.9389279425105777,\n    \"Gradient_Steps\": 170,\n    \"Threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b0074946243080a0000000000000094869452942e\"\n    },\n    \"merge_threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308070000000000000094869452942e\"\n    },\n    \"number_of_points\": 1000,\n    \"outliers_multiplier\": 4.732410618568929\n  },\n  \"_Trial__unresolved_config\": {\n    \"Gradient_LR\": 0.027408257693298785,\n    \"Gradient_Momentum\": 0.9389279425105777,\n    \"Gradient_Steps\": 170,\n    \"Threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b0074946243080a0000000000000094869452942e\"\n    },\n    \"merge_threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308070000000000000094869452942e\"\n    },\n    \"number_of_points\": 1000,\n    \"outliers_multiplier\": 4.732410618568929\n  },\n  \"evaluated_params\": {\n    \"Gradient_LR\": 0.027408257693298785,\n    \"Gradient_Momentum\": 0.9389279425105777,\n    \"Gradient_Steps\": 170,\n    \"Threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b0074946243080a0000000000000094869452942e\"\n    },\n    \"merge_threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308070000000000000094869452942e\"\n    },\n    \"number_of_points\": 1000,\n    \"outliers_multiplier\": 4.732410618568929\n  },\n  \"experiment_tag\": \"4_Gradient_LR=0.0274,Gradient_Momentum=0.9389,Gradient_Steps=170,Threshold=10,merge_threshold=7,number_of_points=1000,outliers_multiplier=4.7324\",\n  \"location\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059546000000000000008c197261792e74756e652e6578706572696d656e742e747269616c948c095f4c6f636174696f6e9493942981947d94288c08686f73746e616d65944e8c03706964944e75622e\"\n  },\n  \"stopping_criterion\": {},\n  \"_setup_default_resource\": true,\n  \"_default_placement_group_factory\": \"800595aa000000000000008c237261792e74756e652e657865637574696f6e2e706c6163656d656e745f67726f757073948c15506c6163656d656e7447726f7570466163746f72799493942981947d94288c085f62756e646c6573945d947d948c0343505594473ff000000000000073618c155f686561645f62756e646c655f69735f656d70747994898c095f7374726174656779948c045041434b948c055f6172677394298c075f6b7761726773947d9475622e\",\n  \"placement_group_factory\": \"800595aa000000000000008c237261792e74756e652e657865637574696f6e2e706c6163656d656e745f67726f757073948c15506c6163656d656e7447726f7570466163746f72799493942981947d94288c085f62756e646c6573945d947d948c0343505594473ff000000000000073618c155f686561645f62756e646c655f69735f656d70747994898c095f7374726174656779948c045041434b948c055f6172677394298c075f6b7761726773947d9475622e\",\n  \"log_to_file\": [\n    null,\n    null\n  ],\n  \"max_failures\": 0,\n  \"_last_result\": {\n    \"score\": 0.8670205975283771,\n    \"time_this_iter_s\": 10.966773986816406,\n    \"done\": true,\n    \"training_iteration\": 1,\n    \"trial_id\": \"d4609241\",\n    \"date\": \"2025-01-23_02-39-42\",\n    \"timestamp\": 1737596382,\n    \"time_total_s\": 10.966773986816406,\n    \"pid\": 30566,\n    \"hostname\": \"MacBook-Pro.ds19.agh.edu.pl\",\n    \"node_ip\": \"127.0.0.1\",\n    \"config\": {\n      \"Gradient_LR\": 0.027408257693298785,\n      \"Gradient_Momentum\": 0.9389279425105777,\n      \"Gradient_Steps\": 170,\n      \"Threshold\": 10,\n      \"merge_threshold\": 7,\n      \"number_of_points\": 1000,\n      \"outliers_multiplier\": 4.732410618568929\n    },\n    \"time_since_restore\": 10.966773986816406,\n    \"iterations_since_restore\": 1,\n    \"experiment_tag\": \"4_Gradient_LR=0.0274,Gradient_Momentum=0.9389,Gradient_Steps=170,Threshold=10,merge_threshold=7,number_of_points=1000,outliers_multiplier=4.7324\"\n  },\n  \"_default_result_or_future\": null,\n  \"last_update_time\": 1737596382.403958,\n  \"metric_analysis\": {\n    \"score\": {\n      \"max\": 0.8670205975283771,\n      \"min\": 0.8670205975283771,\n      \"avg\": 0.8670205975283771,\n      \"last\": 0.8670205975283771,\n      \"last-5-avg\": 0.8670205975283771,\n      \"last-10-avg\": 0.8670205975283771\n    },\n    \"time_this_iter_s\": {\n      \"max\": 10.966773986816406,\n      \"min\": 10.966773986816406,\n      \"avg\": 10.966773986816406,\n      \"last\": 10.966773986816406,\n      \"last-5-avg\": 10.966773986816406,\n      \"last-10-avg\": 10.966773986816406\n    },\n    \"done\": {\n      \"max\": false,\n      \"min\": false,\n      \"avg\": false,\n      \"last\": false,\n      \"last-5-avg\": false,\n      \"last-10-avg\": false\n    },\n    \"training_iteration\": {\n      \"max\": 1,\n      \"min\": 1,\n      \"avg\": 1,\n      \"last\": 1,\n      \"last-5-avg\": 1,\n      \"last-10-avg\": 1\n    },\n    \"time_total_s\": {\n      \"max\": 10.966773986816406,\n      \"min\": 10.966773986816406,\n      \"avg\": 10.966773986816406,\n      \"last\": 10.966773986816406,\n      \"last-5-avg\": 10.966773986816406,\n      \"last-10-avg\": 10.966773986816406\n    },\n    \"time_since_restore\": {\n      \"max\": 10.966773986816406,\n      \"min\": 10.966773986816406,\n      \"avg\": 10.966773986816406,\n      \"last\": 10.966773986816406,\n      \"last-5-avg\": 10.966773986816406,\n      \"last-10-avg\": 10.966773986816406\n    },\n    \"iterations_since_restore\": {\n      \"max\": 1,\n      \"min\": 1,\n      \"avg\": 1,\n      \"last\": 1,\n      \"last-5-avg\": 1,\n      \"last-10-avg\": 1\n    }\n  },\n  \"n_steps\": [\n    5,\n    10\n  ],\n  \"metric_n_steps\": {\n    \"score\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059589000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452948c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02663894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308e0f7eafaa1beeb3f9486945294612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059589000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452948c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02663894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308e0f7eafaa1beeb3f9486945294612e\"\n      }\n    },\n    \"time_this_iter_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294474025eefd00000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294474025eefd00000000612e\"\n      }\n    },\n    \"done\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059522000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529489612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059522000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529489612e\"\n      }\n    },\n    \"training_iteration\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b01612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b01612e\"\n      }\n    },\n    \"time_total_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294474025eefd00000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294474025eefd00000000612e\"\n      }\n    },\n    \"time_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294474025eefd00000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294474025eefd00000000612e\"\n      }\n    },\n    \"iterations_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b01612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b01612e\"\n      }\n    }\n  },\n  \"export_formats\": [],\n  \"status\": \"TERMINATED\",\n  \"start_time\": 1737596371.432683,\n  \"relative_logdir\": \"lambda_d4609241_4_Gradient_LR=0.0274,Gradient_Momentum=0.9389,Gradient_Steps=170,Threshold=10,merge_threshold=7,number_of_points=1_2025-01-23_02-39-29\",\n  \"runner\": null,\n  \"last_debug\": 0,\n  \"error_filename\": null,\n  \"pickled_error_filename\": null,\n  \"trial_name_creator\": null,\n  \"trial_dirname_creator\": null,\n  \"custom_trial_name\": null,\n  \"custom_dirname\": null,\n  \"experiment_dir_name\": \"test\",\n  \"saving_to\": null,\n  \"checkpoint_config\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"800595ff000000000000008c0e7261792e6169722e636f6e666967948c10436865636b706f696e74436f6e6669679493942981947d94288c0b6e756d5f746f5f6b656570944b038c1a636865636b706f696e745f73636f72655f617474726962757465948c12747261696e696e675f697465726174696f6e948c16636865636b706f696e745f73636f72655f6f72646572948c036d6178948c14636865636b706f696e745f6672657175656e6379944b008c11636865636b706f696e745f61745f656e64944e8c1a5f636865636b706f696e745f6b6565705f616c6c5f72616e6b7394898c1f5f636865636b706f696e745f75706c6f61645f66726f6d5f776f726b657273948975622e\"\n  },\n  \"checkpoint_manager\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"800595d2020000000000008c257261792e74756e652e657865637574696f6e2e636865636b706f696e745f6d616e61676572948c125f436865636b706f696e744d616e616765729493942981947d94288c145f636865636b706f696e745f7374726174656779948c0e7261792e6169722e636f6e666967948c10436865636b706f696e74436f6e6669679493942981947d94288c0b6e756d5f746f5f6b656570944b038c1a636865636b706f696e745f73636f72655f617474726962757465948c12747261696e696e675f697465726174696f6e948c16636865636b706f696e745f73636f72655f6f72646572948c036d6178948c14636865636b706f696e745f6672657175656e6379944b008c11636865636b706f696e745f61745f656e64944e8c1a5f636865636b706f696e745f6b6565705f616c6c5f72616e6b7394898c1f5f636865636b706f696e745f75706c6f61645f66726f6d5f776f726b657273948975628c155f6c61746573745f636865636b706f696e745f6964944b008c1a5f746f705f7065727369737465645f636865636b706f696e7473945d948c1a5f626573745f7065727369737465645f636865636b706f696e74944e8c1c5f6c61746573745f7065727369737465645f636865636b706f696e74944e8c195f6c61746573745f6d656d6f72795f636865636b706f696e74948c247261792e6169722e5f696e7465726e616c2e636865636b706f696e745f6d616e61676572948c125f547261636b6564436865636b706f696e749493942981947d94288c0b6469725f6f725f64617461944e8c026964944affffffff8c0c73746f726167655f6d6f646594681a8c11436865636b706f696e7453746f726167659493944b01859452948c0472616e6b944b008c076d657472696373947d948c076e6f64655f6970944e8c155f6d6f76655f696e73746561645f6f665f636f7079948875628c185f636865636b706f696e74735f746f5f636c65616e5f7570948f9475622e\"\n  },\n  \"restore_path\": null,\n  \"restoring_from\": null,\n  \"num_failures\": 0,\n  \"num_restore_failures\": 0,\n  \"results\": \"80054e2e\",\n  \"best_result\": \"80054e2e\",\n  \"param_config\": \"80054e2e\",\n  \"extra_arg\": \"80054e2e\",\n  \"_state_json\": null,\n  \"_state_valid\": false,\n  \"_resources\": \"80054e2e\"\n}",
    "{\n  \"stub\": false,\n  \"trainable_name\": \"lambda\",\n  \"trial_id\": \"ab9c15da\",\n  \"sync_config\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059593000000000000008c0f7261792e74756e652e73796e636572948c0a53796e63436f6e6669679493942981947d94288c0a75706c6f61645f646972944e8c0673796e636572948c046175746f948c0b73796e635f706572696f64944d2c018c0c73796e635f74696d656f7574944d08078c0e73796e635f61727469666163747394888c1273796e635f6f6e5f636865636b706f696e74948875622e\"\n  },\n  \"_orig_experiment_path\": \"/Users/kuba/ijcai_ski/ray_results/test\",\n  \"_orig_experiment_dir_name\": \"test\",\n  \"_local_experiment_path\": \"/Users/kuba/ijcai_ski/ray_results/test\",\n  \"_remote_experiment_path\": null,\n  \"config\": {\n    \"Gradient_LR\": 0.01281639237515841,\n    \"Gradient_Momentum\": 0.985833976903824,\n    \"Gradient_Steps\": 380,\n    \"Threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308000000000000000094869452942e\"\n    },\n    \"merge_threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b0074946243080d0000000000000094869452942e\"\n    },\n    \"number_of_points\": 900,\n    \"outliers_multiplier\": 4.167950151956536\n  },\n  \"_Trial__unresolved_config\": {\n    \"Gradient_LR\": 0.01281639237515841,\n    \"Gradient_Momentum\": 0.985833976903824,\n    \"Gradient_Steps\": 380,\n    \"Threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308000000000000000094869452942e\"\n    },\n    \"merge_threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b0074946243080d0000000000000094869452942e\"\n    },\n    \"number_of_points\": 900,\n    \"outliers_multiplier\": 4.167950151956536\n  },\n  \"evaluated_params\": {\n    \"Gradient_LR\": 0.01281639237515841,\n    \"Gradient_Momentum\": 0.985833976903824,\n    \"Gradient_Steps\": 380,\n    \"Threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308000000000000000094869452942e\"\n    },\n    \"merge_threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b0074946243080d0000000000000094869452942e\"\n    },\n    \"number_of_points\": 900,\n    \"outliers_multiplier\": 4.167950151956536\n  },\n  \"experiment_tag\": \"2_Gradient_LR=0.0128,Gradient_Momentum=0.9858,Gradient_Steps=380,Threshold=0,merge_threshold=13,number_of_points=900,outliers_multiplier=4.1680\",\n  \"location\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059546000000000000008c197261792e74756e652e6578706572696d656e742e747269616c948c095f4c6f636174696f6e9493942981947d94288c08686f73746e616d65944e8c03706964944e75622e\"\n  },\n  \"stopping_criterion\": {},\n  \"_setup_default_resource\": true,\n  \"_default_placement_group_factory\": \"800595aa000000000000008c237261792e74756e652e657865637574696f6e2e706c6163656d656e745f67726f757073948c15506c6163656d656e7447726f7570466163746f72799493942981947d94288c085f62756e646c6573945d947d948c0343505594473ff000000000000073618c155f686561645f62756e646c655f69735f656d70747994898c095f7374726174656779948c045041434b948c055f6172677394298c075f6b7761726773947d9475622e\",\n  \"placement_group_factory\": \"800595aa000000000000008c237261792e74756e652e657865637574696f6e2e706c6163656d656e745f67726f757073948c15506c6163656d656e7447726f7570466163746f72799493942981947d94288c085f62756e646c6573945d947d948c0343505594473ff000000000000073618c155f686561645f62756e646c655f69735f656d70747994898c095f7374726174656779948c045041434b948c055f6172677394298c075f6b7761726773947d9475622e\",\n  \"log_to_file\": [\n    null,\n    null\n  ],\n  \"max_failures\": 0,\n  \"_last_result\": {\n    \"score\": 0.9323407828133126,\n    \"time_this_iter_s\": 12.841746807098389,\n    \"done\": true,\n    \"training_iteration\": 1,\n    \"trial_id\": \"ab9c15da\",\n    \"date\": \"2025-01-23_02-39-40\",\n    \"timestamp\": 1737596380,\n    \"time_total_s\": 12.841746807098389,\n    \"pid\": 30510,\n    \"hostname\": \"MacBook-Pro.ds19.agh.edu.pl\",\n    \"node_ip\": \"127.0.0.1\",\n    \"config\": {\n      \"Gradient_LR\": 0.01281639237515841,\n      \"Gradient_Momentum\": 0.985833976903824,\n      \"Gradient_Steps\": 380,\n      \"Threshold\": 0,\n      \"merge_threshold\": 13,\n      \"number_of_points\": 900,\n      \"outliers_multiplier\": 4.167950151956536\n    },\n    \"time_since_restore\": 12.841746807098389,\n    \"iterations_since_restore\": 1,\n    \"experiment_tag\": \"2_Gradient_LR=0.0128,Gradient_Momentum=0.9858,Gradient_Steps=380,Threshold=0,merge_threshold=13,number_of_points=900,outliers_multiplier=4.1680\"\n  },\n  \"_default_result_or_future\": null,\n  \"last_update_time\": 1737596380.750031,\n  \"metric_analysis\": {\n    \"score\": {\n      \"max\": 0.9323407828133126,\n      \"min\": 0.9323407828133126,\n      \"avg\": 0.9323407828133126,\n      \"last\": 0.9323407828133126,\n      \"last-5-avg\": 0.9323407828133126,\n      \"last-10-avg\": 0.9323407828133126\n    },\n    \"time_this_iter_s\": {\n      \"max\": 12.841746807098389,\n      \"min\": 12.841746807098389,\n      \"avg\": 12.841746807098389,\n      \"last\": 12.841746807098389,\n      \"last-5-avg\": 12.841746807098389,\n      \"last-10-avg\": 12.841746807098389\n    },\n    \"done\": {\n      \"max\": false,\n      \"min\": false,\n      \"avg\": false,\n      \"last\": false,\n      \"last-5-avg\": false,\n      \"last-10-avg\": false\n    },\n    \"training_iteration\": {\n      \"max\": 1,\n      \"min\": 1,\n      \"avg\": 1,\n      \"last\": 1,\n      \"last-5-avg\": 1,\n      \"last-10-avg\": 1\n    },\n    \"time_total_s\": {\n      \"max\": 12.841746807098389,\n      \"min\": 12.841746807098389,\n      \"avg\": 12.841746807098389,\n      \"last\": 12.841746807098389,\n      \"last-5-avg\": 12.841746807098389,\n      \"last-10-avg\": 12.841746807098389\n    },\n    \"time_since_restore\": {\n      \"max\": 12.841746807098389,\n      \"min\": 12.841746807098389,\n      \"avg\": 12.841746807098389,\n      \"last\": 12.841746807098389,\n      \"last-5-avg\": 12.841746807098389,\n      \"last-10-avg\": 12.841746807098389\n    },\n    \"iterations_since_restore\": {\n      \"max\": 1,\n      \"min\": 1,\n      \"avg\": 1,\n      \"last\": 1,\n      \"last-5-avg\": 1,\n      \"last-10-avg\": 1\n    }\n  },\n  \"n_steps\": [\n    5,\n    10\n  ],\n  \"metric_n_steps\": {\n    \"score\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059589000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452948c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02663894898887945294284b038c013c944e4e4e4affffffff4affffffff4b0074946243087e205d56bcd5ed3f9486945294612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059589000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452948c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02663894898887945294284b038c013c944e4e4e4affffffff4affffffff4b0074946243087e205d56bcd5ed3f9486945294612e\"\n      }\n    },\n    \"time_this_iter_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294474029aef970000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294474029aef970000000612e\"\n      }\n    },\n    \"done\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059522000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529489612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059522000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529489612e\"\n      }\n    },\n    \"training_iteration\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b01612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b01612e\"\n      }\n    },\n    \"time_total_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294474029aef970000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294474029aef970000000612e\"\n      }\n    },\n    \"time_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294474029aef970000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294474029aef970000000612e\"\n      }\n    },\n    \"iterations_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b01612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b01612e\"\n      }\n    }\n  },\n  \"export_formats\": [],\n  \"status\": \"TERMINATED\",\n  \"start_time\": 1737596367.9036071,\n  \"relative_logdir\": \"lambda_ab9c15da_2_Gradient_LR=0.0128,Gradient_Momentum=0.9858,Gradient_Steps=380,Threshold=0,merge_threshold=13,number_of_points=9_2025-01-23_02-39-26\",\n  \"runner\": null,\n  \"last_debug\": 0,\n  \"error_filename\": null,\n  \"pickled_error_filename\": null,\n  \"trial_name_creator\": null,\n  \"trial_dirname_creator\": null,\n  \"custom_trial_name\": null,\n  \"custom_dirname\": null,\n  \"experiment_dir_name\": \"test\",\n  \"saving_to\": null,\n  \"checkpoint_config\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"800595ff000000000000008c0e7261792e6169722e636f6e666967948c10436865636b706f696e74436f6e6669679493942981947d94288c0b6e756d5f746f5f6b656570944b038c1a636865636b706f696e745f73636f72655f617474726962757465948c12747261696e696e675f697465726174696f6e948c16636865636b706f696e745f73636f72655f6f72646572948c036d6178948c14636865636b706f696e745f6672657175656e6379944b008c11636865636b706f696e745f61745f656e64944e8c1a5f636865636b706f696e745f6b6565705f616c6c5f72616e6b7394898c1f5f636865636b706f696e745f75706c6f61645f66726f6d5f776f726b657273948975622e\"\n  },\n  \"checkpoint_manager\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"800595d2020000000000008c257261792e74756e652e657865637574696f6e2e636865636b706f696e745f6d616e61676572948c125f436865636b706f696e744d616e616765729493942981947d94288c145f636865636b706f696e745f7374726174656779948c0e7261792e6169722e636f6e666967948c10436865636b706f696e74436f6e6669679493942981947d94288c0b6e756d5f746f5f6b656570944b038c1a636865636b706f696e745f73636f72655f617474726962757465948c12747261696e696e675f697465726174696f6e948c16636865636b706f696e745f73636f72655f6f72646572948c036d6178948c14636865636b706f696e745f6672657175656e6379944b008c11636865636b706f696e745f61745f656e64944e8c1a5f636865636b706f696e745f6b6565705f616c6c5f72616e6b7394898c1f5f636865636b706f696e745f75706c6f61645f66726f6d5f776f726b657273948975628c155f6c61746573745f636865636b706f696e745f6964944b008c1a5f746f705f7065727369737465645f636865636b706f696e7473945d948c1a5f626573745f7065727369737465645f636865636b706f696e74944e8c1c5f6c61746573745f7065727369737465645f636865636b706f696e74944e8c195f6c61746573745f6d656d6f72795f636865636b706f696e74948c247261792e6169722e5f696e7465726e616c2e636865636b706f696e745f6d616e61676572948c125f547261636b6564436865636b706f696e749493942981947d94288c0b6469725f6f725f64617461944e8c026964944affffffff8c0c73746f726167655f6d6f646594681a8c11436865636b706f696e7453746f726167659493944b01859452948c0472616e6b944b008c076d657472696373947d948c076e6f64655f6970944e8c155f6d6f76655f696e73746561645f6f665f636f7079948875628c185f636865636b706f696e74735f746f5f636c65616e5f7570948f9475622e\"\n  },\n  \"restore_path\": null,\n  \"restoring_from\": null,\n  \"num_failures\": 0,\n  \"num_restore_failures\": 0,\n  \"results\": \"80054e2e\",\n  \"best_result\": \"80054e2e\",\n  \"param_config\": \"80054e2e\",\n  \"extra_arg\": \"80054e2e\",\n  \"_state_json\": null,\n  \"_state_valid\": false,\n  \"_resources\": \"80054e2e\"\n}",
    "{\n  \"stub\": false,\n  \"trainable_name\": \"lambda\",\n  \"trial_id\": \"0f302f0a\",\n  \"sync_config\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059593000000000000008c0f7261792e74756e652e73796e636572948c0a53796e63436f6e6669679493942981947d94288c0a75706c6f61645f646972944e8c0673796e636572948c046175746f948c0b73796e635f706572696f64944d2c018c0c73796e635f74696d656f7574944d08078c0e73796e635f61727469666163747394888c1273796e635f6f6e5f636865636b706f696e74948875622e\"\n  },\n  \"_orig_experiment_path\": \"/Users/kuba/ijcai_ski/ray_results/test\",\n  \"_orig_experiment_dir_name\": \"test\",\n  \"_local_experiment_path\": \"/Users/kuba/ijcai_ski/ray_results/test\",\n  \"_remote_experiment_path\": null,\n  \"config\": {\n    \"Gradient_LR\": 0.012787959397374345,\n    \"Gradient_Momentum\": 0.9774737052016004,\n    \"Gradient_Steps\": 460,\n    \"Threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308090000000000000094869452942e\"\n    },\n    \"merge_threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308090000000000000094869452942e\"\n    },\n    \"number_of_points\": 900,\n    \"outliers_multiplier\": 4.495242250663756\n  },\n  \"_Trial__unresolved_config\": {\n    \"Gradient_LR\": 0.012787959397374345,\n    \"Gradient_Momentum\": 0.9774737052016004,\n    \"Gradient_Steps\": 460,\n    \"Threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308090000000000000094869452942e\"\n    },\n    \"merge_threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308090000000000000094869452942e\"\n    },\n    \"number_of_points\": 900,\n    \"outliers_multiplier\": 4.495242250663756\n  },\n  \"evaluated_params\": {\n    \"Gradient_LR\": 0.012787959397374345,\n    \"Gradient_Momentum\": 0.9774737052016004,\n    \"Gradient_Steps\": 460,\n    \"Threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308090000000000000094869452942e\"\n    },\n    \"merge_threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308090000000000000094869452942e\"\n    },\n    \"number_of_points\": 900,\n    \"outliers_multiplier\": 4.495242250663756\n  },\n  \"experiment_tag\": \"5_Gradient_LR=0.0128,Gradient_Momentum=0.9775,Gradient_Steps=460,Threshold=9,merge_threshold=9,number_of_points=900,outliers_multiplier=4.4952\",\n  \"location\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059546000000000000008c197261792e74756e652e6578706572696d656e742e747269616c948c095f4c6f636174696f6e9493942981947d94288c08686f73746e616d65944e8c03706964944e75622e\"\n  },\n  \"stopping_criterion\": {},\n  \"_setup_default_resource\": true,\n  \"_default_placement_group_factory\": \"800595aa000000000000008c237261792e74756e652e657865637574696f6e2e706c6163656d656e745f67726f757073948c15506c6163656d656e7447726f7570466163746f72799493942981947d94288c085f62756e646c6573945d947d948c0343505594473ff000000000000073618c155f686561645f62756e646c655f69735f656d70747994898c095f7374726174656779948c045041434b948c055f6172677394298c075f6b7761726773947d9475622e\",\n  \"placement_group_factory\": \"800595aa000000000000008c237261792e74756e652e657865637574696f6e2e706c6163656d656e745f67726f757073948c15506c6163656d656e7447726f7570466163746f72799493942981947d94288c085f62756e646c6573945d947d948c0343505594473ff000000000000073618c155f686561645f62756e646c655f69735f656d70747994898c095f7374726174656779948c045041434b948c055f6172677394298c075f6b7761726773947d9475622e\",\n  \"log_to_file\": [\n    null,\n    null\n  ],\n  \"max_failures\": 0,\n  \"_last_result\": {\n    \"score\": 0.8782914738884632,\n    \"time_this_iter_s\": 16.40457272529602,\n    \"done\": true,\n    \"training_iteration\": 1,\n    \"trial_id\": \"0f302f0a\",\n    \"date\": \"2025-01-23_02-39-50\",\n    \"timestamp\": 1737596390,\n    \"time_total_s\": 16.40457272529602,\n    \"pid\": 30590,\n    \"hostname\": \"MacBook-Pro.ds19.agh.edu.pl\",\n    \"node_ip\": \"127.0.0.1\",\n    \"config\": {\n      \"Gradient_LR\": 0.012787959397374345,\n      \"Gradient_Momentum\": 0.9774737052016004,\n      \"Gradient_Steps\": 460,\n      \"Threshold\": 9,\n      \"merge_threshold\": 9,\n      \"number_of_points\": 900,\n      \"outliers_multiplier\": 4.495242250663756\n    },\n    \"time_since_restore\": 16.40457272529602,\n    \"iterations_since_restore\": 1,\n    \"experiment_tag\": \"5_Gradient_LR=0.0128,Gradient_Momentum=0.9775,Gradient_Steps=460,Threshold=9,merge_threshold=9,number_of_points=900,outliers_multiplier=4.4952\"\n  },\n  \"_default_result_or_future\": null,\n  \"last_update_time\": 1737596390.01862,\n  \"metric_analysis\": {\n    \"score\": {\n      \"max\": 0.8782914738884632,\n      \"min\": 0.8782914738884632,\n      \"avg\": 0.8782914738884632,\n      \"last\": 0.8782914738884632,\n      \"last-5-avg\": 0.8782914738884632,\n      \"last-10-avg\": 0.8782914738884632\n    },\n    \"time_this_iter_s\": {\n      \"max\": 16.40457272529602,\n      \"min\": 16.40457272529602,\n      \"avg\": 16.40457272529602,\n      \"last\": 16.40457272529602,\n      \"last-5-avg\": 16.40457272529602,\n      \"last-10-avg\": 16.40457272529602\n    },\n    \"done\": {\n      \"max\": false,\n      \"min\": false,\n      \"avg\": false,\n      \"last\": false,\n      \"last-5-avg\": false,\n      \"last-10-avg\": false\n    },\n    \"training_iteration\": {\n      \"max\": 1,\n      \"min\": 1,\n      \"avg\": 1,\n      \"last\": 1,\n      \"last-5-avg\": 1,\n      \"last-10-avg\": 1\n    },\n    \"time_total_s\": {\n      \"max\": 16.40457272529602,\n      \"min\": 16.40457272529602,\n      \"avg\": 16.40457272529602,\n      \"last\": 16.40457272529602,\n      \"last-5-avg\": 16.40457272529602,\n      \"last-10-avg\": 16.40457272529602\n    },\n    \"time_since_restore\": {\n      \"max\": 16.40457272529602,\n      \"min\": 16.40457272529602,\n      \"avg\": 16.40457272529602,\n      \"last\": 16.40457272529602,\n      \"last-5-avg\": 16.40457272529602,\n      \"last-10-avg\": 16.40457272529602\n    },\n    \"iterations_since_restore\": {\n      \"max\": 1,\n      \"min\": 1,\n      \"avg\": 1,\n      \"last\": 1,\n      \"last-5-avg\": 1,\n      \"last-10-avg\": 1\n    }\n  },\n  \"n_steps\": [\n    5,\n    10\n  ],\n  \"metric_n_steps\": {\n    \"score\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059589000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452948c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02663894898887945294284b038c013c944e4e4e4affffffff4affffffff4b0074946243085d9c96b8f61aec3f9486945294612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059589000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452948c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02663894898887945294284b038c013c944e4e4e4affffffff4affffffff4b0074946243085d9c96b8f61aec3f9486945294612e\"\n      }\n    },\n    \"time_this_iter_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294474030679214000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294474030679214000000612e\"\n      }\n    },\n    \"done\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059522000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529489612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059522000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529489612e\"\n      }\n    },\n    \"training_iteration\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b01612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b01612e\"\n      }\n    },\n    \"time_total_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294474030679214000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294474030679214000000612e\"\n      }\n    },\n    \"time_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294474030679214000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294474030679214000000612e\"\n      }\n    },\n    \"iterations_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b01612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b01612e\"\n      }\n    }\n  },\n  \"export_formats\": [],\n  \"status\": \"TERMINATED\",\n  \"start_time\": 1737596373.607907,\n  \"relative_logdir\": \"lambda_0f302f0a_5_Gradient_LR=0.0128,Gradient_Momentum=0.9775,Gradient_Steps=460,Threshold=9,merge_threshold=9,number_of_points=90_2025-01-23_02-39-31\",\n  \"runner\": null,\n  \"last_debug\": 0,\n  \"error_filename\": null,\n  \"pickled_error_filename\": null,\n  \"trial_name_creator\": null,\n  \"trial_dirname_creator\": null,\n  \"custom_trial_name\": null,\n  \"custom_dirname\": null,\n  \"experiment_dir_name\": \"test\",\n  \"saving_to\": null,\n  \"checkpoint_config\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"800595ff000000000000008c0e7261792e6169722e636f6e666967948c10436865636b706f696e74436f6e6669679493942981947d94288c0b6e756d5f746f5f6b656570944b038c1a636865636b706f696e745f73636f72655f617474726962757465948c12747261696e696e675f697465726174696f6e948c16636865636b706f696e745f73636f72655f6f72646572948c036d6178948c14636865636b706f696e745f6672657175656e6379944b008c11636865636b706f696e745f61745f656e64944e8c1a5f636865636b706f696e745f6b6565705f616c6c5f72616e6b7394898c1f5f636865636b706f696e745f75706c6f61645f66726f6d5f776f726b657273948975622e\"\n  },\n  \"checkpoint_manager\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"800595d2020000000000008c257261792e74756e652e657865637574696f6e2e636865636b706f696e745f6d616e61676572948c125f436865636b706f696e744d616e616765729493942981947d94288c145f636865636b706f696e745f7374726174656779948c0e7261792e6169722e636f6e666967948c10436865636b706f696e74436f6e6669679493942981947d94288c0b6e756d5f746f5f6b656570944b038c1a636865636b706f696e745f73636f72655f617474726962757465948c12747261696e696e675f697465726174696f6e948c16636865636b706f696e745f73636f72655f6f72646572948c036d6178948c14636865636b706f696e745f6672657175656e6379944b008c11636865636b706f696e745f61745f656e64944e8c1a5f636865636b706f696e745f6b6565705f616c6c5f72616e6b7394898c1f5f636865636b706f696e745f75706c6f61645f66726f6d5f776f726b657273948975628c155f6c61746573745f636865636b706f696e745f6964944b008c1a5f746f705f7065727369737465645f636865636b706f696e7473945d948c1a5f626573745f7065727369737465645f636865636b706f696e74944e8c1c5f6c61746573745f7065727369737465645f636865636b706f696e74944e8c195f6c61746573745f6d656d6f72795f636865636b706f696e74948c247261792e6169722e5f696e7465726e616c2e636865636b706f696e745f6d616e61676572948c125f547261636b6564436865636b706f696e749493942981947d94288c0b6469725f6f725f64617461944e8c026964944affffffff8c0c73746f726167655f6d6f646594681a8c11436865636b706f696e7453746f726167659493944b01859452948c0472616e6b944b008c076d657472696373947d948c076e6f64655f6970944e8c155f6d6f76655f696e73746561645f6f665f636f7079948875628c185f636865636b706f696e74735f746f5f636c65616e5f7570948f9475622e\"\n  },\n  \"restore_path\": null,\n  \"restoring_from\": null,\n  \"num_failures\": 0,\n  \"num_restore_failures\": 0,\n  \"results\": \"80054e2e\",\n  \"best_result\": \"80054e2e\",\n  \"param_config\": \"80054e2e\",\n  \"extra_arg\": \"80054e2e\",\n  \"_state_json\": null,\n  \"_state_valid\": false,\n  \"_resources\": \"80054e2e\"\n}",
    "{\n  \"stub\": false,\n  \"trainable_name\": \"lambda\",\n  \"trial_id\": \"18ed770e\",\n  \"sync_config\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059593000000000000008c0f7261792e74756e652e73796e636572948c0a53796e63436f6e6669679493942981947d94288c0a75706c6f61645f646972944e8c0673796e636572948c046175746f948c0b73796e635f706572696f64944d2c018c0c73796e635f74696d656f7574944d08078c0e73796e635f61727469666163747394888c1273796e635f6f6e5f636865636b706f696e74948875622e\"\n  },\n  \"_orig_experiment_path\": \"/Users/kuba/ijcai_ski/ray_results/test\",\n  \"_orig_experiment_dir_name\": \"test\",\n  \"_local_experiment_path\": \"/Users/kuba/ijcai_ski/ray_results/test\",\n  \"_remote_experiment_path\": null,\n  \"config\": {\n    \"Gradient_LR\": 0.011391410373367753,\n    \"Gradient_Momentum\": 0.9633683924622485,\n    \"Gradient_Steps\": 450,\n    \"Threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308050000000000000094869452942e\"\n    },\n    \"merge_threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308100000000000000094869452942e\"\n    },\n    \"number_of_points\": 400,\n    \"outliers_multiplier\": 3.487077188149766\n  },\n  \"_Trial__unresolved_config\": {\n    \"Gradient_LR\": 0.011391410373367753,\n    \"Gradient_Momentum\": 0.9633683924622485,\n    \"Gradient_Steps\": 450,\n    \"Threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308050000000000000094869452942e\"\n    },\n    \"merge_threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308100000000000000094869452942e\"\n    },\n    \"number_of_points\": 400,\n    \"outliers_multiplier\": 3.487077188149766\n  },\n  \"evaluated_params\": {\n    \"Gradient_LR\": 0.011391410373367753,\n    \"Gradient_Momentum\": 0.9633683924622485,\n    \"Gradient_Steps\": 450,\n    \"Threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308050000000000000094869452942e\"\n    },\n    \"merge_threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308100000000000000094869452942e\"\n    },\n    \"number_of_points\": 400,\n    \"outliers_multiplier\": 3.487077188149766\n  },\n  \"experiment_tag\": \"11_Gradient_LR=0.0114,Gradient_Momentum=0.9634,Gradient_Steps=450,Threshold=5,merge_threshold=16,number_of_points=400,outliers_multiplier=3.4871\",\n  \"location\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059546000000000000008c197261792e74756e652e6578706572696d656e742e747269616c948c095f4c6f636174696f6e9493942981947d94288c08686f73746e616d65944e8c03706964944e75622e\"\n  },\n  \"stopping_criterion\": {},\n  \"_setup_default_resource\": true,\n  \"_default_placement_group_factory\": \"800595aa000000000000008c237261792e74756e652e657865637574696f6e2e706c6163656d656e745f67726f757073948c15506c6163656d656e7447726f7570466163746f72799493942981947d94288c085f62756e646c6573945d947d948c0343505594473ff000000000000073618c155f686561645f62756e646c655f69735f656d70747994898c095f7374726174656779948c045041434b948c055f6172677394298c075f6b7761726773947d9475622e\",\n  \"placement_group_factory\": \"800595aa000000000000008c237261792e74756e652e657865637574696f6e2e706c6163656d656e745f67726f757073948c15506c6163656d656e7447726f7570466163746f72799493942981947d94288c085f62756e646c6573945d947d948c0343505594473ff000000000000073618c155f686561645f62756e646c655f69735f656d70747994898c095f7374726174656779948c045041434b948c055f6172677394298c075f6b7761726773947d9475622e\",\n  \"log_to_file\": [\n    null,\n    null\n  ],\n  \"max_failures\": 0,\n  \"_last_result\": {\n    \"score\": 0.8932135582270984,\n    \"time_this_iter_s\": 22.70626473426819,\n    \"done\": true,\n    \"training_iteration\": 1,\n    \"trial_id\": \"18ed770e\",\n    \"date\": \"2025-01-23_02-40-08\",\n    \"timestamp\": 1737596408,\n    \"time_total_s\": 22.70626473426819,\n    \"pid\": 30685,\n    \"hostname\": \"MacBook-Pro.ds19.agh.edu.pl\",\n    \"node_ip\": \"127.0.0.1\",\n    \"config\": {\n      \"Gradient_LR\": 0.011391410373367753,\n      \"Gradient_Momentum\": 0.9633683924622485,\n      \"Gradient_Steps\": 450,\n      \"Threshold\": 5,\n      \"merge_threshold\": 16,\n      \"number_of_points\": 400,\n      \"outliers_multiplier\": 3.487077188149766\n    },\n    \"time_since_restore\": 22.70626473426819,\n    \"iterations_since_restore\": 1,\n    \"experiment_tag\": \"11_Gradient_LR=0.0114,Gradient_Momentum=0.9634,Gradient_Steps=450,Threshold=5,merge_threshold=16,number_of_points=400,outliers_multiplier=3.4871\"\n  },\n  \"_default_result_or_future\": null,\n  \"last_update_time\": 1737596408.192372,\n  \"metric_analysis\": {\n    \"score\": {\n      \"max\": 0.8932135582270984,\n      \"min\": 0.8932135582270984,\n      \"avg\": 0.8932135582270984,\n      \"last\": 0.8932135582270984,\n      \"last-5-avg\": 0.8932135582270984,\n      \"last-10-avg\": 0.8932135582270984\n    },\n    \"time_this_iter_s\": {\n      \"max\": 22.70626473426819,\n      \"min\": 22.70626473426819,\n      \"avg\": 22.70626473426819,\n      \"last\": 22.70626473426819,\n      \"last-5-avg\": 22.70626473426819,\n      \"last-10-avg\": 22.70626473426819\n    },\n    \"done\": {\n      \"max\": false,\n      \"min\": false,\n      \"avg\": false,\n      \"last\": false,\n      \"last-5-avg\": false,\n      \"last-10-avg\": false\n    },\n    \"training_iteration\": {\n      \"max\": 1,\n      \"min\": 1,\n      \"avg\": 1,\n      \"last\": 1,\n      \"last-5-avg\": 1,\n      \"last-10-avg\": 1\n    },\n    \"time_total_s\": {\n      \"max\": 22.70626473426819,\n      \"min\": 22.70626473426819,\n      \"avg\": 22.70626473426819,\n      \"last\": 22.70626473426819,\n      \"last-5-avg\": 22.70626473426819,\n      \"last-10-avg\": 22.70626473426819\n    },\n    \"time_since_restore\": {\n      \"max\": 22.70626473426819,\n      \"min\": 22.70626473426819,\n      \"avg\": 22.70626473426819,\n      \"last\": 22.70626473426819,\n      \"last-5-avg\": 22.70626473426819,\n      \"last-10-avg\": 22.70626473426819\n    },\n    \"iterations_since_restore\": {\n      \"max\": 1,\n      \"min\": 1,\n      \"avg\": 1,\n      \"last\": 1,\n      \"last-5-avg\": 1,\n      \"last-10-avg\": 1\n    }\n  },\n  \"n_steps\": [\n    5,\n    10\n  ],\n  \"metric_n_steps\": {\n    \"score\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059589000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452948c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02663894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308d6bb9d993495ec3f9486945294612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059589000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452948c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02663894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308d6bb9d993495ec3f9486945294612e\"\n      }\n    },\n    \"time_this_iter_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294474036b4cdc4000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294474036b4cdc4000000612e\"\n      }\n    },\n    \"done\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059522000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529489612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059522000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529489612e\"\n      }\n    },\n    \"training_iteration\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b01612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b01612e\"\n      }\n    },\n    \"time_total_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294474036b4cdc4000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294474036b4cdc4000000612e\"\n      }\n    },\n    \"time_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294474036b4cdc4000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294474036b4cdc4000000612e\"\n      }\n    },\n    \"iterations_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b01612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b01612e\"\n      }\n    }\n  },\n  \"export_formats\": [],\n  \"status\": \"TERMINATED\",\n  \"start_time\": 1737596385.4775538,\n  \"relative_logdir\": \"lambda_18ed770e_11_Gradient_LR=0.0114,Gradient_Momentum=0.9634,Gradient_Steps=450,Threshold=5,merge_threshold=16,number_of_points=_2025-01-23_02-39-42\",\n  \"runner\": null,\n  \"last_debug\": 0,\n  \"error_filename\": null,\n  \"pickled_error_filename\": null,\n  \"trial_name_creator\": null,\n  \"trial_dirname_creator\": null,\n  \"custom_trial_name\": null,\n  \"custom_dirname\": null,\n  \"experiment_dir_name\": \"test\",\n  \"saving_to\": null,\n  \"checkpoint_config\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"800595ff000000000000008c0e7261792e6169722e636f6e666967948c10436865636b706f696e74436f6e6669679493942981947d94288c0b6e756d5f746f5f6b656570944b038c1a636865636b706f696e745f73636f72655f617474726962757465948c12747261696e696e675f697465726174696f6e948c16636865636b706f696e745f73636f72655f6f72646572948c036d6178948c14636865636b706f696e745f6672657175656e6379944b008c11636865636b706f696e745f61745f656e64944e8c1a5f636865636b706f696e745f6b6565705f616c6c5f72616e6b7394898c1f5f636865636b706f696e745f75706c6f61645f66726f6d5f776f726b657273948975622e\"\n  },\n  \"checkpoint_manager\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"800595d2020000000000008c257261792e74756e652e657865637574696f6e2e636865636b706f696e745f6d616e61676572948c125f436865636b706f696e744d616e616765729493942981947d94288c145f636865636b706f696e745f7374726174656779948c0e7261792e6169722e636f6e666967948c10436865636b706f696e74436f6e6669679493942981947d94288c0b6e756d5f746f5f6b656570944b038c1a636865636b706f696e745f73636f72655f617474726962757465948c12747261696e696e675f697465726174696f6e948c16636865636b706f696e745f73636f72655f6f72646572948c036d6178948c14636865636b706f696e745f6672657175656e6379944b008c11636865636b706f696e745f61745f656e64944e8c1a5f636865636b706f696e745f6b6565705f616c6c5f72616e6b7394898c1f5f636865636b706f696e745f75706c6f61645f66726f6d5f776f726b657273948975628c155f6c61746573745f636865636b706f696e745f6964944b008c1a5f746f705f7065727369737465645f636865636b706f696e7473945d948c1a5f626573745f7065727369737465645f636865636b706f696e74944e8c1c5f6c61746573745f7065727369737465645f636865636b706f696e74944e8c195f6c61746573745f6d656d6f72795f636865636b706f696e74948c247261792e6169722e5f696e7465726e616c2e636865636b706f696e745f6d616e61676572948c125f547261636b6564436865636b706f696e749493942981947d94288c0b6469725f6f725f64617461944e8c026964944affffffff8c0c73746f726167655f6d6f646594681a8c11436865636b706f696e7453746f726167659493944b01859452948c0472616e6b944b008c076d657472696373947d948c076e6f64655f6970944e8c155f6d6f76655f696e73746561645f6f665f636f7079948875628c185f636865636b706f696e74735f746f5f636c65616e5f7570948f9475622e\"\n  },\n  \"restore_path\": null,\n  \"restoring_from\": null,\n  \"num_failures\": 0,\n  \"num_restore_failures\": 0,\n  \"results\": \"80054e2e\",\n  \"best_result\": \"80054e2e\",\n  \"param_config\": \"80054e2e\",\n  \"extra_arg\": \"80054e2e\",\n  \"_state_json\": null,\n  \"_state_valid\": false,\n  \"_resources\": \"80054e2e\"\n}",
    "{\n  \"stub\": false,\n  \"trainable_name\": \"lambda\",\n  \"trial_id\": \"91714d40\",\n  \"sync_config\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059593000000000000008c0f7261792e74756e652e73796e636572948c0a53796e63436f6e6669679493942981947d94288c0a75706c6f61645f646972944e8c0673796e636572948c046175746f948c0b73796e635f706572696f64944d2c018c0c73796e635f74696d656f7574944d08078c0e73796e635f61727469666163747394888c1273796e635f6f6e5f636865636b706f696e74948875622e\"\n  },\n  \"_orig_experiment_path\": \"/Users/kuba/ijcai_ski/ray_results/test\",\n  \"_orig_experiment_dir_name\": \"test\",\n  \"_local_experiment_path\": \"/Users/kuba/ijcai_ski/ray_results/test\",\n  \"_remote_experiment_path\": null,\n  \"config\": {\n    \"Gradient_LR\": 0.020535268773842614,\n    \"Gradient_Momentum\": 0.9567203090087126,\n    \"Gradient_Steps\": 180,\n    \"Threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308020000000000000094869452942e\"\n    },\n    \"merge_threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b0074946243080c0000000000000094869452942e\"\n    },\n    \"number_of_points\": 1000,\n    \"outliers_multiplier\": 3.9103391485588457\n  },\n  \"_Trial__unresolved_config\": {\n    \"Gradient_LR\": 0.020535268773842614,\n    \"Gradient_Momentum\": 0.9567203090087126,\n    \"Gradient_Steps\": 180,\n    \"Threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308020000000000000094869452942e\"\n    },\n    \"merge_threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b0074946243080c0000000000000094869452942e\"\n    },\n    \"number_of_points\": 1000,\n    \"outliers_multiplier\": 3.9103391485588457\n  },\n  \"evaluated_params\": {\n    \"Gradient_LR\": 0.020535268773842614,\n    \"Gradient_Momentum\": 0.9567203090087126,\n    \"Gradient_Steps\": 180,\n    \"Threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308020000000000000094869452942e\"\n    },\n    \"merge_threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b0074946243080c0000000000000094869452942e\"\n    },\n    \"number_of_points\": 1000,\n    \"outliers_multiplier\": 3.9103391485588457\n  },\n  \"experiment_tag\": \"7_Gradient_LR=0.0205,Gradient_Momentum=0.9567,Gradient_Steps=180,Threshold=2,merge_threshold=12,number_of_points=1000,outliers_multiplier=3.9103\",\n  \"location\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059546000000000000008c197261792e74756e652e6578706572696d656e742e747269616c948c095f4c6f636174696f6e9493942981947d94288c08686f73746e616d65944e8c03706964944e75622e\"\n  },\n  \"stopping_criterion\": {},\n  \"_setup_default_resource\": true,\n  \"_default_placement_group_factory\": \"800595aa000000000000008c237261792e74756e652e657865637574696f6e2e706c6163656d656e745f67726f757073948c15506c6163656d656e7447726f7570466163746f72799493942981947d94288c085f62756e646c6573945d947d948c0343505594473ff000000000000073618c155f686561645f62756e646c655f69735f656d70747994898c095f7374726174656779948c045041434b948c055f6172677394298c075f6b7761726773947d9475622e\",\n  \"placement_group_factory\": \"800595aa000000000000008c237261792e74756e652e657865637574696f6e2e706c6163656d656e745f67726f757073948c15506c6163656d656e7447726f7570466163746f72799493942981947d94288c085f62756e646c6573945d947d948c0343505594473ff000000000000073618c155f686561645f62756e646c655f69735f656d70747994898c095f7374726174656779948c045041434b948c055f6172677394298c075f6b7761726773947d9475622e\",\n  \"log_to_file\": [\n    null,\n    null\n  ],\n  \"max_failures\": 0,\n  \"_last_result\": {\n    \"score\": 0.8946517738746852,\n    \"time_this_iter_s\": 11.400901079177856,\n    \"done\": true,\n    \"training_iteration\": 1,\n    \"trial_id\": \"91714d40\",\n    \"date\": \"2025-01-23_02-39-47\",\n    \"timestamp\": 1737596387,\n    \"time_total_s\": 11.400901079177856,\n    \"pid\": 30525,\n    \"hostname\": \"MacBook-Pro.ds19.agh.edu.pl\",\n    \"node_ip\": \"127.0.0.1\",\n    \"config\": {\n      \"Gradient_LR\": 0.020535268773842614,\n      \"Gradient_Momentum\": 0.9567203090087126,\n      \"Gradient_Steps\": 180,\n      \"Threshold\": 2,\n      \"merge_threshold\": 12,\n      \"number_of_points\": 1000,\n      \"outliers_multiplier\": 3.9103391485588457\n    },\n    \"time_since_restore\": 11.400901079177856,\n    \"iterations_since_restore\": 1,\n    \"experiment_tag\": \"7_Gradient_LR=0.0205,Gradient_Momentum=0.9567,Gradient_Steps=180,Threshold=2,merge_threshold=12,number_of_points=1000,outliers_multiplier=3.9103\"\n  },\n  \"_default_result_or_future\": null,\n  \"last_update_time\": 1737596387.960293,\n  \"metric_analysis\": {\n    \"score\": {\n      \"max\": 0.8946517738746852,\n      \"min\": 0.8946517738746852,\n      \"avg\": 0.8946517738746852,\n      \"last\": 0.8946517738746852,\n      \"last-5-avg\": 0.8946517738746852,\n      \"last-10-avg\": 0.8946517738746852\n    },\n    \"time_this_iter_s\": {\n      \"max\": 11.400901079177856,\n      \"min\": 11.400901079177856,\n      \"avg\": 11.400901079177856,\n      \"last\": 11.400901079177856,\n      \"last-5-avg\": 11.400901079177856,\n      \"last-10-avg\": 11.400901079177856\n    },\n    \"done\": {\n      \"max\": false,\n      \"min\": false,\n      \"avg\": false,\n      \"last\": false,\n      \"last-5-avg\": false,\n      \"last-10-avg\": false\n    },\n    \"training_iteration\": {\n      \"max\": 1,\n      \"min\": 1,\n      \"avg\": 1,\n      \"last\": 1,\n      \"last-5-avg\": 1,\n      \"last-10-avg\": 1\n    },\n    \"time_total_s\": {\n      \"max\": 11.400901079177856,\n      \"min\": 11.400901079177856,\n      \"avg\": 11.400901079177856,\n      \"last\": 11.400901079177856,\n      \"last-5-avg\": 11.400901079177856,\n      \"last-10-avg\": 11.400901079177856\n    },\n    \"time_since_restore\": {\n      \"max\": 11.400901079177856,\n      \"min\": 11.400901079177856,\n      \"avg\": 11.400901079177856,\n      \"last\": 11.400901079177856,\n      \"last-5-avg\": 11.400901079177856,\n      \"last-10-avg\": 11.400901079177856\n    },\n    \"iterations_since_restore\": {\n      \"max\": 1,\n      \"min\": 1,\n      \"avg\": 1,\n      \"last\": 1,\n      \"last-5-avg\": 1,\n      \"last-10-avg\": 1\n    }\n  },\n  \"n_steps\": [\n    5,\n    10\n  ],\n  \"metric_n_steps\": {\n    \"score\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059589000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452948c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02663894898887945294284b038c013c944e4e4e4affffffff4affffffff4b0074946243088334c3c1fca0ec3f9486945294612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059589000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452948c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02663894898887945294284b038c013c944e4e4e4affffffff4affffffff4b0074946243088334c3c1fca0ec3f9486945294612e\"\n      }\n    },\n    \"time_this_iter_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294474026cd42e8000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294474026cd42e8000000612e\"\n      }\n    },\n    \"done\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059522000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529489612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059522000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529489612e\"\n      }\n    },\n    \"training_iteration\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b01612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b01612e\"\n      }\n    },\n    \"time_total_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294474026cd42e8000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294474026cd42e8000000612e\"\n      }\n    },\n    \"time_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294474026cd42e8000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294474026cd42e8000000612e\"\n      }\n    },\n    \"iterations_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b01612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b01612e\"\n      }\n    }\n  },\n  \"export_formats\": [],\n  \"status\": \"TERMINATED\",\n  \"start_time\": 1737596376.544914,\n  \"relative_logdir\": \"lambda_91714d40_7_Gradient_LR=0.0205,Gradient_Momentum=0.9567,Gradient_Steps=180,Threshold=2,merge_threshold=12,number_of_points=1_2025-01-23_02-39-35\",\n  \"runner\": null,\n  \"last_debug\": 0,\n  \"error_filename\": null,\n  \"pickled_error_filename\": null,\n  \"trial_name_creator\": null,\n  \"trial_dirname_creator\": null,\n  \"custom_trial_name\": null,\n  \"custom_dirname\": null,\n  \"experiment_dir_name\": \"test\",\n  \"saving_to\": null,\n  \"checkpoint_config\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"800595ff000000000000008c0e7261792e6169722e636f6e666967948c10436865636b706f696e74436f6e6669679493942981947d94288c0b6e756d5f746f5f6b656570944b038c1a636865636b706f696e745f73636f72655f617474726962757465948c12747261696e696e675f697465726174696f6e948c16636865636b706f696e745f73636f72655f6f72646572948c036d6178948c14636865636b706f696e745f6672657175656e6379944b008c11636865636b706f696e745f61745f656e64944e8c1a5f636865636b706f696e745f6b6565705f616c6c5f72616e6b7394898c1f5f636865636b706f696e745f75706c6f61645f66726f6d5f776f726b657273948975622e\"\n  },\n  \"checkpoint_manager\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"800595d2020000000000008c257261792e74756e652e657865637574696f6e2e636865636b706f696e745f6d616e61676572948c125f436865636b706f696e744d616e616765729493942981947d94288c145f636865636b706f696e745f7374726174656779948c0e7261792e6169722e636f6e666967948c10436865636b706f696e74436f6e6669679493942981947d94288c0b6e756d5f746f5f6b656570944b038c1a636865636b706f696e745f73636f72655f617474726962757465948c12747261696e696e675f697465726174696f6e948c16636865636b706f696e745f73636f72655f6f72646572948c036d6178948c14636865636b706f696e745f6672657175656e6379944b008c11636865636b706f696e745f61745f656e64944e8c1a5f636865636b706f696e745f6b6565705f616c6c5f72616e6b7394898c1f5f636865636b706f696e745f75706c6f61645f66726f6d5f776f726b657273948975628c155f6c61746573745f636865636b706f696e745f6964944b008c1a5f746f705f7065727369737465645f636865636b706f696e7473945d948c1a5f626573745f7065727369737465645f636865636b706f696e74944e8c1c5f6c61746573745f7065727369737465645f636865636b706f696e74944e8c195f6c61746573745f6d656d6f72795f636865636b706f696e74948c247261792e6169722e5f696e7465726e616c2e636865636b706f696e745f6d616e61676572948c125f547261636b6564436865636b706f696e749493942981947d94288c0b6469725f6f725f64617461944e8c026964944affffffff8c0c73746f726167655f6d6f646594681a8c11436865636b706f696e7453746f726167659493944b01859452948c0472616e6b944b008c076d657472696373947d948c076e6f64655f6970944e8c155f6d6f76655f696e73746561645f6f665f636f7079948875628c185f636865636b706f696e74735f746f5f636c65616e5f7570948f9475622e\"\n  },\n  \"restore_path\": null,\n  \"restoring_from\": null,\n  \"num_failures\": 0,\n  \"num_restore_failures\": 0,\n  \"results\": \"80054e2e\",\n  \"best_result\": \"80054e2e\",\n  \"param_config\": \"80054e2e\",\n  \"extra_arg\": \"80054e2e\",\n  \"_state_json\": null,\n  \"_state_valid\": false,\n  \"_resources\": \"80054e2e\"\n}",
    "{\n  \"stub\": false,\n  \"trainable_name\": \"lambda\",\n  \"trial_id\": \"0413d2f2\",\n  \"sync_config\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059593000000000000008c0f7261792e74756e652e73796e636572948c0a53796e63436f6e6669679493942981947d94288c0a75706c6f61645f646972944e8c0673796e636572948c046175746f948c0b73796e635f706572696f64944d2c018c0c73796e635f74696d656f7574944d08078c0e73796e635f61727469666163747394888c1273796e635f6f6e5f636865636b706f696e74948875622e\"\n  },\n  \"_orig_experiment_path\": \"/Users/kuba/ijcai_ski/ray_results/test\",\n  \"_orig_experiment_dir_name\": \"test\",\n  \"_local_experiment_path\": \"/Users/kuba/ijcai_ski/ray_results/test\",\n  \"_remote_experiment_path\": null,\n  \"config\": {\n    \"Gradient_LR\": 0.09377027873589988,\n    \"Gradient_Momentum\": 0.9409034056425309,\n    \"Gradient_Steps\": 450,\n    \"Threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308010000000000000094869452942e\"\n    },\n    \"merge_threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308060000000000000094869452942e\"\n    },\n    \"number_of_points\": 600,\n    \"outliers_multiplier\": 4.13113900934057\n  },\n  \"_Trial__unresolved_config\": {\n    \"Gradient_LR\": 0.09377027873589988,\n    \"Gradient_Momentum\": 0.9409034056425309,\n    \"Gradient_Steps\": 450,\n    \"Threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308010000000000000094869452942e\"\n    },\n    \"merge_threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308060000000000000094869452942e\"\n    },\n    \"number_of_points\": 600,\n    \"outliers_multiplier\": 4.13113900934057\n  },\n  \"evaluated_params\": {\n    \"Gradient_LR\": 0.09377027873589988,\n    \"Gradient_Momentum\": 0.9409034056425309,\n    \"Gradient_Steps\": 450,\n    \"Threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308010000000000000094869452942e\"\n    },\n    \"merge_threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308060000000000000094869452942e\"\n    },\n    \"number_of_points\": 600,\n    \"outliers_multiplier\": 4.13113900934057\n  },\n  \"experiment_tag\": \"8_Gradient_LR=0.0938,Gradient_Momentum=0.9409,Gradient_Steps=450,Threshold=1,merge_threshold=6,number_of_points=600,outliers_multiplier=4.1311\",\n  \"location\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059546000000000000008c197261792e74756e652e6578706572696d656e742e747269616c948c095f4c6f636174696f6e9493942981947d94288c08686f73746e616d65944e8c03706964944e75622e\"\n  },\n  \"stopping_criterion\": {},\n  \"_setup_default_resource\": true,\n  \"_default_placement_group_factory\": \"800595aa000000000000008c237261792e74756e652e657865637574696f6e2e706c6163656d656e745f67726f757073948c15506c6163656d656e7447726f7570466163746f72799493942981947d94288c085f62756e646c6573945d947d948c0343505594473ff000000000000073618c155f686561645f62756e646c655f69735f656d70747994898c095f7374726174656779948c045041434b948c055f6172677394298c075f6b7761726773947d9475622e\",\n  \"placement_group_factory\": \"800595aa000000000000008c237261792e74756e652e657865637574696f6e2e706c6163656d656e745f67726f757073948c15506c6163656d656e7447726f7570466163746f72799493942981947d94288c085f62756e646c6573945d947d948c0343505594473ff000000000000073618c155f686561645f62756e646c655f69735f656d70747994898c095f7374726174656779948c045041434b948c055f6172677394298c075f6b7761726773947d9475622e\",\n  \"log_to_file\": [\n    null,\n    null\n  ],\n  \"max_failures\": 0,\n  \"_last_result\": {\n    \"score\": 0.8954600069115118,\n    \"time_this_iter_s\": 16.74844980239868,\n    \"done\": true,\n    \"training_iteration\": 1,\n    \"trial_id\": \"0413d2f2\",\n    \"date\": \"2025-01-23_02-39-54\",\n    \"timestamp\": 1737596394,\n    \"time_total_s\": 16.74844980239868,\n    \"pid\": 30487,\n    \"hostname\": \"MacBook-Pro.ds19.agh.edu.pl\",\n    \"node_ip\": \"127.0.0.1\",\n    \"config\": {\n      \"Gradient_LR\": 0.09377027873589988,\n      \"Gradient_Momentum\": 0.9409034056425309,\n      \"Gradient_Steps\": 450,\n      \"Threshold\": 1,\n      \"merge_threshold\": 6,\n      \"number_of_points\": 600,\n      \"outliers_multiplier\": 4.13113900934057\n    },\n    \"time_since_restore\": 16.74844980239868,\n    \"iterations_since_restore\": 1,\n    \"experiment_tag\": \"8_Gradient_LR=0.0938,Gradient_Momentum=0.9409,Gradient_Steps=450,Threshold=1,merge_threshold=6,number_of_points=600,outliers_multiplier=4.1311\"\n  },\n  \"_default_result_or_future\": null,\n  \"last_update_time\": 1737596394.0584042,\n  \"metric_analysis\": {\n    \"score\": {\n      \"max\": 0.8954600069115118,\n      \"min\": 0.8954600069115118,\n      \"avg\": 0.8954600069115118,\n      \"last\": 0.8954600069115118,\n      \"last-5-avg\": 0.8954600069115118,\n      \"last-10-avg\": 0.8954600069115118\n    },\n    \"time_this_iter_s\": {\n      \"max\": 16.74844980239868,\n      \"min\": 16.74844980239868,\n      \"avg\": 16.74844980239868,\n      \"last\": 16.74844980239868,\n      \"last-5-avg\": 16.74844980239868,\n      \"last-10-avg\": 16.74844980239868\n    },\n    \"done\": {\n      \"max\": false,\n      \"min\": false,\n      \"avg\": false,\n      \"last\": false,\n      \"last-5-avg\": false,\n      \"last-10-avg\": false\n    },\n    \"training_iteration\": {\n      \"max\": 1,\n      \"min\": 1,\n      \"avg\": 1,\n      \"last\": 1,\n      \"last-5-avg\": 1,\n      \"last-10-avg\": 1\n    },\n    \"time_total_s\": {\n      \"max\": 16.74844980239868,\n      \"min\": 16.74844980239868,\n      \"avg\": 16.74844980239868,\n      \"last\": 16.74844980239868,\n      \"last-5-avg\": 16.74844980239868,\n      \"last-10-avg\": 16.74844980239868\n    },\n    \"time_since_restore\": {\n      \"max\": 16.74844980239868,\n      \"min\": 16.74844980239868,\n      \"avg\": 16.74844980239868,\n      \"last\": 16.74844980239868,\n      \"last-5-avg\": 16.74844980239868,\n      \"last-10-avg\": 16.74844980239868\n    },\n    \"iterations_since_restore\": {\n      \"max\": 1,\n      \"min\": 1,\n      \"avg\": 1,\n      \"last\": 1,\n      \"last-5-avg\": 1,\n      \"last-10-avg\": 1\n    }\n  },\n  \"n_steps\": [\n    5,\n    10\n  ],\n  \"metric_n_steps\": {\n    \"score\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059589000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452948c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02663894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308b5f291be9ba7ec3f9486945294612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059589000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452948c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02663894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308b5f291be9ba7ec3f9486945294612e\"\n      }\n    },\n    \"time_this_iter_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294474030bf9a68000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294474030bf9a68000000612e\"\n      }\n    },\n    \"done\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059522000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529489612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059522000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529489612e\"\n      }\n    },\n    \"training_iteration\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b01612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b01612e\"\n      }\n    },\n    \"time_total_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294474030bf9a68000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294474030bf9a68000000612e\"\n      }\n    },\n    \"time_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294474030bf9a68000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294474030bf9a68000000612e\"\n      }\n    },\n    \"iterations_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b01612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b01612e\"\n      }\n    }\n  },\n  \"export_formats\": [],\n  \"status\": \"TERMINATED\",\n  \"start_time\": 1737596377.2870772,\n  \"relative_logdir\": \"lambda_0413d2f2_8_Gradient_LR=0.0938,Gradient_Momentum=0.9409,Gradient_Steps=450,Threshold=1,merge_threshold=6,number_of_points=60_2025-01-23_02-39-36\",\n  \"runner\": null,\n  \"last_debug\": 0,\n  \"error_filename\": null,\n  \"pickled_error_filename\": null,\n  \"trial_name_creator\": null,\n  \"trial_dirname_creator\": null,\n  \"custom_trial_name\": null,\n  \"custom_dirname\": null,\n  \"experiment_dir_name\": \"test\",\n  \"saving_to\": null,\n  \"checkpoint_config\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"800595ff000000000000008c0e7261792e6169722e636f6e666967948c10436865636b706f696e74436f6e6669679493942981947d94288c0b6e756d5f746f5f6b656570944b038c1a636865636b706f696e745f73636f72655f617474726962757465948c12747261696e696e675f697465726174696f6e948c16636865636b706f696e745f73636f72655f6f72646572948c036d6178948c14636865636b706f696e745f6672657175656e6379944b008c11636865636b706f696e745f61745f656e64944e8c1a5f636865636b706f696e745f6b6565705f616c6c5f72616e6b7394898c1f5f636865636b706f696e745f75706c6f61645f66726f6d5f776f726b657273948975622e\"\n  },\n  \"checkpoint_manager\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"800595d2020000000000008c257261792e74756e652e657865637574696f6e2e636865636b706f696e745f6d616e61676572948c125f436865636b706f696e744d616e616765729493942981947d94288c145f636865636b706f696e745f7374726174656779948c0e7261792e6169722e636f6e666967948c10436865636b706f696e74436f6e6669679493942981947d94288c0b6e756d5f746f5f6b656570944b038c1a636865636b706f696e745f73636f72655f617474726962757465948c12747261696e696e675f697465726174696f6e948c16636865636b706f696e745f73636f72655f6f72646572948c036d6178948c14636865636b706f696e745f6672657175656e6379944b008c11636865636b706f696e745f61745f656e64944e8c1a5f636865636b706f696e745f6b6565705f616c6c5f72616e6b7394898c1f5f636865636b706f696e745f75706c6f61645f66726f6d5f776f726b657273948975628c155f6c61746573745f636865636b706f696e745f6964944b008c1a5f746f705f7065727369737465645f636865636b706f696e7473945d948c1a5f626573745f7065727369737465645f636865636b706f696e74944e8c1c5f6c61746573745f7065727369737465645f636865636b706f696e74944e8c195f6c61746573745f6d656d6f72795f636865636b706f696e74948c247261792e6169722e5f696e7465726e616c2e636865636b706f696e745f6d616e61676572948c125f547261636b6564436865636b706f696e749493942981947d94288c0b6469725f6f725f64617461944e8c026964944affffffff8c0c73746f726167655f6d6f646594681a8c11436865636b706f696e7453746f726167659493944b01859452948c0472616e6b944b008c076d657472696373947d948c076e6f64655f6970944e8c155f6d6f76655f696e73746561645f6f665f636f7079948875628c185f636865636b706f696e74735f746f5f636c65616e5f7570948f9475622e\"\n  },\n  \"restore_path\": null,\n  \"restoring_from\": null,\n  \"num_failures\": 0,\n  \"num_restore_failures\": 0,\n  \"results\": \"80054e2e\",\n  \"best_result\": \"80054e2e\",\n  \"param_config\": \"80054e2e\",\n  \"extra_arg\": \"80054e2e\",\n  \"_state_json\": null,\n  \"_state_valid\": false,\n  \"_resources\": \"80054e2e\"\n}",
    "{\n  \"stub\": false,\n  \"trainable_name\": \"lambda\",\n  \"trial_id\": \"4214a9f0\",\n  \"sync_config\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059593000000000000008c0f7261792e74756e652e73796e636572948c0a53796e63436f6e6669679493942981947d94288c0a75706c6f61645f646972944e8c0673796e636572948c046175746f948c0b73796e635f706572696f64944d2c018c0c73796e635f74696d656f7574944d08078c0e73796e635f61727469666163747394888c1273796e635f6f6e5f636865636b706f696e74948875622e\"\n  },\n  \"_orig_experiment_path\": \"/Users/kuba/ijcai_ski/ray_results/test\",\n  \"_orig_experiment_dir_name\": \"test\",\n  \"_local_experiment_path\": \"/Users/kuba/ijcai_ski/ray_results/test\",\n  \"_remote_experiment_path\": null,\n  \"config\": {\n    \"Gradient_LR\": 0.011268512934006047,\n    \"Gradient_Momentum\": 0.921303583574259,\n    \"Gradient_Steps\": 500,\n    \"Threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b0074946243080a0000000000000094869452942e\"\n    },\n    \"merge_threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308100000000000000094869452942e\"\n    },\n    \"number_of_points\": 800,\n    \"outliers_multiplier\": 2.811245785599433\n  },\n  \"_Trial__unresolved_config\": {\n    \"Gradient_LR\": 0.011268512934006047,\n    \"Gradient_Momentum\": 0.921303583574259,\n    \"Gradient_Steps\": 500,\n    \"Threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b0074946243080a0000000000000094869452942e\"\n    },\n    \"merge_threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308100000000000000094869452942e\"\n    },\n    \"number_of_points\": 800,\n    \"outliers_multiplier\": 2.811245785599433\n  },\n  \"evaluated_params\": {\n    \"Gradient_LR\": 0.011268512934006047,\n    \"Gradient_Momentum\": 0.921303583574259,\n    \"Gradient_Steps\": 500,\n    \"Threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b0074946243080a0000000000000094869452942e\"\n    },\n    \"merge_threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308100000000000000094869452942e\"\n    },\n    \"number_of_points\": 800,\n    \"outliers_multiplier\": 2.811245785599433\n  },\n  \"experiment_tag\": \"9_Gradient_LR=0.0113,Gradient_Momentum=0.9213,Gradient_Steps=500,Threshold=10,merge_threshold=16,number_of_points=800,outliers_multiplier=2.8112\",\n  \"location\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059546000000000000008c197261792e74756e652e6578706572696d656e742e747269616c948c095f4c6f636174696f6e9493942981947d94288c08686f73746e616d65944e8c03706964944e75622e\"\n  },\n  \"stopping_criterion\": {},\n  \"_setup_default_resource\": true,\n  \"_default_placement_group_factory\": \"800595aa000000000000008c237261792e74756e652e657865637574696f6e2e706c6163656d656e745f67726f757073948c15506c6163656d656e7447726f7570466163746f72799493942981947d94288c085f62756e646c6573945d947d948c0343505594473ff000000000000073618c155f686561645f62756e646c655f69735f656d70747994898c095f7374726174656779948c045041434b948c055f6172677394298c075f6b7761726773947d9475622e\",\n  \"placement_group_factory\": \"800595aa000000000000008c237261792e74756e652e657865637574696f6e2e706c6163656d656e745f67726f757073948c15506c6163656d656e7447726f7570466163746f72799493942981947d94288c085f62756e646c6573945d947d948c0343505594473ff000000000000073618c155f686561645f62756e646c655f69735f656d70747994898c095f7374726174656779948c045041434b948c055f6172677394298c075f6b7761726773947d9475622e\",\n  \"log_to_file\": [\n    null,\n    null\n  ],\n  \"max_failures\": 0,\n  \"_last_result\": {\n    \"score\": 0.8666134669843901,\n    \"time_this_iter_s\": 24.577895879745483,\n    \"done\": true,\n    \"training_iteration\": 1,\n    \"trial_id\": \"4214a9f0\",\n    \"date\": \"2025-01-23_02-40-05\",\n    \"timestamp\": 1737596405,\n    \"time_total_s\": 24.577895879745483,\n    \"pid\": 30510,\n    \"hostname\": \"MacBook-Pro.ds19.agh.edu.pl\",\n    \"node_ip\": \"127.0.0.1\",\n    \"config\": {\n      \"Gradient_LR\": 0.011268512934006047,\n      \"Gradient_Momentum\": 0.921303583574259,\n      \"Gradient_Steps\": 500,\n      \"Threshold\": 10,\n      \"merge_threshold\": 16,\n      \"number_of_points\": 800,\n      \"outliers_multiplier\": 2.811245785599433\n    },\n    \"time_since_restore\": 24.577895879745483,\n    \"iterations_since_restore\": 1,\n    \"experiment_tag\": \"9_Gradient_LR=0.0113,Gradient_Momentum=0.9213,Gradient_Steps=500,Threshold=10,merge_threshold=16,number_of_points=800,outliers_multiplier=2.8112\"\n  },\n  \"_default_result_or_future\": null,\n  \"last_update_time\": 1737596405.5500648,\n  \"metric_analysis\": {\n    \"score\": {\n      \"max\": 0.8666134669843901,\n      \"min\": 0.8666134669843901,\n      \"avg\": 0.8666134669843901,\n      \"last\": 0.8666134669843901,\n      \"last-5-avg\": 0.8666134669843901,\n      \"last-10-avg\": 0.8666134669843901\n    },\n    \"time_this_iter_s\": {\n      \"max\": 24.577895879745483,\n      \"min\": 24.577895879745483,\n      \"avg\": 24.577895879745483,\n      \"last\": 24.577895879745483,\n      \"last-5-avg\": 24.577895879745483,\n      \"last-10-avg\": 24.577895879745483\n    },\n    \"done\": {\n      \"max\": false,\n      \"min\": false,\n      \"avg\": false,\n      \"last\": false,\n      \"last-5-avg\": false,\n      \"last-10-avg\": false\n    },\n    \"training_iteration\": {\n      \"max\": 1,\n      \"min\": 1,\n      \"avg\": 1,\n      \"last\": 1,\n      \"last-5-avg\": 1,\n      \"last-10-avg\": 1\n    },\n    \"time_total_s\": {\n      \"max\": 24.577895879745483,\n      \"min\": 24.577895879745483,\n      \"avg\": 24.577895879745483,\n      \"last\": 24.577895879745483,\n      \"last-5-avg\": 24.577895879745483,\n      \"last-10-avg\": 24.577895879745483\n    },\n    \"time_since_restore\": {\n      \"max\": 24.577895879745483,\n      \"min\": 24.577895879745483,\n      \"avg\": 24.577895879745483,\n      \"last\": 24.577895879745483,\n      \"last-5-avg\": 24.577895879745483,\n      \"last-10-avg\": 24.577895879745483\n    },\n    \"iterations_since_restore\": {\n      \"max\": 1,\n      \"min\": 1,\n      \"avg\": 1,\n      \"last\": 1,\n      \"last-5-avg\": 1,\n      \"last-10-avg\": 1\n    }\n  },\n  \"n_steps\": [\n    5,\n    10\n  ],\n  \"metric_n_steps\": {\n    \"score\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059589000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452948c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02663894898887945294284b038c013c944e4e4e4affffffff4affffffff4b00749462430882135f2a4cbbeb3f9486945294612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059589000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452948c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02663894898887945294284b038c013c944e4e4e4affffffff4affffffff4b00749462430882135f2a4cbbeb3f9486945294612e\"\n      }\n    },\n    \"time_this_iter_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529447403893f0fc000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529447403893f0fc000000612e\"\n      }\n    },\n    \"done\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059522000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529489612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059522000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529489612e\"\n      }\n    },\n    \"training_iteration\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b01612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b01612e\"\n      }\n    },\n    \"time_total_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529447403893f0fc000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529447403893f0fc000000612e\"\n      }\n    },\n    \"time_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529447403893f0fc000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529447403893f0fc000000612e\"\n      }\n    },\n    \"iterations_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b01612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b01612e\"\n      }\n    }\n  },\n  \"export_formats\": [],\n  \"status\": \"TERMINATED\",\n  \"start_time\": 1737596380.758053,\n  \"relative_logdir\": \"lambda_4214a9f0_9_Gradient_LR=0.0113,Gradient_Momentum=0.9213,Gradient_Steps=500,Threshold=10,merge_threshold=16,number_of_points=_2025-01-23_02-39-37\",\n  \"runner\": null,\n  \"last_debug\": 0,\n  \"error_filename\": null,\n  \"pickled_error_filename\": null,\n  \"trial_name_creator\": null,\n  \"trial_dirname_creator\": null,\n  \"custom_trial_name\": null,\n  \"custom_dirname\": null,\n  \"experiment_dir_name\": \"test\",\n  \"saving_to\": null,\n  \"checkpoint_config\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"800595ff000000000000008c0e7261792e6169722e636f6e666967948c10436865636b706f696e74436f6e6669679493942981947d94288c0b6e756d5f746f5f6b656570944b038c1a636865636b706f696e745f73636f72655f617474726962757465948c12747261696e696e675f697465726174696f6e948c16636865636b706f696e745f73636f72655f6f72646572948c036d6178948c14636865636b706f696e745f6672657175656e6379944b008c11636865636b706f696e745f61745f656e64944e8c1a5f636865636b706f696e745f6b6565705f616c6c5f72616e6b7394898c1f5f636865636b706f696e745f75706c6f61645f66726f6d5f776f726b657273948975622e\"\n  },\n  \"checkpoint_manager\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"800595d2020000000000008c257261792e74756e652e657865637574696f6e2e636865636b706f696e745f6d616e61676572948c125f436865636b706f696e744d616e616765729493942981947d94288c145f636865636b706f696e745f7374726174656779948c0e7261792e6169722e636f6e666967948c10436865636b706f696e74436f6e6669679493942981947d94288c0b6e756d5f746f5f6b656570944b038c1a636865636b706f696e745f73636f72655f617474726962757465948c12747261696e696e675f697465726174696f6e948c16636865636b706f696e745f73636f72655f6f72646572948c036d6178948c14636865636b706f696e745f6672657175656e6379944b008c11636865636b706f696e745f61745f656e64944e8c1a5f636865636b706f696e745f6b6565705f616c6c5f72616e6b7394898c1f5f636865636b706f696e745f75706c6f61645f66726f6d5f776f726b657273948975628c155f6c61746573745f636865636b706f696e745f6964944b008c1a5f746f705f7065727369737465645f636865636b706f696e7473945d948c1a5f626573745f7065727369737465645f636865636b706f696e74944e8c1c5f6c61746573745f7065727369737465645f636865636b706f696e74944e8c195f6c61746573745f6d656d6f72795f636865636b706f696e74948c247261792e6169722e5f696e7465726e616c2e636865636b706f696e745f6d616e61676572948c125f547261636b6564436865636b706f696e749493942981947d94288c0b6469725f6f725f64617461944e8c026964944affffffff8c0c73746f726167655f6d6f646594681a8c11436865636b706f696e7453746f726167659493944b01859452948c0472616e6b944b008c076d657472696373947d948c076e6f64655f6970944e8c155f6d6f76655f696e73746561645f6f665f636f7079948875628c185f636865636b706f696e74735f746f5f636c65616e5f7570948f9475622e\"\n  },\n  \"restore_path\": null,\n  \"restoring_from\": null,\n  \"num_failures\": 0,\n  \"num_restore_failures\": 0,\n  \"results\": \"80054e2e\",\n  \"best_result\": \"80054e2e\",\n  \"param_config\": \"80054e2e\",\n  \"extra_arg\": \"80054e2e\",\n  \"_state_json\": null,\n  \"_state_valid\": false,\n  \"_resources\": \"80054e2e\"\n}",
    "{\n  \"stub\": false,\n  \"trainable_name\": \"lambda\",\n  \"trial_id\": \"c7501bfa\",\n  \"sync_config\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059593000000000000008c0f7261792e74756e652e73796e636572948c0a53796e63436f6e6669679493942981947d94288c0a75706c6f61645f646972944e8c0673796e636572948c046175746f948c0b73796e635f706572696f64944d2c018c0c73796e635f74696d656f7574944d08078c0e73796e635f61727469666163747394888c1273796e635f6f6e5f636865636b706f696e74948875622e\"\n  },\n  \"_orig_experiment_path\": \"/Users/kuba/ijcai_ski/ray_results/test\",\n  \"_orig_experiment_dir_name\": \"test\",\n  \"_local_experiment_path\": \"/Users/kuba/ijcai_ski/ray_results/test\",\n  \"_remote_experiment_path\": null,\n  \"config\": {\n    \"Gradient_LR\": 0.018727287847779113,\n    \"Gradient_Momentum\": 0.9244268195678768,\n    \"Gradient_Steps\": 290,\n    \"Threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b0074946243080c0000000000000094869452942e\"\n    },\n    \"merge_threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308050000000000000094869452942e\"\n    },\n    \"number_of_points\": 500,\n    \"outliers_multiplier\": 3.216608209732418\n  },\n  \"_Trial__unresolved_config\": {\n    \"Gradient_LR\": 0.018727287847779113,\n    \"Gradient_Momentum\": 0.9244268195678768,\n    \"Gradient_Steps\": 290,\n    \"Threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b0074946243080c0000000000000094869452942e\"\n    },\n    \"merge_threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308050000000000000094869452942e\"\n    },\n    \"number_of_points\": 500,\n    \"outliers_multiplier\": 3.216608209732418\n  },\n  \"evaluated_params\": {\n    \"Gradient_LR\": 0.018727287847779113,\n    \"Gradient_Momentum\": 0.9244268195678768,\n    \"Gradient_Steps\": 290,\n    \"Threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b0074946243080c0000000000000094869452942e\"\n    },\n    \"merge_threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308050000000000000094869452942e\"\n    },\n    \"number_of_points\": 500,\n    \"outliers_multiplier\": 3.216608209732418\n  },\n  \"experiment_tag\": \"10_Gradient_LR=0.0187,Gradient_Momentum=0.9244,Gradient_Steps=290,Threshold=12,merge_threshold=5,number_of_points=500,outliers_multiplier=3.2166\",\n  \"location\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059546000000000000008c197261792e74756e652e6578706572696d656e742e747269616c948c095f4c6f636174696f6e9493942981947d94288c08686f73746e616d65944e8c03706964944e75622e\"\n  },\n  \"stopping_criterion\": {},\n  \"_setup_default_resource\": true,\n  \"_default_placement_group_factory\": \"800595aa000000000000008c237261792e74756e652e657865637574696f6e2e706c6163656d656e745f67726f757073948c15506c6163656d656e7447726f7570466163746f72799493942981947d94288c085f62756e646c6573945d947d948c0343505594473ff000000000000073618c155f686561645f62756e646c655f69735f656d70747994898c095f7374726174656779948c045041434b948c055f6172677394298c075f6b7761726773947d9475622e\",\n  \"placement_group_factory\": \"800595aa000000000000008c237261792e74756e652e657865637574696f6e2e706c6163656d656e745f67726f757073948c15506c6163656d656e7447726f7570466163746f72799493942981947d94288c085f62756e646c6573945d947d948c0343505594473ff000000000000073618c155f686561645f62756e646c655f69735f656d70747994898c095f7374726174656779948c045041434b948c055f6172677394298c075f6b7761726773947d9475622e\",\n  \"log_to_file\": [\n    null,\n    null\n  ],\n  \"max_failures\": 0,\n  \"_last_result\": {\n    \"score\": 0.8621115989761631,\n    \"time_this_iter_s\": 13.160781860351562,\n    \"done\": true,\n    \"training_iteration\": 1,\n    \"trial_id\": \"c7501bfa\",\n    \"date\": \"2025-01-23_02-39-55\",\n    \"timestamp\": 1737596395,\n    \"time_total_s\": 13.160781860351562,\n    \"pid\": 30566,\n    \"hostname\": \"MacBook-Pro.ds19.agh.edu.pl\",\n    \"node_ip\": \"127.0.0.1\",\n    \"config\": {\n      \"Gradient_LR\": 0.018727287847779113,\n      \"Gradient_Momentum\": 0.9244268195678768,\n      \"Gradient_Steps\": 290,\n      \"Threshold\": 12,\n      \"merge_threshold\": 5,\n      \"number_of_points\": 500,\n      \"outliers_multiplier\": 3.216608209732418\n    },\n    \"time_since_restore\": 13.160781860351562,\n    \"iterations_since_restore\": 1,\n    \"experiment_tag\": \"10_Gradient_LR=0.0187,Gradient_Momentum=0.9244,Gradient_Steps=290,Threshold=12,merge_threshold=5,number_of_points=500,outliers_multiplier=3.2166\"\n  },\n  \"_default_result_or_future\": null,\n  \"last_update_time\": 1737596395.5763369,\n  \"metric_analysis\": {\n    \"score\": {\n      \"max\": 0.8621115989761631,\n      \"min\": 0.8621115989761631,\n      \"avg\": 0.8621115989761631,\n      \"last\": 0.8621115989761631,\n      \"last-5-avg\": 0.8621115989761631,\n      \"last-10-avg\": 0.8621115989761631\n    },\n    \"time_this_iter_s\": {\n      \"max\": 13.160781860351562,\n      \"min\": 13.160781860351562,\n      \"avg\": 13.160781860351562,\n      \"last\": 13.160781860351562,\n      \"last-5-avg\": 13.160781860351562,\n      \"last-10-avg\": 13.160781860351562\n    },\n    \"done\": {\n      \"max\": false,\n      \"min\": false,\n      \"avg\": false,\n      \"last\": false,\n      \"last-5-avg\": false,\n      \"last-10-avg\": false\n    },\n    \"training_iteration\": {\n      \"max\": 1,\n      \"min\": 1,\n      \"avg\": 1,\n      \"last\": 1,\n      \"last-5-avg\": 1,\n      \"last-10-avg\": 1\n    },\n    \"time_total_s\": {\n      \"max\": 13.160781860351562,\n      \"min\": 13.160781860351562,\n      \"avg\": 13.160781860351562,\n      \"last\": 13.160781860351562,\n      \"last-5-avg\": 13.160781860351562,\n      \"last-10-avg\": 13.160781860351562\n    },\n    \"time_since_restore\": {\n      \"max\": 13.160781860351562,\n      \"min\": 13.160781860351562,\n      \"avg\": 13.160781860351562,\n      \"last\": 13.160781860351562,\n      \"last-5-avg\": 13.160781860351562,\n      \"last-10-avg\": 13.160781860351562\n    },\n    \"iterations_since_restore\": {\n      \"max\": 1,\n      \"min\": 1,\n      \"avg\": 1,\n      \"last\": 1,\n      \"last-5-avg\": 1,\n      \"last-10-avg\": 1\n    }\n  },\n  \"n_steps\": [\n    5,\n    10\n  ],\n  \"metric_n_steps\": {\n    \"score\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059589000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452948c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02663894898887945294284b038c013c944e4e4e4affffffff4affffffff4b0074946243083d5b63106b96eb3f9486945294612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059589000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452948c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02663894898887945294284b038c013c944e4e4e4affffffff4affffffff4b0074946243083d5b63106b96eb3f9486945294612e\"\n      }\n    },\n    \"time_this_iter_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529447402a525200000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529447402a525200000000612e\"\n      }\n    },\n    \"done\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059522000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529489612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059522000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529489612e\"\n      }\n    },\n    \"training_iteration\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b01612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b01612e\"\n      }\n    },\n    \"time_total_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529447402a525200000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529447402a525200000000612e\"\n      }\n    },\n    \"time_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529447402a525200000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529447402a525200000000612e\"\n      }\n    },\n    \"iterations_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b01612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b01612e\"\n      }\n    }\n  },\n  \"export_formats\": [],\n  \"status\": \"TERMINATED\",\n  \"start_time\": 1737596382.4111302,\n  \"relative_logdir\": \"lambda_c7501bfa_10_Gradient_LR=0.0187,Gradient_Momentum=0.9244,Gradient_Steps=290,Threshold=12,merge_threshold=5,number_of_points=_2025-01-23_02-39-40\",\n  \"runner\": null,\n  \"last_debug\": 0,\n  \"error_filename\": null,\n  \"pickled_error_filename\": null,\n  \"trial_name_creator\": null,\n  \"trial_dirname_creator\": null,\n  \"custom_trial_name\": null,\n  \"custom_dirname\": null,\n  \"experiment_dir_name\": \"test\",\n  \"saving_to\": null,\n  \"checkpoint_config\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"800595ff000000000000008c0e7261792e6169722e636f6e666967948c10436865636b706f696e74436f6e6669679493942981947d94288c0b6e756d5f746f5f6b656570944b038c1a636865636b706f696e745f73636f72655f617474726962757465948c12747261696e696e675f697465726174696f6e948c16636865636b706f696e745f73636f72655f6f72646572948c036d6178948c14636865636b706f696e745f6672657175656e6379944b008c11636865636b706f696e745f61745f656e64944e8c1a5f636865636b706f696e745f6b6565705f616c6c5f72616e6b7394898c1f5f636865636b706f696e745f75706c6f61645f66726f6d5f776f726b657273948975622e\"\n  },\n  \"checkpoint_manager\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"800595d2020000000000008c257261792e74756e652e657865637574696f6e2e636865636b706f696e745f6d616e61676572948c125f436865636b706f696e744d616e616765729493942981947d94288c145f636865636b706f696e745f7374726174656779948c0e7261792e6169722e636f6e666967948c10436865636b706f696e74436f6e6669679493942981947d94288c0b6e756d5f746f5f6b656570944b038c1a636865636b706f696e745f73636f72655f617474726962757465948c12747261696e696e675f697465726174696f6e948c16636865636b706f696e745f73636f72655f6f72646572948c036d6178948c14636865636b706f696e745f6672657175656e6379944b008c11636865636b706f696e745f61745f656e64944e8c1a5f636865636b706f696e745f6b6565705f616c6c5f72616e6b7394898c1f5f636865636b706f696e745f75706c6f61645f66726f6d5f776f726b657273948975628c155f6c61746573745f636865636b706f696e745f6964944b008c1a5f746f705f7065727369737465645f636865636b706f696e7473945d948c1a5f626573745f7065727369737465645f636865636b706f696e74944e8c1c5f6c61746573745f7065727369737465645f636865636b706f696e74944e8c195f6c61746573745f6d656d6f72795f636865636b706f696e74948c247261792e6169722e5f696e7465726e616c2e636865636b706f696e745f6d616e61676572948c125f547261636b6564436865636b706f696e749493942981947d94288c0b6469725f6f725f64617461944e8c026964944affffffff8c0c73746f726167655f6d6f646594681a8c11436865636b706f696e7453746f726167659493944b01859452948c0472616e6b944b008c076d657472696373947d948c076e6f64655f6970944e8c155f6d6f76655f696e73746561645f6f665f636f7079948875628c185f636865636b706f696e74735f746f5f636c65616e5f7570948f9475622e\"\n  },\n  \"restore_path\": null,\n  \"restoring_from\": null,\n  \"num_failures\": 0,\n  \"num_restore_failures\": 0,\n  \"results\": \"80054e2e\",\n  \"best_result\": \"80054e2e\",\n  \"param_config\": \"80054e2e\",\n  \"extra_arg\": \"80054e2e\",\n  \"_state_json\": null,\n  \"_state_valid\": false,\n  \"_resources\": \"80054e2e\"\n}",
    "{\n  \"stub\": false,\n  \"trainable_name\": \"lambda\",\n  \"trial_id\": \"48bc8f40\",\n  \"sync_config\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059593000000000000008c0f7261792e74756e652e73796e636572948c0a53796e63436f6e6669679493942981947d94288c0a75706c6f61645f646972944e8c0673796e636572948c046175746f948c0b73796e635f706572696f64944d2c018c0c73796e635f74696d656f7574944d08078c0e73796e635f61727469666163747394888c1273796e635f6f6e5f636865636b706f696e74948875622e\"\n  },\n  \"_orig_experiment_path\": \"/Users/kuba/ijcai_ski/ray_results/test\",\n  \"_orig_experiment_dir_name\": \"test\",\n  \"_local_experiment_path\": \"/Users/kuba/ijcai_ski/ray_results/test\",\n  \"_remote_experiment_path\": null,\n  \"config\": {\n    \"Gradient_LR\": 0.07014387657553545,\n    \"Gradient_Momentum\": 0.9032411731869995,\n    \"Gradient_Steps\": 430,\n    \"Threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b0074946243080c0000000000000094869452942e\"\n    },\n    \"merge_threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308110000000000000094869452942e\"\n    },\n    \"number_of_points\": 500,\n    \"outliers_multiplier\": 3.1060827840301037\n  },\n  \"_Trial__unresolved_config\": {\n    \"Gradient_LR\": 0.07014387657553545,\n    \"Gradient_Momentum\": 0.9032411731869995,\n    \"Gradient_Steps\": 430,\n    \"Threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b0074946243080c0000000000000094869452942e\"\n    },\n    \"merge_threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308110000000000000094869452942e\"\n    },\n    \"number_of_points\": 500,\n    \"outliers_multiplier\": 3.1060827840301037\n  },\n  \"evaluated_params\": {\n    \"Gradient_LR\": 0.07014387657553545,\n    \"Gradient_Momentum\": 0.9032411731869995,\n    \"Gradient_Steps\": 430,\n    \"Threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b0074946243080c0000000000000094869452942e\"\n    },\n    \"merge_threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308110000000000000094869452942e\"\n    },\n    \"number_of_points\": 500,\n    \"outliers_multiplier\": 3.1060827840301037\n  },\n  \"experiment_tag\": \"16_Gradient_LR=0.0701,Gradient_Momentum=0.9032,Gradient_Steps=430,Threshold=12,merge_threshold=17,number_of_points=500,outliers_multiplier=3.1061\",\n  \"location\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059546000000000000008c197261792e74756e652e6578706572696d656e742e747269616c948c095f4c6f636174696f6e9493942981947d94288c08686f73746e616d65944e8c03706964944e75622e\"\n  },\n  \"stopping_criterion\": {},\n  \"_setup_default_resource\": true,\n  \"_default_placement_group_factory\": \"800595aa000000000000008c237261792e74756e652e657865637574696f6e2e706c6163656d656e745f67726f757073948c15506c6163656d656e7447726f7570466163746f72799493942981947d94288c085f62756e646c6573945d947d948c0343505594473ff000000000000073618c155f686561645f62756e646c655f69735f656d70747994898c095f7374726174656779948c045041434b948c055f6172677394298c075f6b7761726773947d9475622e\",\n  \"placement_group_factory\": \"800595aa000000000000008c237261792e74756e652e657865637574696f6e2e706c6163656d656e745f67726f757073948c15506c6163656d656e7447726f7570466163746f72799493942981947d94288c085f62756e646c6573945d947d948c0343505594473ff000000000000073618c155f686561645f62756e646c655f69735f656d70747994898c095f7374726174656779948c045041434b948c055f6172677394298c075f6b7761726773947d9475622e\",\n  \"log_to_file\": [\n    null,\n    null\n  ],\n  \"max_failures\": 0,\n  \"_last_result\": {\n    \"score\": 0.8550048616372358,\n    \"time_this_iter_s\": 20.94186019897461,\n    \"done\": true,\n    \"training_iteration\": 1,\n    \"trial_id\": \"48bc8f40\",\n    \"date\": \"2025-01-23_02-40-16\",\n    \"timestamp\": 1737596416,\n    \"time_total_s\": 20.94186019897461,\n    \"pid\": 30566,\n    \"hostname\": \"MacBook-Pro.ds19.agh.edu.pl\",\n    \"node_ip\": \"127.0.0.1\",\n    \"config\": {\n      \"Gradient_LR\": 0.07014387657553545,\n      \"Gradient_Momentum\": 0.9032411731869995,\n      \"Gradient_Steps\": 430,\n      \"Threshold\": 12,\n      \"merge_threshold\": 17,\n      \"number_of_points\": 500,\n      \"outliers_multiplier\": 3.1060827840301037\n    },\n    \"time_since_restore\": 20.94186019897461,\n    \"iterations_since_restore\": 1,\n    \"experiment_tag\": \"16_Gradient_LR=0.0701,Gradient_Momentum=0.9032,Gradient_Steps=430,Threshold=12,merge_threshold=17,number_of_points=500,outliers_multiplier=3.1061\"\n  },\n  \"_default_result_or_future\": null,\n  \"last_update_time\": 1737596416.624115,\n  \"metric_analysis\": {\n    \"score\": {\n      \"max\": 0.8550048616372358,\n      \"min\": 0.8550048616372358,\n      \"avg\": 0.8550048616372358,\n      \"last\": 0.8550048616372358,\n      \"last-5-avg\": 0.8550048616372358,\n      \"last-10-avg\": 0.8550048616372358\n    },\n    \"time_this_iter_s\": {\n      \"max\": 20.94186019897461,\n      \"min\": 20.94186019897461,\n      \"avg\": 20.94186019897461,\n      \"last\": 20.94186019897461,\n      \"last-5-avg\": 20.94186019897461,\n      \"last-10-avg\": 20.94186019897461\n    },\n    \"done\": {\n      \"max\": false,\n      \"min\": false,\n      \"avg\": false,\n      \"last\": false,\n      \"last-5-avg\": false,\n      \"last-10-avg\": false\n    },\n    \"training_iteration\": {\n      \"max\": 1,\n      \"min\": 1,\n      \"avg\": 1,\n      \"last\": 1,\n      \"last-5-avg\": 1,\n      \"last-10-avg\": 1\n    },\n    \"time_total_s\": {\n      \"max\": 20.94186019897461,\n      \"min\": 20.94186019897461,\n      \"avg\": 20.94186019897461,\n      \"last\": 20.94186019897461,\n      \"last-5-avg\": 20.94186019897461,\n      \"last-10-avg\": 20.94186019897461\n    },\n    \"time_since_restore\": {\n      \"max\": 20.94186019897461,\n      \"min\": 20.94186019897461,\n      \"avg\": 20.94186019897461,\n      \"last\": 20.94186019897461,\n      \"last-5-avg\": 20.94186019897461,\n      \"last-10-avg\": 20.94186019897461\n    },\n    \"iterations_since_restore\": {\n      \"max\": 1,\n      \"min\": 1,\n      \"avg\": 1,\n      \"last\": 1,\n      \"last-5-avg\": 1,\n      \"last-10-avg\": 1\n    }\n  },\n  \"n_steps\": [\n    5,\n    10\n  ],\n  \"metric_n_steps\": {\n    \"score\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059589000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452948c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02663894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308d3e4d427335ceb3f9486945294612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059589000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452948c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02663894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308d3e4d427335ceb3f9486945294612e\"\n      }\n    },\n    \"time_this_iter_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294474034f11dc0000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294474034f11dc0000000612e\"\n      }\n    },\n    \"done\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059522000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529489612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059522000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529489612e\"\n      }\n    },\n    \"training_iteration\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b01612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b01612e\"\n      }\n    },\n    \"time_total_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294474034f11dc0000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294474034f11dc0000000612e\"\n      }\n    },\n    \"time_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294474034f11dc0000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294474034f11dc0000000612e\"\n      }\n    },\n    \"iterations_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b01612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b01612e\"\n      }\n    }\n  },\n  \"export_formats\": [],\n  \"status\": \"TERMINATED\",\n  \"start_time\": 1737596395.582431,\n  \"relative_logdir\": \"lambda_48bc8f40_16_Gradient_LR=0.0701,Gradient_Momentum=0.9032,Gradient_Steps=430,Threshold=12,merge_threshold=17,number_of_points_2025-01-23_02-39-54\",\n  \"runner\": null,\n  \"last_debug\": 0,\n  \"error_filename\": null,\n  \"pickled_error_filename\": null,\n  \"trial_name_creator\": null,\n  \"trial_dirname_creator\": null,\n  \"custom_trial_name\": null,\n  \"custom_dirname\": null,\n  \"experiment_dir_name\": \"test\",\n  \"saving_to\": null,\n  \"checkpoint_config\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"800595ff000000000000008c0e7261792e6169722e636f6e666967948c10436865636b706f696e74436f6e6669679493942981947d94288c0b6e756d5f746f5f6b656570944b038c1a636865636b706f696e745f73636f72655f617474726962757465948c12747261696e696e675f697465726174696f6e948c16636865636b706f696e745f73636f72655f6f72646572948c036d6178948c14636865636b706f696e745f6672657175656e6379944b008c11636865636b706f696e745f61745f656e64944e8c1a5f636865636b706f696e745f6b6565705f616c6c5f72616e6b7394898c1f5f636865636b706f696e745f75706c6f61645f66726f6d5f776f726b657273948975622e\"\n  },\n  \"checkpoint_manager\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"800595d2020000000000008c257261792e74756e652e657865637574696f6e2e636865636b706f696e745f6d616e61676572948c125f436865636b706f696e744d616e616765729493942981947d94288c145f636865636b706f696e745f7374726174656779948c0e7261792e6169722e636f6e666967948c10436865636b706f696e74436f6e6669679493942981947d94288c0b6e756d5f746f5f6b656570944b038c1a636865636b706f696e745f73636f72655f617474726962757465948c12747261696e696e675f697465726174696f6e948c16636865636b706f696e745f73636f72655f6f72646572948c036d6178948c14636865636b706f696e745f6672657175656e6379944b008c11636865636b706f696e745f61745f656e64944e8c1a5f636865636b706f696e745f6b6565705f616c6c5f72616e6b7394898c1f5f636865636b706f696e745f75706c6f61645f66726f6d5f776f726b657273948975628c155f6c61746573745f636865636b706f696e745f6964944b008c1a5f746f705f7065727369737465645f636865636b706f696e7473945d948c1a5f626573745f7065727369737465645f636865636b706f696e74944e8c1c5f6c61746573745f7065727369737465645f636865636b706f696e74944e8c195f6c61746573745f6d656d6f72795f636865636b706f696e74948c247261792e6169722e5f696e7465726e616c2e636865636b706f696e745f6d616e61676572948c125f547261636b6564436865636b706f696e749493942981947d94288c0b6469725f6f725f64617461944e8c026964944affffffff8c0c73746f726167655f6d6f646594681a8c11436865636b706f696e7453746f726167659493944b01859452948c0472616e6b944b008c076d657472696373947d948c076e6f64655f6970944e8c155f6d6f76655f696e73746561645f6f665f636f7079948875628c185f636865636b706f696e74735f746f5f636c65616e5f7570948f9475622e\"\n  },\n  \"restore_path\": null,\n  \"restoring_from\": null,\n  \"num_failures\": 0,\n  \"num_restore_failures\": 0,\n  \"results\": \"80054e2e\",\n  \"best_result\": \"80054e2e\",\n  \"param_config\": \"80054e2e\",\n  \"extra_arg\": \"80054e2e\",\n  \"_state_json\": null,\n  \"_state_valid\": false,\n  \"_resources\": \"80054e2e\"\n}",
    "{\n  \"stub\": false,\n  \"trainable_name\": \"lambda\",\n  \"trial_id\": \"39862fdd\",\n  \"sync_config\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059593000000000000008c0f7261792e74756e652e73796e636572948c0a53796e63436f6e6669679493942981947d94288c0a75706c6f61645f646972944e8c0673796e636572948c046175746f948c0b73796e635f706572696f64944d2c018c0c73796e635f74696d656f7574944d08078c0e73796e635f61727469666163747394888c1273796e635f6f6e5f636865636b706f696e74948875622e\"\n  },\n  \"_orig_experiment_path\": \"/Users/kuba/ijcai_ski/ray_results/test\",\n  \"_orig_experiment_dir_name\": \"test\",\n  \"_local_experiment_path\": \"/Users/kuba/ijcai_ski/ray_results/test\",\n  \"_remote_experiment_path\": null,\n  \"config\": {\n    \"Gradient_LR\": 0.057162760846176504,\n    \"Gradient_Momentum\": 0.9701970005305472,\n    \"Gradient_Steps\": 370,\n    \"Threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308090000000000000094869452942e\"\n    },\n    \"merge_threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308120000000000000094869452942e\"\n    },\n    \"number_of_points\": 700,\n    \"outliers_multiplier\": 2.8808162170193086\n  },\n  \"_Trial__unresolved_config\": {\n    \"Gradient_LR\": 0.057162760846176504,\n    \"Gradient_Momentum\": 0.9701970005305472,\n    \"Gradient_Steps\": 370,\n    \"Threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308090000000000000094869452942e\"\n    },\n    \"merge_threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308120000000000000094869452942e\"\n    },\n    \"number_of_points\": 700,\n    \"outliers_multiplier\": 2.8808162170193086\n  },\n  \"evaluated_params\": {\n    \"Gradient_LR\": 0.057162760846176504,\n    \"Gradient_Momentum\": 0.9701970005305472,\n    \"Gradient_Steps\": 370,\n    \"Threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308090000000000000094869452942e\"\n    },\n    \"merge_threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308120000000000000094869452942e\"\n    },\n    \"number_of_points\": 700,\n    \"outliers_multiplier\": 2.8808162170193086\n  },\n  \"experiment_tag\": \"12_Gradient_LR=0.0572,Gradient_Momentum=0.9702,Gradient_Steps=370,Threshold=9,merge_threshold=18,number_of_points=700,outliers_multiplier=2.8808\",\n  \"location\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059546000000000000008c197261792e74756e652e6578706572696d656e742e747269616c948c095f4c6f636174696f6e9493942981947d94288c08686f73746e616d65944e8c03706964944e75622e\"\n  },\n  \"stopping_criterion\": {},\n  \"_setup_default_resource\": true,\n  \"_default_placement_group_factory\": \"800595aa000000000000008c237261792e74756e652e657865637574696f6e2e706c6163656d656e745f67726f757073948c15506c6163656d656e7447726f7570466163746f72799493942981947d94288c085f62756e646c6573945d947d948c0343505594473ff000000000000073618c155f686561645f62756e646c655f69735f656d70747994898c095f7374726174656779948c045041434b948c055f6172677394298c075f6b7761726773947d9475622e\",\n  \"placement_group_factory\": \"800595aa000000000000008c237261792e74756e652e657865637574696f6e2e706c6163656d656e745f67726f757073948c15506c6163656d656e7447726f7570466163746f72799493942981947d94288c085f62756e646c6573945d947d948c0343505594473ff000000000000073618c155f686561645f62756e646c655f69735f656d70747994898c095f7374726174656779948c045041434b948c055f6172677394298c075f6b7761726773947d9475622e\",\n  \"log_to_file\": [\n    null,\n    null\n  ],\n  \"max_failures\": 0,\n  \"_last_result\": {\n    \"score\": 0.8654751492758649,\n    \"time_this_iter_s\": 22.732887744903564,\n    \"done\": true,\n    \"training_iteration\": 1,\n    \"trial_id\": \"39862fdd\",\n    \"date\": \"2025-01-23_02-40-09\",\n    \"timestamp\": 1737596409,\n    \"time_total_s\": 22.732887744903564,\n    \"pid\": 30619,\n    \"hostname\": \"MacBook-Pro.ds19.agh.edu.pl\",\n    \"node_ip\": \"127.0.0.1\",\n    \"config\": {\n      \"Gradient_LR\": 0.057162760846176504,\n      \"Gradient_Momentum\": 0.9701970005305472,\n      \"Gradient_Steps\": 370,\n      \"Threshold\": 9,\n      \"merge_threshold\": 18,\n      \"number_of_points\": 700,\n      \"outliers_multiplier\": 2.8808162170193086\n    },\n    \"time_since_restore\": 22.732887744903564,\n    \"iterations_since_restore\": 1,\n    \"experiment_tag\": \"12_Gradient_LR=0.0572,Gradient_Momentum=0.9702,Gradient_Steps=370,Threshold=9,merge_threshold=18,number_of_points=700,outliers_multiplier=2.8808\"\n  },\n  \"_default_result_or_future\": null,\n  \"last_update_time\": 1737596409.707732,\n  \"metric_analysis\": {\n    \"score\": {\n      \"max\": 0.8654751492758649,\n      \"min\": 0.8654751492758649,\n      \"avg\": 0.8654751492758649,\n      \"last\": 0.8654751492758649,\n      \"last-5-avg\": 0.8654751492758649,\n      \"last-10-avg\": 0.8654751492758649\n    },\n    \"time_this_iter_s\": {\n      \"max\": 22.732887744903564,\n      \"min\": 22.732887744903564,\n      \"avg\": 22.732887744903564,\n      \"last\": 22.732887744903564,\n      \"last-5-avg\": 22.732887744903564,\n      \"last-10-avg\": 22.732887744903564\n    },\n    \"done\": {\n      \"max\": false,\n      \"min\": false,\n      \"avg\": false,\n      \"last\": false,\n      \"last-5-avg\": false,\n      \"last-10-avg\": false\n    },\n    \"training_iteration\": {\n      \"max\": 1,\n      \"min\": 1,\n      \"avg\": 1,\n      \"last\": 1,\n      \"last-5-avg\": 1,\n      \"last-10-avg\": 1\n    },\n    \"time_total_s\": {\n      \"max\": 22.732887744903564,\n      \"min\": 22.732887744903564,\n      \"avg\": 22.732887744903564,\n      \"last\": 22.732887744903564,\n      \"last-5-avg\": 22.732887744903564,\n      \"last-10-avg\": 22.732887744903564\n    },\n    \"time_since_restore\": {\n      \"max\": 22.732887744903564,\n      \"min\": 22.732887744903564,\n      \"avg\": 22.732887744903564,\n      \"last\": 22.732887744903564,\n      \"last-5-avg\": 22.732887744903564,\n      \"last-10-avg\": 22.732887744903564\n    },\n    \"iterations_since_restore\": {\n      \"max\": 1,\n      \"min\": 1,\n      \"avg\": 1,\n      \"last\": 1,\n      \"last-5-avg\": 1,\n      \"last-10-avg\": 1\n    }\n  },\n  \"n_steps\": [\n    5,\n    10\n  ],\n  \"metric_n_steps\": {\n    \"score\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059589000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452948c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02663894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308737fb4f0f8b1eb3f9486945294612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059589000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452948c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02663894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308737fb4f0f8b1eb3f9486945294612e\"\n      }\n    },\n    \"time_this_iter_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294474036bb9e88000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294474036bb9e88000000612e\"\n      }\n    },\n    \"done\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059522000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529489612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059522000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529489612e\"\n      }\n    },\n    \"training_iteration\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b01612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b01612e\"\n      }\n    },\n    \"time_total_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294474036bb9e88000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294474036bb9e88000000612e\"\n      }\n    },\n    \"time_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294474036bb9e88000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294474036bb9e88000000612e\"\n      }\n    },\n    \"iterations_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b01612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b01612e\"\n      }\n    }\n  },\n  \"export_formats\": [],\n  \"status\": \"TERMINATED\",\n  \"start_time\": 1737596386.9688761,\n  \"relative_logdir\": \"lambda_39862fdd_12_Gradient_LR=0.0572,Gradient_Momentum=0.9702,Gradient_Steps=370,Threshold=9,merge_threshold=18,number_of_points=_2025-01-23_02-39-45\",\n  \"runner\": null,\n  \"last_debug\": 0,\n  \"error_filename\": null,\n  \"pickled_error_filename\": null,\n  \"trial_name_creator\": null,\n  \"trial_dirname_creator\": null,\n  \"custom_trial_name\": null,\n  \"custom_dirname\": null,\n  \"experiment_dir_name\": \"test\",\n  \"saving_to\": null,\n  \"checkpoint_config\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"800595ff000000000000008c0e7261792e6169722e636f6e666967948c10436865636b706f696e74436f6e6669679493942981947d94288c0b6e756d5f746f5f6b656570944b038c1a636865636b706f696e745f73636f72655f617474726962757465948c12747261696e696e675f697465726174696f6e948c16636865636b706f696e745f73636f72655f6f72646572948c036d6178948c14636865636b706f696e745f6672657175656e6379944b008c11636865636b706f696e745f61745f656e64944e8c1a5f636865636b706f696e745f6b6565705f616c6c5f72616e6b7394898c1f5f636865636b706f696e745f75706c6f61645f66726f6d5f776f726b657273948975622e\"\n  },\n  \"checkpoint_manager\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"800595d2020000000000008c257261792e74756e652e657865637574696f6e2e636865636b706f696e745f6d616e61676572948c125f436865636b706f696e744d616e616765729493942981947d94288c145f636865636b706f696e745f7374726174656779948c0e7261792e6169722e636f6e666967948c10436865636b706f696e74436f6e6669679493942981947d94288c0b6e756d5f746f5f6b656570944b038c1a636865636b706f696e745f73636f72655f617474726962757465948c12747261696e696e675f697465726174696f6e948c16636865636b706f696e745f73636f72655f6f72646572948c036d6178948c14636865636b706f696e745f6672657175656e6379944b008c11636865636b706f696e745f61745f656e64944e8c1a5f636865636b706f696e745f6b6565705f616c6c5f72616e6b7394898c1f5f636865636b706f696e745f75706c6f61645f66726f6d5f776f726b657273948975628c155f6c61746573745f636865636b706f696e745f6964944b008c1a5f746f705f7065727369737465645f636865636b706f696e7473945d948c1a5f626573745f7065727369737465645f636865636b706f696e74944e8c1c5f6c61746573745f7065727369737465645f636865636b706f696e74944e8c195f6c61746573745f6d656d6f72795f636865636b706f696e74948c247261792e6169722e5f696e7465726e616c2e636865636b706f696e745f6d616e61676572948c125f547261636b6564436865636b706f696e749493942981947d94288c0b6469725f6f725f64617461944e8c026964944affffffff8c0c73746f726167655f6d6f646594681a8c11436865636b706f696e7453746f726167659493944b01859452948c0472616e6b944b008c076d657472696373947d948c076e6f64655f6970944e8c155f6d6f76655f696e73746561645f6f665f636f7079948875628c185f636865636b706f696e74735f746f5f636c65616e5f7570948f9475622e\"\n  },\n  \"restore_path\": null,\n  \"restoring_from\": null,\n  \"num_failures\": 0,\n  \"num_restore_failures\": 0,\n  \"results\": \"80054e2e\",\n  \"best_result\": \"80054e2e\",\n  \"param_config\": \"80054e2e\",\n  \"extra_arg\": \"80054e2e\",\n  \"_state_json\": null,\n  \"_state_valid\": false,\n  \"_resources\": \"80054e2e\"\n}",
    "{\n  \"stub\": false,\n  \"trainable_name\": \"lambda\",\n  \"trial_id\": \"9356fc9d\",\n  \"sync_config\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059593000000000000008c0f7261792e74756e652e73796e636572948c0a53796e63436f6e6669679493942981947d94288c0a75706c6f61645f646972944e8c0673796e636572948c046175746f948c0b73796e635f706572696f64944d2c018c0c73796e635f74696d656f7574944d08078c0e73796e635f61727469666163747394888c1273796e635f6f6e5f636865636b706f696e74948875622e\"\n  },\n  \"_orig_experiment_path\": \"/Users/kuba/ijcai_ski/ray_results/test\",\n  \"_orig_experiment_dir_name\": \"test\",\n  \"_local_experiment_path\": \"/Users/kuba/ijcai_ski/ray_results/test\",\n  \"_remote_experiment_path\": null,\n  \"config\": {\n    \"Gradient_LR\": 0.07939648490715823,\n    \"Gradient_Momentum\": 0.979411498885289,\n    \"Gradient_Steps\": 410,\n    \"Threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308040000000000000094869452942e\"\n    },\n    \"merge_threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308090000000000000094869452942e\"\n    },\n    \"number_of_points\": 400,\n    \"outliers_multiplier\": 4.567737875695231\n  },\n  \"_Trial__unresolved_config\": {\n    \"Gradient_LR\": 0.07939648490715823,\n    \"Gradient_Momentum\": 0.979411498885289,\n    \"Gradient_Steps\": 410,\n    \"Threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308040000000000000094869452942e\"\n    },\n    \"merge_threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308090000000000000094869452942e\"\n    },\n    \"number_of_points\": 400,\n    \"outliers_multiplier\": 4.567737875695231\n  },\n  \"evaluated_params\": {\n    \"Gradient_LR\": 0.07939648490715823,\n    \"Gradient_Momentum\": 0.979411498885289,\n    \"Gradient_Steps\": 410,\n    \"Threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308040000000000000094869452942e\"\n    },\n    \"merge_threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308090000000000000094869452942e\"\n    },\n    \"number_of_points\": 400,\n    \"outliers_multiplier\": 4.567737875695231\n  },\n  \"experiment_tag\": \"15_Gradient_LR=0.0794,Gradient_Momentum=0.9794,Gradient_Steps=410,Threshold=4,merge_threshold=9,number_of_points=400,outliers_multiplier=4.5677\",\n  \"location\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059546000000000000008c197261792e74756e652e6578706572696d656e742e747269616c948c095f4c6f636174696f6e9493942981947d94288c08686f73746e616d65944e8c03706964944e75622e\"\n  },\n  \"stopping_criterion\": {},\n  \"_setup_default_resource\": true,\n  \"_default_placement_group_factory\": \"800595aa000000000000008c237261792e74756e652e657865637574696f6e2e706c6163656d656e745f67726f757073948c15506c6163656d656e7447726f7570466163746f72799493942981947d94288c085f62756e646c6573945d947d948c0343505594473ff000000000000073618c155f686561645f62756e646c655f69735f656d70747994898c095f7374726174656779948c045041434b948c055f6172677394298c075f6b7761726773947d9475622e\",\n  \"placement_group_factory\": \"800595aa000000000000008c237261792e74756e652e657865637574696f6e2e706c6163656d656e745f67726f757073948c15506c6163656d656e7447726f7570466163746f72799493942981947d94288c085f62756e646c6573945d947d948c0343505594473ff000000000000073618c155f686561645f62756e646c655f69735f656d70747994898c095f7374726174656779948c045041434b948c055f6172677394298c075f6b7761726773947d9475622e\",\n  \"log_to_file\": [\n    null,\n    null\n  ],\n  \"max_failures\": 0,\n  \"_last_result\": {\n    \"score\": 0.8857325171049292,\n    \"time_this_iter_s\": 22.433563947677612,\n    \"done\": true,\n    \"training_iteration\": 1,\n    \"trial_id\": \"9356fc9d\",\n    \"date\": \"2025-01-23_02-40-16\",\n    \"timestamp\": 1737596416,\n    \"time_total_s\": 22.433563947677612,\n    \"pid\": 30487,\n    \"hostname\": \"MacBook-Pro.ds19.agh.edu.pl\",\n    \"node_ip\": \"127.0.0.1\",\n    \"config\": {\n      \"Gradient_LR\": 0.07939648490715823,\n      \"Gradient_Momentum\": 0.979411498885289,\n      \"Gradient_Steps\": 410,\n      \"Threshold\": 4,\n      \"merge_threshold\": 9,\n      \"number_of_points\": 400,\n      \"outliers_multiplier\": 4.567737875695231\n    },\n    \"time_since_restore\": 22.433563947677612,\n    \"iterations_since_restore\": 1,\n    \"experiment_tag\": \"15_Gradient_LR=0.0794,Gradient_Momentum=0.9794,Gradient_Steps=410,Threshold=4,merge_threshold=9,number_of_points=400,outliers_multiplier=4.5677\"\n  },\n  \"_default_result_or_future\": null,\n  \"last_update_time\": 1737596416.515342,\n  \"metric_analysis\": {\n    \"score\": {\n      \"max\": 0.8857325171049292,\n      \"min\": 0.8857325171049292,\n      \"avg\": 0.8857325171049292,\n      \"last\": 0.8857325171049292,\n      \"last-5-avg\": 0.8857325171049292,\n      \"last-10-avg\": 0.8857325171049292\n    },\n    \"time_this_iter_s\": {\n      \"max\": 22.433563947677612,\n      \"min\": 22.433563947677612,\n      \"avg\": 22.433563947677612,\n      \"last\": 22.433563947677612,\n      \"last-5-avg\": 22.433563947677612,\n      \"last-10-avg\": 22.433563947677612\n    },\n    \"done\": {\n      \"max\": false,\n      \"min\": false,\n      \"avg\": false,\n      \"last\": false,\n      \"last-5-avg\": false,\n      \"last-10-avg\": false\n    },\n    \"training_iteration\": {\n      \"max\": 1,\n      \"min\": 1,\n      \"avg\": 1,\n      \"last\": 1,\n      \"last-5-avg\": 1,\n      \"last-10-avg\": 1\n    },\n    \"time_total_s\": {\n      \"max\": 22.433563947677612,\n      \"min\": 22.433563947677612,\n      \"avg\": 22.433563947677612,\n      \"last\": 22.433563947677612,\n      \"last-5-avg\": 22.433563947677612,\n      \"last-10-avg\": 22.433563947677612\n    },\n    \"time_since_restore\": {\n      \"max\": 22.433563947677612,\n      \"min\": 22.433563947677612,\n      \"avg\": 22.433563947677612,\n      \"last\": 22.433563947677612,\n      \"last-5-avg\": 22.433563947677612,\n      \"last-10-avg\": 22.433563947677612\n    },\n    \"iterations_since_restore\": {\n      \"max\": 1,\n      \"min\": 1,\n      \"avg\": 1,\n      \"last\": 1,\n      \"last-5-avg\": 1,\n      \"last-10-avg\": 1\n    }\n  },\n  \"n_steps\": [\n    5,\n    10\n  ],\n  \"metric_n_steps\": {\n    \"score\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059589000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452948c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02663894898887945294284b038c013c944e4e4e4affffffff4affffffff4b00749462430895053fb8eb57ec3f9486945294612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059589000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452948c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02663894898887945294284b038c013c944e4e4e4affffffff4affffffff4b00749462430895053fb8eb57ec3f9486945294612e\"\n      }\n    },\n    \"time_this_iter_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944740366efe0c000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944740366efe0c000000612e\"\n      }\n    },\n    \"done\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059522000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529489612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059522000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529489612e\"\n      }\n    },\n    \"training_iteration\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b01612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b01612e\"\n      }\n    },\n    \"time_total_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944740366efe0c000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944740366efe0c000000612e\"\n      }\n    },\n    \"time_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944740366efe0c000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944740366efe0c000000612e\"\n      }\n    },\n    \"iterations_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b01612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b01612e\"\n      }\n    }\n  },\n  \"export_formats\": [],\n  \"status\": \"TERMINATED\",\n  \"start_time\": 1737596394.0751848,\n  \"relative_logdir\": \"lambda_9356fc9d_15_Gradient_LR=0.0794,Gradient_Momentum=0.9794,Gradient_Steps=410,Threshold=4,merge_threshold=9,number_of_points=4_2025-01-23_02-39-50\",\n  \"runner\": null,\n  \"last_debug\": 0,\n  \"error_filename\": null,\n  \"pickled_error_filename\": null,\n  \"trial_name_creator\": null,\n  \"trial_dirname_creator\": null,\n  \"custom_trial_name\": null,\n  \"custom_dirname\": null,\n  \"experiment_dir_name\": \"test\",\n  \"saving_to\": null,\n  \"checkpoint_config\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"800595ff000000000000008c0e7261792e6169722e636f6e666967948c10436865636b706f696e74436f6e6669679493942981947d94288c0b6e756d5f746f5f6b656570944b038c1a636865636b706f696e745f73636f72655f617474726962757465948c12747261696e696e675f697465726174696f6e948c16636865636b706f696e745f73636f72655f6f72646572948c036d6178948c14636865636b706f696e745f6672657175656e6379944b008c11636865636b706f696e745f61745f656e64944e8c1a5f636865636b706f696e745f6b6565705f616c6c5f72616e6b7394898c1f5f636865636b706f696e745f75706c6f61645f66726f6d5f776f726b657273948975622e\"\n  },\n  \"checkpoint_manager\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"800595d2020000000000008c257261792e74756e652e657865637574696f6e2e636865636b706f696e745f6d616e61676572948c125f436865636b706f696e744d616e616765729493942981947d94288c145f636865636b706f696e745f7374726174656779948c0e7261792e6169722e636f6e666967948c10436865636b706f696e74436f6e6669679493942981947d94288c0b6e756d5f746f5f6b656570944b038c1a636865636b706f696e745f73636f72655f617474726962757465948c12747261696e696e675f697465726174696f6e948c16636865636b706f696e745f73636f72655f6f72646572948c036d6178948c14636865636b706f696e745f6672657175656e6379944b008c11636865636b706f696e745f61745f656e64944e8c1a5f636865636b706f696e745f6b6565705f616c6c5f72616e6b7394898c1f5f636865636b706f696e745f75706c6f61645f66726f6d5f776f726b657273948975628c155f6c61746573745f636865636b706f696e745f6964944b008c1a5f746f705f7065727369737465645f636865636b706f696e7473945d948c1a5f626573745f7065727369737465645f636865636b706f696e74944e8c1c5f6c61746573745f7065727369737465645f636865636b706f696e74944e8c195f6c61746573745f6d656d6f72795f636865636b706f696e74948c247261792e6169722e5f696e7465726e616c2e636865636b706f696e745f6d616e61676572948c125f547261636b6564436865636b706f696e749493942981947d94288c0b6469725f6f725f64617461944e8c026964944affffffff8c0c73746f726167655f6d6f646594681a8c11436865636b706f696e7453746f726167659493944b01859452948c0472616e6b944b008c076d657472696373947d948c076e6f64655f6970944e8c155f6d6f76655f696e73746561645f6f665f636f7079948875628c185f636865636b706f696e74735f746f5f636c65616e5f7570948f9475622e\"\n  },\n  \"restore_path\": null,\n  \"restoring_from\": null,\n  \"num_failures\": 0,\n  \"num_restore_failures\": 0,\n  \"results\": \"80054e2e\",\n  \"best_result\": \"80054e2e\",\n  \"param_config\": \"80054e2e\",\n  \"extra_arg\": \"80054e2e\",\n  \"_state_json\": null,\n  \"_state_valid\": false,\n  \"_resources\": \"80054e2e\"\n}",
    "{\n  \"stub\": false,\n  \"trainable_name\": \"lambda\",\n  \"trial_id\": \"8b69e246\",\n  \"sync_config\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059593000000000000008c0f7261792e74756e652e73796e636572948c0a53796e63436f6e6669679493942981947d94288c0a75706c6f61645f646972944e8c0673796e636572948c046175746f948c0b73796e635f706572696f64944d2c018c0c73796e635f74696d656f7574944d08078c0e73796e635f61727469666163747394888c1273796e635f6f6e5f636865636b706f696e74948875622e\"\n  },\n  \"_orig_experiment_path\": \"/Users/kuba/ijcai_ski/ray_results/test\",\n  \"_orig_experiment_dir_name\": \"test\",\n  \"_local_experiment_path\": \"/Users/kuba/ijcai_ski/ray_results/test\",\n  \"_remote_experiment_path\": null,\n  \"config\": {\n    \"Gradient_LR\": 0.046595608036283306,\n    \"Gradient_Momentum\": 0.9785808236079405,\n    \"Gradient_Steps\": 240,\n    \"Threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308020000000000000094869452942e\"\n    },\n    \"merge_threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b0074946243080a0000000000000094869452942e\"\n    },\n    \"number_of_points\": 1000,\n    \"outliers_multiplier\": 3.4893880913200874\n  },\n  \"_Trial__unresolved_config\": {\n    \"Gradient_LR\": 0.046595608036283306,\n    \"Gradient_Momentum\": 0.9785808236079405,\n    \"Gradient_Steps\": 240,\n    \"Threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308020000000000000094869452942e\"\n    },\n    \"merge_threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b0074946243080a0000000000000094869452942e\"\n    },\n    \"number_of_points\": 1000,\n    \"outliers_multiplier\": 3.4893880913200874\n  },\n  \"evaluated_params\": {\n    \"Gradient_LR\": 0.046595608036283306,\n    \"Gradient_Momentum\": 0.9785808236079405,\n    \"Gradient_Steps\": 240,\n    \"Threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308020000000000000094869452942e\"\n    },\n    \"merge_threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b0074946243080a0000000000000094869452942e\"\n    },\n    \"number_of_points\": 1000,\n    \"outliers_multiplier\": 3.4893880913200874\n  },\n  \"experiment_tag\": \"13_Gradient_LR=0.0466,Gradient_Momentum=0.9786,Gradient_Steps=240,Threshold=2,merge_threshold=10,number_of_points=1000,outliers_multiplier=3.4894\",\n  \"location\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059546000000000000008c197261792e74756e652e6578706572696d656e742e747269616c948c095f4c6f636174696f6e9493942981947d94288c08686f73746e616d65944e8c03706964944e75622e\"\n  },\n  \"stopping_criterion\": {},\n  \"_setup_default_resource\": true,\n  \"_default_placement_group_factory\": \"800595aa000000000000008c237261792e74756e652e657865637574696f6e2e706c6163656d656e745f67726f757073948c15506c6163656d656e7447726f7570466163746f72799493942981947d94288c085f62756e646c6573945d947d948c0343505594473ff000000000000073618c155f686561645f62756e646c655f69735f656d70747994898c095f7374726174656779948c045041434b948c055f6172677394298c075f6b7761726773947d9475622e\",\n  \"placement_group_factory\": \"800595aa000000000000008c237261792e74756e652e657865637574696f6e2e706c6163656d656e745f67726f757073948c15506c6163656d656e7447726f7570466163746f72799493942981947d94288c085f62756e646c6573945d947d948c0343505594473ff000000000000073618c155f686561645f62756e646c655f69735f656d70747994898c095f7374726174656779948c045041434b948c055f6172677394298c075f6b7761726773947d9475622e\",\n  \"log_to_file\": [\n    null,\n    null\n  ],\n  \"max_failures\": 0,\n  \"_last_result\": {\n    \"score\": 0.9058575231193678,\n    \"time_this_iter_s\": 20.71266508102417,\n    \"done\": true,\n    \"training_iteration\": 1,\n    \"trial_id\": \"8b69e246\",\n    \"date\": \"2025-01-23_02-40-08\",\n    \"timestamp\": 1737596408,\n    \"time_total_s\": 20.71266508102417,\n    \"pid\": 30525,\n    \"hostname\": \"MacBook-Pro.ds19.agh.edu.pl\",\n    \"node_ip\": \"127.0.0.1\",\n    \"config\": {\n      \"Gradient_LR\": 0.046595608036283306,\n      \"Gradient_Momentum\": 0.9785808236079405,\n      \"Gradient_Steps\": 240,\n      \"Threshold\": 2,\n      \"merge_threshold\": 10,\n      \"number_of_points\": 1000,\n      \"outliers_multiplier\": 3.4893880913200874\n    },\n    \"time_since_restore\": 20.71266508102417,\n    \"iterations_since_restore\": 1,\n    \"experiment_tag\": \"13_Gradient_LR=0.0466,Gradient_Momentum=0.9786,Gradient_Steps=240,Threshold=2,merge_threshold=10,number_of_points=1000,outliers_multiplier=3.4894\"\n  },\n  \"_default_result_or_future\": null,\n  \"last_update_time\": 1737596408.692659,\n  \"metric_analysis\": {\n    \"score\": {\n      \"max\": 0.9058575231193678,\n      \"min\": 0.9058575231193678,\n      \"avg\": 0.9058575231193678,\n      \"last\": 0.9058575231193678,\n      \"last-5-avg\": 0.9058575231193678,\n      \"last-10-avg\": 0.9058575231193678\n    },\n    \"time_this_iter_s\": {\n      \"max\": 20.71266508102417,\n      \"min\": 20.71266508102417,\n      \"avg\": 20.71266508102417,\n      \"last\": 20.71266508102417,\n      \"last-5-avg\": 20.71266508102417,\n      \"last-10-avg\": 20.71266508102417\n    },\n    \"done\": {\n      \"max\": false,\n      \"min\": false,\n      \"avg\": false,\n      \"last\": false,\n      \"last-5-avg\": false,\n      \"last-10-avg\": false\n    },\n    \"training_iteration\": {\n      \"max\": 1,\n      \"min\": 1,\n      \"avg\": 1,\n      \"last\": 1,\n      \"last-5-avg\": 1,\n      \"last-10-avg\": 1\n    },\n    \"time_total_s\": {\n      \"max\": 20.71266508102417,\n      \"min\": 20.71266508102417,\n      \"avg\": 20.71266508102417,\n      \"last\": 20.71266508102417,\n      \"last-5-avg\": 20.71266508102417,\n      \"last-10-avg\": 20.71266508102417\n    },\n    \"time_since_restore\": {\n      \"max\": 20.71266508102417,\n      \"min\": 20.71266508102417,\n      \"avg\": 20.71266508102417,\n      \"last\": 20.71266508102417,\n      \"last-5-avg\": 20.71266508102417,\n      \"last-10-avg\": 20.71266508102417\n    },\n    \"iterations_since_restore\": {\n      \"max\": 1,\n      \"min\": 1,\n      \"avg\": 1,\n      \"last\": 1,\n      \"last-5-avg\": 1,\n      \"last-10-avg\": 1\n    }\n  },\n  \"n_steps\": [\n    5,\n    10\n  ],\n  \"metric_n_steps\": {\n    \"score\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059589000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452948c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02663894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308934394eac8fcec3f9486945294612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059589000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452948c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02663894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308934394eac8fcec3f9486945294612e\"\n      }\n    },\n    \"time_this_iter_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294474034b67138000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294474034b67138000000612e\"\n      }\n    },\n    \"done\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059522000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529489612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059522000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529489612e\"\n      }\n    },\n    \"training_iteration\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b01612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b01612e\"\n      }\n    },\n    \"time_total_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294474034b67138000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294474034b67138000000612e\"\n      }\n    },\n    \"time_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294474034b67138000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294474034b67138000000612e\"\n      }\n    },\n    \"iterations_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b01612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b01612e\"\n      }\n    }\n  },\n  \"export_formats\": [],\n  \"status\": \"TERMINATED\",\n  \"start_time\": 1737596387.970442,\n  \"relative_logdir\": \"lambda_8b69e246_13_Gradient_LR=0.0466,Gradient_Momentum=0.9786,Gradient_Steps=240,Threshold=2,merge_threshold=10,number_of_points=_2025-01-23_02-39-47\",\n  \"runner\": null,\n  \"last_debug\": 0,\n  \"error_filename\": null,\n  \"pickled_error_filename\": null,\n  \"trial_name_creator\": null,\n  \"trial_dirname_creator\": null,\n  \"custom_trial_name\": null,\n  \"custom_dirname\": null,\n  \"experiment_dir_name\": \"test\",\n  \"saving_to\": null,\n  \"checkpoint_config\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"800595ff000000000000008c0e7261792e6169722e636f6e666967948c10436865636b706f696e74436f6e6669679493942981947d94288c0b6e756d5f746f5f6b656570944b038c1a636865636b706f696e745f73636f72655f617474726962757465948c12747261696e696e675f697465726174696f6e948c16636865636b706f696e745f73636f72655f6f72646572948c036d6178948c14636865636b706f696e745f6672657175656e6379944b008c11636865636b706f696e745f61745f656e64944e8c1a5f636865636b706f696e745f6b6565705f616c6c5f72616e6b7394898c1f5f636865636b706f696e745f75706c6f61645f66726f6d5f776f726b657273948975622e\"\n  },\n  \"checkpoint_manager\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"800595d2020000000000008c257261792e74756e652e657865637574696f6e2e636865636b706f696e745f6d616e61676572948c125f436865636b706f696e744d616e616765729493942981947d94288c145f636865636b706f696e745f7374726174656779948c0e7261792e6169722e636f6e666967948c10436865636b706f696e74436f6e6669679493942981947d94288c0b6e756d5f746f5f6b656570944b038c1a636865636b706f696e745f73636f72655f617474726962757465948c12747261696e696e675f697465726174696f6e948c16636865636b706f696e745f73636f72655f6f72646572948c036d6178948c14636865636b706f696e745f6672657175656e6379944b008c11636865636b706f696e745f61745f656e64944e8c1a5f636865636b706f696e745f6b6565705f616c6c5f72616e6b7394898c1f5f636865636b706f696e745f75706c6f61645f66726f6d5f776f726b657273948975628c155f6c61746573745f636865636b706f696e745f6964944b008c1a5f746f705f7065727369737465645f636865636b706f696e7473945d948c1a5f626573745f7065727369737465645f636865636b706f696e74944e8c1c5f6c61746573745f7065727369737465645f636865636b706f696e74944e8c195f6c61746573745f6d656d6f72795f636865636b706f696e74948c247261792e6169722e5f696e7465726e616c2e636865636b706f696e745f6d616e61676572948c125f547261636b6564436865636b706f696e749493942981947d94288c0b6469725f6f725f64617461944e8c026964944affffffff8c0c73746f726167655f6d6f646594681a8c11436865636b706f696e7453746f726167659493944b01859452948c0472616e6b944b008c076d657472696373947d948c076e6f64655f6970944e8c155f6d6f76655f696e73746561645f6f665f636f7079948875628c185f636865636b706f696e74735f746f5f636c65616e5f7570948f9475622e\"\n  },\n  \"restore_path\": null,\n  \"restoring_from\": null,\n  \"num_failures\": 0,\n  \"num_restore_failures\": 0,\n  \"results\": \"80054e2e\",\n  \"best_result\": \"80054e2e\",\n  \"param_config\": \"80054e2e\",\n  \"extra_arg\": \"80054e2e\",\n  \"_state_json\": null,\n  \"_state_valid\": false,\n  \"_resources\": \"80054e2e\"\n}",
    "{\n  \"stub\": false,\n  \"trainable_name\": \"lambda\",\n  \"trial_id\": \"6ee6d0ce\",\n  \"sync_config\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059593000000000000008c0f7261792e74756e652e73796e636572948c0a53796e63436f6e6669679493942981947d94288c0a75706c6f61645f646972944e8c0673796e636572948c046175746f948c0b73796e635f706572696f64944d2c018c0c73796e635f74696d656f7574944d08078c0e73796e635f61727469666163747394888c1273796e635f6f6e5f636865636b706f696e74948875622e\"\n  },\n  \"_orig_experiment_path\": \"/Users/kuba/ijcai_ski/ray_results/test\",\n  \"_orig_experiment_dir_name\": \"test\",\n  \"_local_experiment_path\": \"/Users/kuba/ijcai_ski/ray_results/test\",\n  \"_remote_experiment_path\": null,\n  \"config\": {\n    \"Gradient_LR\": 0.01058750328726178,\n    \"Gradient_Momentum\": 0.9420417694453609,\n    \"Gradient_Steps\": 390,\n    \"Threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308060000000000000094869452942e\"\n    },\n    \"merge_threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308130000000000000094869452942e\"\n    },\n    \"number_of_points\": 400,\n    \"outliers_multiplier\": 2.0570725205246214\n  },\n  \"_Trial__unresolved_config\": {\n    \"Gradient_LR\": 0.01058750328726178,\n    \"Gradient_Momentum\": 0.9420417694453609,\n    \"Gradient_Steps\": 390,\n    \"Threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308060000000000000094869452942e\"\n    },\n    \"merge_threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308130000000000000094869452942e\"\n    },\n    \"number_of_points\": 400,\n    \"outliers_multiplier\": 2.0570725205246214\n  },\n  \"evaluated_params\": {\n    \"Gradient_LR\": 0.01058750328726178,\n    \"Gradient_Momentum\": 0.9420417694453609,\n    \"Gradient_Steps\": 390,\n    \"Threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308060000000000000094869452942e\"\n    },\n    \"merge_threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308130000000000000094869452942e\"\n    },\n    \"number_of_points\": 400,\n    \"outliers_multiplier\": 2.0570725205246214\n  },\n  \"experiment_tag\": \"14_Gradient_LR=0.0106,Gradient_Momentum=0.9420,Gradient_Steps=390,Threshold=6,merge_threshold=19,number_of_points=400,outliers_multiplier=2.0571\",\n  \"location\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059546000000000000008c197261792e74756e652e6578706572696d656e742e747269616c948c095f4c6f636174696f6e9493942981947d94288c08686f73746e616d65944e8c03706964944e75622e\"\n  },\n  \"stopping_criterion\": {},\n  \"_setup_default_resource\": true,\n  \"_default_placement_group_factory\": \"800595aa000000000000008c237261792e74756e652e657865637574696f6e2e706c6163656d656e745f67726f757073948c15506c6163656d656e7447726f7570466163746f72799493942981947d94288c085f62756e646c6573945d947d948c0343505594473ff000000000000073618c155f686561645f62756e646c655f69735f656d70747994898c095f7374726174656779948c045041434b948c055f6172677394298c075f6b7761726773947d9475622e\",\n  \"placement_group_factory\": \"800595aa000000000000008c237261792e74756e652e657865637574696f6e2e706c6163656d656e745f67726f757073948c15506c6163656d656e7447726f7570466163746f72799493942981947d94288c085f62756e646c6573945d947d948c0343505594473ff000000000000073618c155f686561645f62756e646c655f69735f656d70747994898c095f7374726174656779948c045041434b948c055f6172677394298c075f6b7761726773947d9475622e\",\n  \"log_to_file\": [\n    null,\n    null\n  ],\n  \"max_failures\": 0,\n  \"_last_result\": {\n    \"score\": 0.8659571907765986,\n    \"time_this_iter_s\": 21.971745014190674,\n    \"done\": true,\n    \"training_iteration\": 1,\n    \"trial_id\": \"6ee6d0ce\",\n    \"date\": \"2025-01-23_02-40-11\",\n    \"timestamp\": 1737596411,\n    \"time_total_s\": 21.971745014190674,\n    \"pid\": 30590,\n    \"hostname\": \"MacBook-Pro.ds19.agh.edu.pl\",\n    \"node_ip\": \"127.0.0.1\",\n    \"config\": {\n      \"Gradient_LR\": 0.01058750328726178,\n      \"Gradient_Momentum\": 0.9420417694453609,\n      \"Gradient_Steps\": 390,\n      \"Threshold\": 6,\n      \"merge_threshold\": 19,\n      \"number_of_points\": 400,\n      \"outliers_multiplier\": 2.0570725205246214\n    },\n    \"time_since_restore\": 21.971745014190674,\n    \"iterations_since_restore\": 1,\n    \"experiment_tag\": \"14_Gradient_LR=0.0106,Gradient_Momentum=0.9420,Gradient_Steps=390,Threshold=6,merge_threshold=19,number_of_points=400,outliers_multiplier=2.0571\"\n  },\n  \"_default_result_or_future\": null,\n  \"last_update_time\": 1737596412.004009,\n  \"metric_analysis\": {\n    \"score\": {\n      \"max\": 0.8659571907765986,\n      \"min\": 0.8659571907765986,\n      \"avg\": 0.8659571907765986,\n      \"last\": 0.8659571907765986,\n      \"last-5-avg\": 0.8659571907765986,\n      \"last-10-avg\": 0.8659571907765986\n    },\n    \"time_this_iter_s\": {\n      \"max\": 21.971745014190674,\n      \"min\": 21.971745014190674,\n      \"avg\": 21.971745014190674,\n      \"last\": 21.971745014190674,\n      \"last-5-avg\": 21.971745014190674,\n      \"last-10-avg\": 21.971745014190674\n    },\n    \"done\": {\n      \"max\": false,\n      \"min\": false,\n      \"avg\": false,\n      \"last\": false,\n      \"last-5-avg\": false,\n      \"last-10-avg\": false\n    },\n    \"training_iteration\": {\n      \"max\": 1,\n      \"min\": 1,\n      \"avg\": 1,\n      \"last\": 1,\n      \"last-5-avg\": 1,\n      \"last-10-avg\": 1\n    },\n    \"time_total_s\": {\n      \"max\": 21.971745014190674,\n      \"min\": 21.971745014190674,\n      \"avg\": 21.971745014190674,\n      \"last\": 21.971745014190674,\n      \"last-5-avg\": 21.971745014190674,\n      \"last-10-avg\": 21.971745014190674\n    },\n    \"time_since_restore\": {\n      \"max\": 21.971745014190674,\n      \"min\": 21.971745014190674,\n      \"avg\": 21.971745014190674,\n      \"last\": 21.971745014190674,\n      \"last-5-avg\": 21.971745014190674,\n      \"last-10-avg\": 21.971745014190674\n    },\n    \"iterations_since_restore\": {\n      \"max\": 1,\n      \"min\": 1,\n      \"avg\": 1,\n      \"last\": 1,\n      \"last-5-avg\": 1,\n      \"last-10-avg\": 1\n    }\n  },\n  \"n_steps\": [\n    5,\n    10\n  ],\n  \"metric_n_steps\": {\n    \"score\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059589000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452948c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02663894898887945294284b038c013c944e4e4e4affffffff4affffffff4b00749462430886e3c3daebb5eb3f9486945294612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059589000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452948c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02663894898887945294284b038c013c944e4e4e4affffffff4affffffff4b00749462430886e3c3daebb5eb3f9486945294612e\"\n      }\n    },\n    \"time_this_iter_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294474035f8c448000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294474035f8c448000000612e\"\n      }\n    },\n    \"done\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059522000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529489612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059522000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529489612e\"\n      }\n    },\n    \"training_iteration\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b01612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b01612e\"\n      }\n    },\n    \"time_total_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294474035f8c448000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294474035f8c448000000612e\"\n      }\n    },\n    \"time_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294474035f8c448000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294474035f8c448000000612e\"\n      }\n    },\n    \"iterations_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b01612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b01612e\"\n      }\n    }\n  },\n  \"export_formats\": [],\n  \"status\": \"TERMINATED\",\n  \"start_time\": 1737596390.027116,\n  \"relative_logdir\": \"lambda_6ee6d0ce_14_Gradient_LR=0.0106,Gradient_Momentum=0.9420,Gradient_Steps=390,Threshold=6,merge_threshold=19,number_of_points=_2025-01-23_02-39-48\",\n  \"runner\": null,\n  \"last_debug\": 0,\n  \"error_filename\": null,\n  \"pickled_error_filename\": null,\n  \"trial_name_creator\": null,\n  \"trial_dirname_creator\": null,\n  \"custom_trial_name\": null,\n  \"custom_dirname\": null,\n  \"experiment_dir_name\": \"test\",\n  \"saving_to\": null,\n  \"checkpoint_config\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"800595ff000000000000008c0e7261792e6169722e636f6e666967948c10436865636b706f696e74436f6e6669679493942981947d94288c0b6e756d5f746f5f6b656570944b038c1a636865636b706f696e745f73636f72655f617474726962757465948c12747261696e696e675f697465726174696f6e948c16636865636b706f696e745f73636f72655f6f72646572948c036d6178948c14636865636b706f696e745f6672657175656e6379944b008c11636865636b706f696e745f61745f656e64944e8c1a5f636865636b706f696e745f6b6565705f616c6c5f72616e6b7394898c1f5f636865636b706f696e745f75706c6f61645f66726f6d5f776f726b657273948975622e\"\n  },\n  \"checkpoint_manager\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"800595d2020000000000008c257261792e74756e652e657865637574696f6e2e636865636b706f696e745f6d616e61676572948c125f436865636b706f696e744d616e616765729493942981947d94288c145f636865636b706f696e745f7374726174656779948c0e7261792e6169722e636f6e666967948c10436865636b706f696e74436f6e6669679493942981947d94288c0b6e756d5f746f5f6b656570944b038c1a636865636b706f696e745f73636f72655f617474726962757465948c12747261696e696e675f697465726174696f6e948c16636865636b706f696e745f73636f72655f6f72646572948c036d6178948c14636865636b706f696e745f6672657175656e6379944b008c11636865636b706f696e745f61745f656e64944e8c1a5f636865636b706f696e745f6b6565705f616c6c5f72616e6b7394898c1f5f636865636b706f696e745f75706c6f61645f66726f6d5f776f726b657273948975628c155f6c61746573745f636865636b706f696e745f6964944b008c1a5f746f705f7065727369737465645f636865636b706f696e7473945d948c1a5f626573745f7065727369737465645f636865636b706f696e74944e8c1c5f6c61746573745f7065727369737465645f636865636b706f696e74944e8c195f6c61746573745f6d656d6f72795f636865636b706f696e74948c247261792e6169722e5f696e7465726e616c2e636865636b706f696e745f6d616e61676572948c125f547261636b6564436865636b706f696e749493942981947d94288c0b6469725f6f725f64617461944e8c026964944affffffff8c0c73746f726167655f6d6f646594681a8c11436865636b706f696e7453746f726167659493944b01859452948c0472616e6b944b008c076d657472696373947d948c076e6f64655f6970944e8c155f6d6f76655f696e73746561645f6f665f636f7079948875628c185f636865636b706f696e74735f746f5f636c65616e5f7570948f9475622e\"\n  },\n  \"restore_path\": null,\n  \"restoring_from\": null,\n  \"num_failures\": 0,\n  \"num_restore_failures\": 0,\n  \"results\": \"80054e2e\",\n  \"best_result\": \"80054e2e\",\n  \"param_config\": \"80054e2e\",\n  \"extra_arg\": \"80054e2e\",\n  \"_state_json\": null,\n  \"_state_valid\": false,\n  \"_resources\": \"80054e2e\"\n}",
    "{\n  \"stub\": false,\n  \"trainable_name\": \"lambda\",\n  \"trial_id\": \"95409baa\",\n  \"sync_config\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059593000000000000008c0f7261792e74756e652e73796e636572948c0a53796e63436f6e6669679493942981947d94288c0a75706c6f61645f646972944e8c0673796e636572948c046175746f948c0b73796e635f706572696f64944d2c018c0c73796e635f74696d656f7574944d08078c0e73796e635f61727469666163747394888c1273796e635f6f6e5f636865636b706f696e74948875622e\"\n  },\n  \"_orig_experiment_path\": \"/Users/kuba/ijcai_ski/ray_results/test\",\n  \"_orig_experiment_dir_name\": \"test\",\n  \"_local_experiment_path\": \"/Users/kuba/ijcai_ski/ray_results/test\",\n  \"_remote_experiment_path\": null,\n  \"config\": {\n    \"Gradient_LR\": 0.01499354962818902,\n    \"Gradient_Momentum\": 0.9894154626381019,\n    \"Gradient_Steps\": 260,\n    \"Threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b0074946243080e0000000000000094869452942e\"\n    },\n    \"merge_threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308050000000000000094869452942e\"\n    },\n    \"number_of_points\": 1000,\n    \"outliers_multiplier\": 3.2409092043086365\n  },\n  \"_Trial__unresolved_config\": {\n    \"Gradient_LR\": 0.01499354962818902,\n    \"Gradient_Momentum\": 0.9894154626381019,\n    \"Gradient_Steps\": 260,\n    \"Threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b0074946243080e0000000000000094869452942e\"\n    },\n    \"merge_threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308050000000000000094869452942e\"\n    },\n    \"number_of_points\": 1000,\n    \"outliers_multiplier\": 3.2409092043086365\n  },\n  \"evaluated_params\": {\n    \"Gradient_LR\": 0.01499354962818902,\n    \"Gradient_Momentum\": 0.9894154626381019,\n    \"Gradient_Steps\": 260,\n    \"Threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b0074946243080e0000000000000094869452942e\"\n    },\n    \"merge_threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308050000000000000094869452942e\"\n    },\n    \"number_of_points\": 1000,\n    \"outliers_multiplier\": 3.2409092043086365\n  },\n  \"experiment_tag\": \"17_Gradient_LR=0.0150,Gradient_Momentum=0.9894,Gradient_Steps=260,Threshold=14,merge_threshold=5,number_of_points=1000,outliers_multiplier=3.2409\",\n  \"location\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059546000000000000008c197261792e74756e652e6578706572696d656e742e747269616c948c095f4c6f636174696f6e9493942981947d94288c08686f73746e616d65944e8c03706964944e75622e\"\n  },\n  \"stopping_criterion\": {},\n  \"_setup_default_resource\": true,\n  \"_default_placement_group_factory\": \"800595aa000000000000008c237261792e74756e652e657865637574696f6e2e706c6163656d656e745f67726f757073948c15506c6163656d656e7447726f7570466163746f72799493942981947d94288c085f62756e646c6573945d947d948c0343505594473ff000000000000073618c155f686561645f62756e646c655f69735f656d70747994898c095f7374726174656779948c045041434b948c055f6172677394298c075f6b7761726773947d9475622e\",\n  \"placement_group_factory\": \"800595aa000000000000008c237261792e74756e652e657865637574696f6e2e706c6163656d656e745f67726f757073948c15506c6163656d656e7447726f7570466163746f72799493942981947d94288c085f62756e646c6573945d947d948c0343505594473ff000000000000073618c155f686561645f62756e646c655f69735f656d70747994898c095f7374726174656779948c045041434b948c055f6172677394298c075f6b7761726773947d9475622e\",\n  \"log_to_file\": [\n    null,\n    null\n  ],\n  \"max_failures\": 0,\n  \"_last_result\": {\n    \"score\": 0.862471005490965,\n    \"time_this_iter_s\": 16.522397994995117,\n    \"done\": true,\n    \"training_iteration\": 1,\n    \"trial_id\": \"95409baa\",\n    \"date\": \"2025-01-23_02-40-21\",\n    \"timestamp\": 1737596421,\n    \"time_total_s\": 16.522397994995117,\n    \"pid\": 30798,\n    \"hostname\": \"MacBook-Pro.ds19.agh.edu.pl\",\n    \"node_ip\": \"127.0.0.1\",\n    \"config\": {\n      \"Gradient_LR\": 0.01499354962818902,\n      \"Gradient_Momentum\": 0.9894154626381019,\n      \"Gradient_Steps\": 260,\n      \"Threshold\": 14,\n      \"merge_threshold\": 5,\n      \"number_of_points\": 1000,\n      \"outliers_multiplier\": 3.2409092043086365\n    },\n    \"time_since_restore\": 16.522397994995117,\n    \"iterations_since_restore\": 1,\n    \"experiment_tag\": \"17_Gradient_LR=0.0150,Gradient_Momentum=0.9894,Gradient_Steps=260,Threshold=14,merge_threshold=5,number_of_points=1000,outliers_multiplier=3.2409\"\n  },\n  \"_default_result_or_future\": null,\n  \"last_update_time\": 1737596421.657682,\n  \"metric_analysis\": {\n    \"score\": {\n      \"max\": 0.862471005490965,\n      \"min\": 0.862471005490965,\n      \"avg\": 0.862471005490965,\n      \"last\": 0.862471005490965,\n      \"last-5-avg\": 0.862471005490965,\n      \"last-10-avg\": 0.862471005490965\n    },\n    \"time_this_iter_s\": {\n      \"max\": 16.522397994995117,\n      \"min\": 16.522397994995117,\n      \"avg\": 16.522397994995117,\n      \"last\": 16.522397994995117,\n      \"last-5-avg\": 16.522397994995117,\n      \"last-10-avg\": 16.522397994995117\n    },\n    \"done\": {\n      \"max\": false,\n      \"min\": false,\n      \"avg\": false,\n      \"last\": false,\n      \"last-5-avg\": false,\n      \"last-10-avg\": false\n    },\n    \"training_iteration\": {\n      \"max\": 1,\n      \"min\": 1,\n      \"avg\": 1,\n      \"last\": 1,\n      \"last-5-avg\": 1,\n      \"last-10-avg\": 1\n    },\n    \"time_total_s\": {\n      \"max\": 16.522397994995117,\n      \"min\": 16.522397994995117,\n      \"avg\": 16.522397994995117,\n      \"last\": 16.522397994995117,\n      \"last-5-avg\": 16.522397994995117,\n      \"last-10-avg\": 16.522397994995117\n    },\n    \"time_since_restore\": {\n      \"max\": 16.522397994995117,\n      \"min\": 16.522397994995117,\n      \"avg\": 16.522397994995117,\n      \"last\": 16.522397994995117,\n      \"last-5-avg\": 16.522397994995117,\n      \"last-10-avg\": 16.522397994995117\n    },\n    \"iterations_since_restore\": {\n      \"max\": 1,\n      \"min\": 1,\n      \"avg\": 1,\n      \"last\": 1,\n      \"last-5-avg\": 1,\n      \"last-10-avg\": 1\n    }\n  },\n  \"n_steps\": [\n    5,\n    10\n  ],\n  \"metric_n_steps\": {\n    \"score\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059589000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452948c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02663894898887945294284b038c013c944e4e4e4affffffff4affffffff4b0074946243082e9f4acb5c99eb3f9486945294612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059589000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452948c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02663894898887945294284b038c013c944e4e4e4affffffff4affffffff4b0074946243082e9f4acb5c99eb3f9486945294612e\"\n      }\n    },\n    \"time_this_iter_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529447403085bbe0000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529447403085bbe0000000612e\"\n      }\n    },\n    \"done\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059522000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529489612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059522000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529489612e\"\n      }\n    },\n    \"training_iteration\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b01612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b01612e\"\n      }\n    },\n    \"time_total_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529447403085bbe0000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529447403085bbe0000000612e\"\n      }\n    },\n    \"time_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529447403085bbe0000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529447403085bbe0000000612e\"\n      }\n    },\n    \"iterations_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b01612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b01612e\"\n      }\n    }\n  },\n  \"export_formats\": [],\n  \"status\": \"TERMINATED\",\n  \"start_time\": 1737596405.123307,\n  \"relative_logdir\": \"lambda_95409baa_17_Gradient_LR=0.0150,Gradient_Momentum=0.9894,Gradient_Steps=260,Threshold=14,merge_threshold=5,number_of_points=_2025-01-23_02-39-55\",\n  \"runner\": null,\n  \"last_debug\": 0,\n  \"error_filename\": null,\n  \"pickled_error_filename\": null,\n  \"trial_name_creator\": null,\n  \"trial_dirname_creator\": null,\n  \"custom_trial_name\": null,\n  \"custom_dirname\": null,\n  \"experiment_dir_name\": \"test\",\n  \"saving_to\": null,\n  \"checkpoint_config\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"800595ff000000000000008c0e7261792e6169722e636f6e666967948c10436865636b706f696e74436f6e6669679493942981947d94288c0b6e756d5f746f5f6b656570944b038c1a636865636b706f696e745f73636f72655f617474726962757465948c12747261696e696e675f697465726174696f6e948c16636865636b706f696e745f73636f72655f6f72646572948c036d6178948c14636865636b706f696e745f6672657175656e6379944b008c11636865636b706f696e745f61745f656e64944e8c1a5f636865636b706f696e745f6b6565705f616c6c5f72616e6b7394898c1f5f636865636b706f696e745f75706c6f61645f66726f6d5f776f726b657273948975622e\"\n  },\n  \"checkpoint_manager\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"800595d2020000000000008c257261792e74756e652e657865637574696f6e2e636865636b706f696e745f6d616e61676572948c125f436865636b706f696e744d616e616765729493942981947d94288c145f636865636b706f696e745f7374726174656779948c0e7261792e6169722e636f6e666967948c10436865636b706f696e74436f6e6669679493942981947d94288c0b6e756d5f746f5f6b656570944b038c1a636865636b706f696e745f73636f72655f617474726962757465948c12747261696e696e675f697465726174696f6e948c16636865636b706f696e745f73636f72655f6f72646572948c036d6178948c14636865636b706f696e745f6672657175656e6379944b008c11636865636b706f696e745f61745f656e64944e8c1a5f636865636b706f696e745f6b6565705f616c6c5f72616e6b7394898c1f5f636865636b706f696e745f75706c6f61645f66726f6d5f776f726b657273948975628c155f6c61746573745f636865636b706f696e745f6964944b008c1a5f746f705f7065727369737465645f636865636b706f696e7473945d948c1a5f626573745f7065727369737465645f636865636b706f696e74944e8c1c5f6c61746573745f7065727369737465645f636865636b706f696e74944e8c195f6c61746573745f6d656d6f72795f636865636b706f696e74948c247261792e6169722e5f696e7465726e616c2e636865636b706f696e745f6d616e61676572948c125f547261636b6564436865636b706f696e749493942981947d94288c0b6469725f6f725f64617461944e8c026964944affffffff8c0c73746f726167655f6d6f646594681a8c11436865636b706f696e7453746f726167659493944b01859452948c0472616e6b944b008c076d657472696373947d948c076e6f64655f6970944e8c155f6d6f76655f696e73746561645f6f665f636f7079948875628c185f636865636b706f696e74735f746f5f636c65616e5f7570948f9475622e\"\n  },\n  \"restore_path\": null,\n  \"restoring_from\": null,\n  \"num_failures\": 0,\n  \"num_restore_failures\": 0,\n  \"results\": \"80054e2e\",\n  \"best_result\": \"80054e2e\",\n  \"param_config\": \"80054e2e\",\n  \"extra_arg\": \"80054e2e\",\n  \"_state_json\": null,\n  \"_state_valid\": false,\n  \"_resources\": \"80054e2e\"\n}",
    "{\n  \"stub\": false,\n  \"trainable_name\": \"lambda\",\n  \"trial_id\": \"381ab233\",\n  \"sync_config\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059593000000000000008c0f7261792e74756e652e73796e636572948c0a53796e63436f6e6669679493942981947d94288c0a75706c6f61645f646972944e8c0673796e636572948c046175746f948c0b73796e635f706572696f64944d2c018c0c73796e635f74696d656f7574944d08078c0e73796e635f61727469666163747394888c1273796e635f6f6e5f636865636b706f696e74948875622e\"\n  },\n  \"_orig_experiment_path\": \"/Users/kuba/ijcai_ski/ray_results/test\",\n  \"_orig_experiment_dir_name\": \"test\",\n  \"_local_experiment_path\": \"/Users/kuba/ijcai_ski/ray_results/test\",\n  \"_remote_experiment_path\": null,\n  \"config\": {\n    \"Gradient_LR\": 0.03848026443089728,\n    \"Gradient_Momentum\": 0.9964629441681798,\n    \"Gradient_Steps\": 380,\n    \"Threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308030000000000000094869452942e\"\n    },\n    \"merge_threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b0074946243080d0000000000000094869452942e\"\n    },\n    \"number_of_points\": 900,\n    \"outliers_multiplier\": 3.8529531750447856\n  },\n  \"_Trial__unresolved_config\": {\n    \"Gradient_LR\": 0.03848026443089728,\n    \"Gradient_Momentum\": 0.9964629441681798,\n    \"Gradient_Steps\": 380,\n    \"Threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308030000000000000094869452942e\"\n    },\n    \"merge_threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b0074946243080d0000000000000094869452942e\"\n    },\n    \"number_of_points\": 900,\n    \"outliers_multiplier\": 3.8529531750447856\n  },\n  \"evaluated_params\": {\n    \"Gradient_LR\": 0.03848026443089728,\n    \"Gradient_Momentum\": 0.9964629441681798,\n    \"Gradient_Steps\": 380,\n    \"Threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308030000000000000094869452942e\"\n    },\n    \"merge_threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b0074946243080d0000000000000094869452942e\"\n    },\n    \"number_of_points\": 900,\n    \"outliers_multiplier\": 3.8529531750447856\n  },\n  \"experiment_tag\": \"21_Gradient_LR=0.0385,Gradient_Momentum=0.9965,Gradient_Steps=380,Threshold=3,merge_threshold=13,number_of_points=900,outliers_multiplier=3.8530\",\n  \"location\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059546000000000000008c197261792e74756e652e6578706572696d656e742e747269616c948c095f4c6f636174696f6e9493942981947d94288c08686f73746e616d65944e8c03706964944e75622e\"\n  },\n  \"stopping_criterion\": {},\n  \"_setup_default_resource\": true,\n  \"_default_placement_group_factory\": \"800595aa000000000000008c237261792e74756e652e657865637574696f6e2e706c6163656d656e745f67726f757073948c15506c6163656d656e7447726f7570466163746f72799493942981947d94288c085f62756e646c6573945d947d948c0343505594473ff000000000000073618c155f686561645f62756e646c655f69735f656d70747994898c095f7374726174656779948c045041434b948c055f6172677394298c075f6b7761726773947d9475622e\",\n  \"placement_group_factory\": \"800595aa000000000000008c237261792e74756e652e657865637574696f6e2e706c6163656d656e745f67726f757073948c15506c6163656d656e7447726f7570466163746f72799493942981947d94288c085f62756e646c6573945d947d948c0343505594473ff000000000000073618c155f686561645f62756e646c655f69735f656d70747994898c095f7374726174656779948c045041434b948c055f6172677394298c075f6b7761726773947d9475622e\",\n  \"log_to_file\": [\n    null,\n    null\n  ],\n  \"max_failures\": 0,\n  \"_last_result\": {\n    \"score\": 0.7727149092644775,\n    \"time_this_iter_s\": 15.376286029815674,\n    \"done\": true,\n    \"training_iteration\": 1,\n    \"trial_id\": \"381ab233\",\n    \"date\": \"2025-01-23_02-40-25\",\n    \"timestamp\": 1737596425,\n    \"time_total_s\": 15.376286029815674,\n    \"pid\": 30619,\n    \"hostname\": \"MacBook-Pro.ds19.agh.edu.pl\",\n    \"node_ip\": \"127.0.0.1\",\n    \"config\": {\n      \"Gradient_LR\": 0.03848026443089728,\n      \"Gradient_Momentum\": 0.9964629441681798,\n      \"Gradient_Steps\": 380,\n      \"Threshold\": 3,\n      \"merge_threshold\": 13,\n      \"number_of_points\": 900,\n      \"outliers_multiplier\": 3.8529531750447856\n    },\n    \"time_since_restore\": 15.376286029815674,\n    \"iterations_since_restore\": 1,\n    \"experiment_tag\": \"21_Gradient_LR=0.0385,Gradient_Momentum=0.9965,Gradient_Steps=380,Threshold=3,merge_threshold=13,number_of_points=900,outliers_multiplier=3.8530\"\n  },\n  \"_default_result_or_future\": null,\n  \"last_update_time\": 1737596425.113052,\n  \"metric_analysis\": {\n    \"score\": {\n      \"max\": 0.7727149092644775,\n      \"min\": 0.7727149092644775,\n      \"avg\": 0.7727149092644775,\n      \"last\": 0.7727149092644775,\n      \"last-5-avg\": 0.7727149092644775,\n      \"last-10-avg\": 0.7727149092644775\n    },\n    \"time_this_iter_s\": {\n      \"max\": 15.376286029815674,\n      \"min\": 15.376286029815674,\n      \"avg\": 15.376286029815674,\n      \"last\": 15.376286029815674,\n      \"last-5-avg\": 15.376286029815674,\n      \"last-10-avg\": 15.376286029815674\n    },\n    \"done\": {\n      \"max\": false,\n      \"min\": false,\n      \"avg\": false,\n      \"last\": false,\n      \"last-5-avg\": false,\n      \"last-10-avg\": false\n    },\n    \"training_iteration\": {\n      \"max\": 1,\n      \"min\": 1,\n      \"avg\": 1,\n      \"last\": 1,\n      \"last-5-avg\": 1,\n      \"last-10-avg\": 1\n    },\n    \"time_total_s\": {\n      \"max\": 15.376286029815674,\n      \"min\": 15.376286029815674,\n      \"avg\": 15.376286029815674,\n      \"last\": 15.376286029815674,\n      \"last-5-avg\": 15.376286029815674,\n      \"last-10-avg\": 15.376286029815674\n    },\n    \"time_since_restore\": {\n      \"max\": 15.376286029815674,\n      \"min\": 15.376286029815674,\n      \"avg\": 15.376286029815674,\n      \"last\": 15.376286029815674,\n      \"last-5-avg\": 15.376286029815674,\n      \"last-10-avg\": 15.376286029815674\n    },\n    \"iterations_since_restore\": {\n      \"max\": 1,\n      \"min\": 1,\n      \"avg\": 1,\n      \"last\": 1,\n      \"last-5-avg\": 1,\n      \"last-10-avg\": 1\n    }\n  },\n  \"n_steps\": [\n    5,\n    10\n  ],\n  \"metric_n_steps\": {\n    \"score\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059589000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452948c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02663894898887945294284b038c013c944e4e4e4affffffff4affffffff4b0074946243086f850d9e14bae83f9486945294612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059589000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452948c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02663894898887945294284b038c013c944e4e4e4affffffff4affffffff4b0074946243086f850d9e14bae83f9486945294612e\"\n      }\n    },\n    \"time_this_iter_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529447402ec0a890000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529447402ec0a890000000612e\"\n      }\n    },\n    \"done\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059522000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529489612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059522000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529489612e\"\n      }\n    },\n    \"training_iteration\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b01612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b01612e\"\n      }\n    },\n    \"time_total_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529447402ec0a890000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529447402ec0a890000000612e\"\n      }\n    },\n    \"time_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529447402ec0a890000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529447402ec0a890000000612e\"\n      }\n    },\n    \"iterations_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b01612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b01612e\"\n      }\n    }\n  },\n  \"export_formats\": [],\n  \"status\": \"TERMINATED\",\n  \"start_time\": 1737596409.7283268,\n  \"relative_logdir\": \"lambda_381ab233_21_Gradient_LR=0.0385,Gradient_Momentum=0.9965,Gradient_Steps=380,Threshold=3,merge_threshold=13,number_of_points=_2025-01-23_02-40-08\",\n  \"runner\": null,\n  \"last_debug\": 0,\n  \"error_filename\": null,\n  \"pickled_error_filename\": null,\n  \"trial_name_creator\": null,\n  \"trial_dirname_creator\": null,\n  \"custom_trial_name\": null,\n  \"custom_dirname\": null,\n  \"experiment_dir_name\": \"test\",\n  \"saving_to\": null,\n  \"checkpoint_config\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"800595ff000000000000008c0e7261792e6169722e636f6e666967948c10436865636b706f696e74436f6e6669679493942981947d94288c0b6e756d5f746f5f6b656570944b038c1a636865636b706f696e745f73636f72655f617474726962757465948c12747261696e696e675f697465726174696f6e948c16636865636b706f696e745f73636f72655f6f72646572948c036d6178948c14636865636b706f696e745f6672657175656e6379944b008c11636865636b706f696e745f61745f656e64944e8c1a5f636865636b706f696e745f6b6565705f616c6c5f72616e6b7394898c1f5f636865636b706f696e745f75706c6f61645f66726f6d5f776f726b657273948975622e\"\n  },\n  \"checkpoint_manager\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"800595d2020000000000008c257261792e74756e652e657865637574696f6e2e636865636b706f696e745f6d616e61676572948c125f436865636b706f696e744d616e616765729493942981947d94288c145f636865636b706f696e745f7374726174656779948c0e7261792e6169722e636f6e666967948c10436865636b706f696e74436f6e6669679493942981947d94288c0b6e756d5f746f5f6b656570944b038c1a636865636b706f696e745f73636f72655f617474726962757465948c12747261696e696e675f697465726174696f6e948c16636865636b706f696e745f73636f72655f6f72646572948c036d6178948c14636865636b706f696e745f6672657175656e6379944b008c11636865636b706f696e745f61745f656e64944e8c1a5f636865636b706f696e745f6b6565705f616c6c5f72616e6b7394898c1f5f636865636b706f696e745f75706c6f61645f66726f6d5f776f726b657273948975628c155f6c61746573745f636865636b706f696e745f6964944b008c1a5f746f705f7065727369737465645f636865636b706f696e7473945d948c1a5f626573745f7065727369737465645f636865636b706f696e74944e8c1c5f6c61746573745f7065727369737465645f636865636b706f696e74944e8c195f6c61746573745f6d656d6f72795f636865636b706f696e74948c247261792e6169722e5f696e7465726e616c2e636865636b706f696e745f6d616e61676572948c125f547261636b6564436865636b706f696e749493942981947d94288c0b6469725f6f725f64617461944e8c026964944affffffff8c0c73746f726167655f6d6f646594681a8c11436865636b706f696e7453746f726167659493944b01859452948c0472616e6b944b008c076d657472696373947d948c076e6f64655f6970944e8c155f6d6f76655f696e73746561645f6f665f636f7079948875628c185f636865636b706f696e74735f746f5f636c65616e5f7570948f9475622e\"\n  },\n  \"restore_path\": null,\n  \"restoring_from\": null,\n  \"num_failures\": 0,\n  \"num_restore_failures\": 0,\n  \"results\": \"80054e2e\",\n  \"best_result\": \"80054e2e\",\n  \"param_config\": \"80054e2e\",\n  \"extra_arg\": \"80054e2e\",\n  \"_state_json\": null,\n  \"_state_valid\": false,\n  \"_resources\": \"80054e2e\"\n}",
    "{\n  \"stub\": false,\n  \"trainable_name\": \"lambda\",\n  \"trial_id\": \"284b6bc0\",\n  \"sync_config\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059593000000000000008c0f7261792e74756e652e73796e636572948c0a53796e63436f6e6669679493942981947d94288c0a75706c6f61645f646972944e8c0673796e636572948c046175746f948c0b73796e635f706572696f64944d2c018c0c73796e635f74696d656f7574944d08078c0e73796e635f61727469666163747394888c1273796e635f6f6e5f636865636b706f696e74948875622e\"\n  },\n  \"_orig_experiment_path\": \"/Users/kuba/ijcai_ski/ray_results/test\",\n  \"_orig_experiment_dir_name\": \"test\",\n  \"_local_experiment_path\": \"/Users/kuba/ijcai_ski/ray_results/test\",\n  \"_remote_experiment_path\": null,\n  \"config\": {\n    \"Gradient_LR\": 0.029430458230505614,\n    \"Gradient_Momentum\": 0.9852730072789317,\n    \"Gradient_Steps\": 230,\n    \"Threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b0074946243080d0000000000000094869452942e\"\n    },\n    \"merge_threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b0074946243080a0000000000000094869452942e\"\n    },\n    \"number_of_points\": 900,\n    \"outliers_multiplier\": 3.769257792932316\n  },\n  \"_Trial__unresolved_config\": {\n    \"Gradient_LR\": 0.029430458230505614,\n    \"Gradient_Momentum\": 0.9852730072789317,\n    \"Gradient_Steps\": 230,\n    \"Threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b0074946243080d0000000000000094869452942e\"\n    },\n    \"merge_threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b0074946243080a0000000000000094869452942e\"\n    },\n    \"number_of_points\": 900,\n    \"outliers_multiplier\": 3.769257792932316\n  },\n  \"evaluated_params\": {\n    \"Gradient_LR\": 0.029430458230505614,\n    \"Gradient_Momentum\": 0.9852730072789317,\n    \"Gradient_Steps\": 230,\n    \"Threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b0074946243080d0000000000000094869452942e\"\n    },\n    \"merge_threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b0074946243080a0000000000000094869452942e\"\n    },\n    \"number_of_points\": 900,\n    \"outliers_multiplier\": 3.769257792932316\n  },\n  \"experiment_tag\": \"23_Gradient_LR=0.0294,Gradient_Momentum=0.9853,Gradient_Steps=230,Threshold=13,merge_threshold=10,number_of_points=900,outliers_multiplier=3.7693\",\n  \"location\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059546000000000000008c197261792e74756e652e6578706572696d656e742e747269616c948c095f4c6f636174696f6e9493942981947d94288c08686f73746e616d65944e8c03706964944e75622e\"\n  },\n  \"stopping_criterion\": {},\n  \"_setup_default_resource\": true,\n  \"_default_placement_group_factory\": \"800595aa000000000000008c237261792e74756e652e657865637574696f6e2e706c6163656d656e745f67726f757073948c15506c6163656d656e7447726f7570466163746f72799493942981947d94288c085f62756e646c6573945d947d948c0343505594473ff000000000000073618c155f686561645f62756e646c655f69735f656d70747994898c095f7374726174656779948c045041434b948c055f6172677394298c075f6b7761726773947d9475622e\",\n  \"placement_group_factory\": \"800595aa000000000000008c237261792e74756e652e657865637574696f6e2e706c6163656d656e745f67726f757073948c15506c6163656d656e7447726f7570466163746f72799493942981947d94288c085f62756e646c6573945d947d948c0343505594473ff000000000000073618c155f686561645f62756e646c655f69735f656d70747994898c095f7374726174656779948c045041434b948c055f6172677394298c075f6b7761726773947d9475622e\",\n  \"log_to_file\": [\n    null,\n    null\n  ],\n  \"max_failures\": 0,\n  \"_last_result\": {\n    \"score\": 0.8627639029899508,\n    \"time_this_iter_s\": 13.113409996032715,\n    \"done\": true,\n    \"training_iteration\": 1,\n    \"trial_id\": \"284b6bc0\",\n    \"date\": \"2025-01-23_02-40-29\",\n    \"timestamp\": 1737596429,\n    \"time_total_s\": 13.113409996032715,\n    \"pid\": 30487,\n    \"hostname\": \"MacBook-Pro.ds19.agh.edu.pl\",\n    \"node_ip\": \"127.0.0.1\",\n    \"config\": {\n      \"Gradient_LR\": 0.029430458230505614,\n      \"Gradient_Momentum\": 0.9852730072789317,\n      \"Gradient_Steps\": 230,\n      \"Threshold\": 13,\n      \"merge_threshold\": 10,\n      \"number_of_points\": 900,\n      \"outliers_multiplier\": 3.769257792932316\n    },\n    \"time_since_restore\": 13.113409996032715,\n    \"iterations_since_restore\": 1,\n    \"experiment_tag\": \"23_Gradient_LR=0.0294,Gradient_Momentum=0.9853,Gradient_Steps=230,Threshold=13,merge_threshold=10,number_of_points=900,outliers_multiplier=3.7693\"\n  },\n  \"_default_result_or_future\": null,\n  \"last_update_time\": 1737596429.658296,\n  \"metric_analysis\": {\n    \"score\": {\n      \"max\": 0.8627639029899508,\n      \"min\": 0.8627639029899508,\n      \"avg\": 0.8627639029899508,\n      \"last\": 0.8627639029899508,\n      \"last-5-avg\": 0.8627639029899508,\n      \"last-10-avg\": 0.8627639029899508\n    },\n    \"time_this_iter_s\": {\n      \"max\": 13.113409996032715,\n      \"min\": 13.113409996032715,\n      \"avg\": 13.113409996032715,\n      \"last\": 13.113409996032715,\n      \"last-5-avg\": 13.113409996032715,\n      \"last-10-avg\": 13.113409996032715\n    },\n    \"done\": {\n      \"max\": false,\n      \"min\": false,\n      \"avg\": false,\n      \"last\": false,\n      \"last-5-avg\": false,\n      \"last-10-avg\": false\n    },\n    \"training_iteration\": {\n      \"max\": 1,\n      \"min\": 1,\n      \"avg\": 1,\n      \"last\": 1,\n      \"last-5-avg\": 1,\n      \"last-10-avg\": 1\n    },\n    \"time_total_s\": {\n      \"max\": 13.113409996032715,\n      \"min\": 13.113409996032715,\n      \"avg\": 13.113409996032715,\n      \"last\": 13.113409996032715,\n      \"last-5-avg\": 13.113409996032715,\n      \"last-10-avg\": 13.113409996032715\n    },\n    \"time_since_restore\": {\n      \"max\": 13.113409996032715,\n      \"min\": 13.113409996032715,\n      \"avg\": 13.113409996032715,\n      \"last\": 13.113409996032715,\n      \"last-5-avg\": 13.113409996032715,\n      \"last-10-avg\": 13.113409996032715\n    },\n    \"iterations_since_restore\": {\n      \"max\": 1,\n      \"min\": 1,\n      \"avg\": 1,\n      \"last\": 1,\n      \"last-5-avg\": 1,\n      \"last-10-avg\": 1\n    }\n  },\n  \"n_steps\": [\n    5,\n    10\n  ],\n  \"metric_n_steps\": {\n    \"score\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059589000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452948c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02663894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308625b700bc39beb3f9486945294612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059589000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452948c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02663894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308625b700bc39beb3f9486945294612e\"\n      }\n    },\n    \"time_this_iter_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529447402a3a10e0000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529447402a3a10e0000000612e\"\n      }\n    },\n    \"done\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059522000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529489612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059522000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529489612e\"\n      }\n    },\n    \"training_iteration\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b01612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b01612e\"\n      }\n    },\n    \"time_total_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529447402a3a10e0000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529447402a3a10e0000000612e\"\n      }\n    },\n    \"time_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529447402a3a10e0000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529447402a3a10e0000000612e\"\n      }\n    },\n    \"iterations_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b01612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b01612e\"\n      }\n    }\n  },\n  \"export_formats\": [],\n  \"status\": \"TERMINATED\",\n  \"start_time\": 1737596416.5266938,\n  \"relative_logdir\": \"lambda_284b6bc0_23_Gradient_LR=0.0294,Gradient_Momentum=0.9853,Gradient_Steps=230,Threshold=13,merge_threshold=10,number_of_points_2025-01-23_02-40-12\",\n  \"runner\": null,\n  \"last_debug\": 0,\n  \"error_filename\": null,\n  \"pickled_error_filename\": null,\n  \"trial_name_creator\": null,\n  \"trial_dirname_creator\": null,\n  \"custom_trial_name\": null,\n  \"custom_dirname\": null,\n  \"experiment_dir_name\": \"test\",\n  \"saving_to\": null,\n  \"checkpoint_config\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"800595ff000000000000008c0e7261792e6169722e636f6e666967948c10436865636b706f696e74436f6e6669679493942981947d94288c0b6e756d5f746f5f6b656570944b038c1a636865636b706f696e745f73636f72655f617474726962757465948c12747261696e696e675f697465726174696f6e948c16636865636b706f696e745f73636f72655f6f72646572948c036d6178948c14636865636b706f696e745f6672657175656e6379944b008c11636865636b706f696e745f61745f656e64944e8c1a5f636865636b706f696e745f6b6565705f616c6c5f72616e6b7394898c1f5f636865636b706f696e745f75706c6f61645f66726f6d5f776f726b657273948975622e\"\n  },\n  \"checkpoint_manager\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"800595d2020000000000008c257261792e74756e652e657865637574696f6e2e636865636b706f696e745f6d616e61676572948c125f436865636b706f696e744d616e616765729493942981947d94288c145f636865636b706f696e745f7374726174656779948c0e7261792e6169722e636f6e666967948c10436865636b706f696e74436f6e6669679493942981947d94288c0b6e756d5f746f5f6b656570944b038c1a636865636b706f696e745f73636f72655f617474726962757465948c12747261696e696e675f697465726174696f6e948c16636865636b706f696e745f73636f72655f6f72646572948c036d6178948c14636865636b706f696e745f6672657175656e6379944b008c11636865636b706f696e745f61745f656e64944e8c1a5f636865636b706f696e745f6b6565705f616c6c5f72616e6b7394898c1f5f636865636b706f696e745f75706c6f61645f66726f6d5f776f726b657273948975628c155f6c61746573745f636865636b706f696e745f6964944b008c1a5f746f705f7065727369737465645f636865636b706f696e7473945d948c1a5f626573745f7065727369737465645f636865636b706f696e74944e8c1c5f6c61746573745f7065727369737465645f636865636b706f696e74944e8c195f6c61746573745f6d656d6f72795f636865636b706f696e74948c247261792e6169722e5f696e7465726e616c2e636865636b706f696e745f6d616e61676572948c125f547261636b6564436865636b706f696e749493942981947d94288c0b6469725f6f725f64617461944e8c026964944affffffff8c0c73746f726167655f6d6f646594681a8c11436865636b706f696e7453746f726167659493944b01859452948c0472616e6b944b008c076d657472696373947d948c076e6f64655f6970944e8c155f6d6f76655f696e73746561645f6f665f636f7079948875628c185f636865636b706f696e74735f746f5f636c65616e5f7570948f9475622e\"\n  },\n  \"restore_path\": null,\n  \"restoring_from\": null,\n  \"num_failures\": 0,\n  \"num_restore_failures\": 0,\n  \"results\": \"80054e2e\",\n  \"best_result\": \"80054e2e\",\n  \"param_config\": \"80054e2e\",\n  \"extra_arg\": \"80054e2e\",\n  \"_state_json\": null,\n  \"_state_valid\": false,\n  \"_resources\": \"80054e2e\"\n}",
    "{\n  \"stub\": false,\n  \"trainable_name\": \"lambda\",\n  \"trial_id\": \"02024010\",\n  \"sync_config\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059593000000000000008c0f7261792e74756e652e73796e636572948c0a53796e63436f6e6669679493942981947d94288c0a75706c6f61645f646972944e8c0673796e636572948c046175746f948c0b73796e635f706572696f64944d2c018c0c73796e635f74696d656f7574944d08078c0e73796e635f61727469666163747394888c1273796e635f6f6e5f636865636b706f696e74948875622e\"\n  },\n  \"_orig_experiment_path\": \"/Users/kuba/ijcai_ski/ray_results/test\",\n  \"_orig_experiment_dir_name\": \"test\",\n  \"_local_experiment_path\": \"/Users/kuba/ijcai_ski/ray_results/test\",\n  \"_remote_experiment_path\": null,\n  \"config\": {\n    \"Gradient_LR\": 0.05262926203879942,\n    \"Gradient_Momentum\": 0.9611674590514012,\n    \"Gradient_Steps\": 210,\n    \"Threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308130000000000000094869452942e\"\n    },\n    \"merge_threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308070000000000000094869452942e\"\n    },\n    \"number_of_points\": 500,\n    \"outliers_multiplier\": 4.8721692183230285\n  },\n  \"_Trial__unresolved_config\": {\n    \"Gradient_LR\": 0.05262926203879942,\n    \"Gradient_Momentum\": 0.9611674590514012,\n    \"Gradient_Steps\": 210,\n    \"Threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308130000000000000094869452942e\"\n    },\n    \"merge_threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308070000000000000094869452942e\"\n    },\n    \"number_of_points\": 500,\n    \"outliers_multiplier\": 4.8721692183230285\n  },\n  \"evaluated_params\": {\n    \"Gradient_LR\": 0.05262926203879942,\n    \"Gradient_Momentum\": 0.9611674590514012,\n    \"Gradient_Steps\": 210,\n    \"Threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308130000000000000094869452942e\"\n    },\n    \"merge_threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308070000000000000094869452942e\"\n    },\n    \"number_of_points\": 500,\n    \"outliers_multiplier\": 4.8721692183230285\n  },\n  \"experiment_tag\": \"20_Gradient_LR=0.0526,Gradient_Momentum=0.9612,Gradient_Steps=210,Threshold=19,merge_threshold=7,number_of_points=500,outliers_multiplier=4.8722\",\n  \"location\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059546000000000000008c197261792e74756e652e6578706572696d656e742e747269616c948c095f4c6f636174696f6e9493942981947d94288c08686f73746e616d65944e8c03706964944e75622e\"\n  },\n  \"stopping_criterion\": {},\n  \"_setup_default_resource\": true,\n  \"_default_placement_group_factory\": \"800595aa000000000000008c237261792e74756e652e657865637574696f6e2e706c6163656d656e745f67726f757073948c15506c6163656d656e7447726f7570466163746f72799493942981947d94288c085f62756e646c6573945d947d948c0343505594473ff000000000000073618c155f686561645f62756e646c655f69735f656d70747994898c095f7374726174656779948c045041434b948c055f6172677394298c075f6b7761726773947d9475622e\",\n  \"placement_group_factory\": \"800595aa000000000000008c237261792e74756e652e657865637574696f6e2e706c6163656d656e745f67726f757073948c15506c6163656d656e7447726f7570466163746f72799493942981947d94288c085f62756e646c6573945d947d948c0343505594473ff000000000000073618c155f686561645f62756e646c655f69735f656d70747994898c095f7374726174656779948c045041434b948c055f6172677394298c075f6b7761726773947d9475622e\",\n  \"log_to_file\": [\n    null,\n    null\n  ],\n  \"max_failures\": 0,\n  \"_last_result\": {\n    \"score\": 0.8161782375056383,\n    \"time_this_iter_s\": 13.300286054611206,\n    \"done\": true,\n    \"training_iteration\": 1,\n    \"trial_id\": \"02024010\",\n    \"date\": \"2025-01-23_02-40-22\",\n    \"timestamp\": 1737596422,\n    \"time_total_s\": 13.300286054611206,\n    \"pid\": 30525,\n    \"hostname\": \"MacBook-Pro.ds19.agh.edu.pl\",\n    \"node_ip\": \"127.0.0.1\",\n    \"config\": {\n      \"Gradient_LR\": 0.05262926203879942,\n      \"Gradient_Momentum\": 0.9611674590514012,\n      \"Gradient_Steps\": 210,\n      \"Threshold\": 19,\n      \"merge_threshold\": 7,\n      \"number_of_points\": 500,\n      \"outliers_multiplier\": 4.8721692183230285\n    },\n    \"time_since_restore\": 13.300286054611206,\n    \"iterations_since_restore\": 1,\n    \"experiment_tag\": \"20_Gradient_LR=0.0526,Gradient_Momentum=0.9612,Gradient_Steps=210,Threshold=19,merge_threshold=7,number_of_points=500,outliers_multiplier=4.8722\"\n  },\n  \"_default_result_or_future\": null,\n  \"last_update_time\": 1737596422.029675,\n  \"metric_analysis\": {\n    \"score\": {\n      \"max\": 0.8161782375056383,\n      \"min\": 0.8161782375056383,\n      \"avg\": 0.8161782375056383,\n      \"last\": 0.8161782375056383,\n      \"last-5-avg\": 0.8161782375056383,\n      \"last-10-avg\": 0.8161782375056383\n    },\n    \"time_this_iter_s\": {\n      \"max\": 13.300286054611206,\n      \"min\": 13.300286054611206,\n      \"avg\": 13.300286054611206,\n      \"last\": 13.300286054611206,\n      \"last-5-avg\": 13.300286054611206,\n      \"last-10-avg\": 13.300286054611206\n    },\n    \"done\": {\n      \"max\": false,\n      \"min\": false,\n      \"avg\": false,\n      \"last\": false,\n      \"last-5-avg\": false,\n      \"last-10-avg\": false\n    },\n    \"training_iteration\": {\n      \"max\": 1,\n      \"min\": 1,\n      \"avg\": 1,\n      \"last\": 1,\n      \"last-5-avg\": 1,\n      \"last-10-avg\": 1\n    },\n    \"time_total_s\": {\n      \"max\": 13.300286054611206,\n      \"min\": 13.300286054611206,\n      \"avg\": 13.300286054611206,\n      \"last\": 13.300286054611206,\n      \"last-5-avg\": 13.300286054611206,\n      \"last-10-avg\": 13.300286054611206\n    },\n    \"time_since_restore\": {\n      \"max\": 13.300286054611206,\n      \"min\": 13.300286054611206,\n      \"avg\": 13.300286054611206,\n      \"last\": 13.300286054611206,\n      \"last-5-avg\": 13.300286054611206,\n      \"last-10-avg\": 13.300286054611206\n    },\n    \"iterations_since_restore\": {\n      \"max\": 1,\n      \"min\": 1,\n      \"avg\": 1,\n      \"last\": 1,\n      \"last-5-avg\": 1,\n      \"last-10-avg\": 1\n    }\n  },\n  \"n_steps\": [\n    5,\n    10\n  ],\n  \"metric_n_steps\": {\n    \"score\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059589000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452948c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02663894898887945294284b038c013c944e4e4e4affffffff4affffffff4b0074946243087a65b9d2211eea3f9486945294612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059589000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452948c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02663894898887945294284b038c013c944e4e4e4affffffff4affffffff4b0074946243087a65b9d2211eea3f9486945294612e\"\n      }\n    },\n    \"time_this_iter_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529447402a99bf18000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529447402a99bf18000000612e\"\n      }\n    },\n    \"done\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059522000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529489612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059522000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529489612e\"\n      }\n    },\n    \"training_iteration\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b01612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b01612e\"\n      }\n    },\n    \"time_total_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529447402a99bf18000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529447402a99bf18000000612e\"\n      }\n    },\n    \"time_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529447402a99bf18000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529447402a99bf18000000612e\"\n      }\n    },\n    \"iterations_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b01612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b01612e\"\n      }\n    }\n  },\n  \"export_formats\": [],\n  \"status\": \"TERMINATED\",\n  \"start_time\": 1737596408.7047338,\n  \"relative_logdir\": \"lambda_02024010_20_Gradient_LR=0.0526,Gradient_Momentum=0.9612,Gradient_Steps=210,Threshold=19,merge_threshold=7,number_of_points=_2025-01-23_02-40-08\",\n  \"runner\": null,\n  \"last_debug\": 0,\n  \"error_filename\": null,\n  \"pickled_error_filename\": null,\n  \"trial_name_creator\": null,\n  \"trial_dirname_creator\": null,\n  \"custom_trial_name\": null,\n  \"custom_dirname\": null,\n  \"experiment_dir_name\": \"test\",\n  \"saving_to\": null,\n  \"checkpoint_config\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"800595ff000000000000008c0e7261792e6169722e636f6e666967948c10436865636b706f696e74436f6e6669679493942981947d94288c0b6e756d5f746f5f6b656570944b038c1a636865636b706f696e745f73636f72655f617474726962757465948c12747261696e696e675f697465726174696f6e948c16636865636b706f696e745f73636f72655f6f72646572948c036d6178948c14636865636b706f696e745f6672657175656e6379944b008c11636865636b706f696e745f61745f656e64944e8c1a5f636865636b706f696e745f6b6565705f616c6c5f72616e6b7394898c1f5f636865636b706f696e745f75706c6f61645f66726f6d5f776f726b657273948975622e\"\n  },\n  \"checkpoint_manager\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"800595d2020000000000008c257261792e74756e652e657865637574696f6e2e636865636b706f696e745f6d616e61676572948c125f436865636b706f696e744d616e616765729493942981947d94288c145f636865636b706f696e745f7374726174656779948c0e7261792e6169722e636f6e666967948c10436865636b706f696e74436f6e6669679493942981947d94288c0b6e756d5f746f5f6b656570944b038c1a636865636b706f696e745f73636f72655f617474726962757465948c12747261696e696e675f697465726174696f6e948c16636865636b706f696e745f73636f72655f6f72646572948c036d6178948c14636865636b706f696e745f6672657175656e6379944b008c11636865636b706f696e745f61745f656e64944e8c1a5f636865636b706f696e745f6b6565705f616c6c5f72616e6b7394898c1f5f636865636b706f696e745f75706c6f61645f66726f6d5f776f726b657273948975628c155f6c61746573745f636865636b706f696e745f6964944b008c1a5f746f705f7065727369737465645f636865636b706f696e7473945d948c1a5f626573745f7065727369737465645f636865636b706f696e74944e8c1c5f6c61746573745f7065727369737465645f636865636b706f696e74944e8c195f6c61746573745f6d656d6f72795f636865636b706f696e74948c247261792e6169722e5f696e7465726e616c2e636865636b706f696e745f6d616e61676572948c125f547261636b6564436865636b706f696e749493942981947d94288c0b6469725f6f725f64617461944e8c026964944affffffff8c0c73746f726167655f6d6f646594681a8c11436865636b706f696e7453746f726167659493944b01859452948c0472616e6b944b008c076d657472696373947d948c076e6f64655f6970944e8c155f6d6f76655f696e73746561645f6f665f636f7079948875628c185f636865636b706f696e74735f746f5f636c65616e5f7570948f9475622e\"\n  },\n  \"restore_path\": null,\n  \"restoring_from\": null,\n  \"num_failures\": 0,\n  \"num_restore_failures\": 0,\n  \"results\": \"80054e2e\",\n  \"best_result\": \"80054e2e\",\n  \"param_config\": \"80054e2e\",\n  \"extra_arg\": \"80054e2e\",\n  \"_state_json\": null,\n  \"_state_valid\": false,\n  \"_resources\": \"80054e2e\"\n}",
    "{\n  \"stub\": false,\n  \"trainable_name\": \"lambda\",\n  \"trial_id\": \"a158585c\",\n  \"sync_config\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059593000000000000008c0f7261792e74756e652e73796e636572948c0a53796e63436f6e6669679493942981947d94288c0a75706c6f61645f646972944e8c0673796e636572948c046175746f948c0b73796e635f706572696f64944d2c018c0c73796e635f74696d656f7574944d08078c0e73796e635f61727469666163747394888c1273796e635f6f6e5f636865636b706f696e74948875622e\"\n  },\n  \"_orig_experiment_path\": \"/Users/kuba/ijcai_ski/ray_results/test\",\n  \"_orig_experiment_dir_name\": \"test\",\n  \"_local_experiment_path\": \"/Users/kuba/ijcai_ski/ray_results/test\",\n  \"_remote_experiment_path\": null,\n  \"config\": {\n    \"Gradient_LR\": 0.012508766443324926,\n    \"Gradient_Momentum\": 0.9373352987270256,\n    \"Gradient_Steps\": 370,\n    \"Threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b0074946243080a0000000000000094869452942e\"\n    },\n    \"merge_threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308060000000000000094869452942e\"\n    },\n    \"number_of_points\": 700,\n    \"outliers_multiplier\": 3.0817999426925327\n  },\n  \"_Trial__unresolved_config\": {\n    \"Gradient_LR\": 0.012508766443324926,\n    \"Gradient_Momentum\": 0.9373352987270256,\n    \"Gradient_Steps\": 370,\n    \"Threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b0074946243080a0000000000000094869452942e\"\n    },\n    \"merge_threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308060000000000000094869452942e\"\n    },\n    \"number_of_points\": 700,\n    \"outliers_multiplier\": 3.0817999426925327\n  },\n  \"evaluated_params\": {\n    \"Gradient_LR\": 0.012508766443324926,\n    \"Gradient_Momentum\": 0.9373352987270256,\n    \"Gradient_Steps\": 370,\n    \"Threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b0074946243080a0000000000000094869452942e\"\n    },\n    \"merge_threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308060000000000000094869452942e\"\n    },\n    \"number_of_points\": 700,\n    \"outliers_multiplier\": 3.0817999426925327\n  },\n  \"experiment_tag\": \"19_Gradient_LR=0.0125,Gradient_Momentum=0.9373,Gradient_Steps=370,Threshold=10,merge_threshold=6,number_of_points=700,outliers_multiplier=3.0818\",\n  \"location\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059546000000000000008c197261792e74756e652e6578706572696d656e742e747269616c948c095f4c6f636174696f6e9493942981947d94288c08686f73746e616d65944e8c03706964944e75622e\"\n  },\n  \"stopping_criterion\": {},\n  \"_setup_default_resource\": true,\n  \"_default_placement_group_factory\": \"800595aa000000000000008c237261792e74756e652e657865637574696f6e2e706c6163656d656e745f67726f757073948c15506c6163656d656e7447726f7570466163746f72799493942981947d94288c085f62756e646c6573945d947d948c0343505594473ff000000000000073618c155f686561645f62756e646c655f69735f656d70747994898c095f7374726174656779948c045041434b948c055f6172677394298c075f6b7761726773947d9475622e\",\n  \"placement_group_factory\": \"800595aa000000000000008c237261792e74756e652e657865637574696f6e2e706c6163656d656e745f67726f757073948c15506c6163656d656e7447726f7570466163746f72799493942981947d94288c085f62756e646c6573945d947d948c0343505594473ff000000000000073618c155f686561645f62756e646c655f69735f656d70747994898c095f7374726174656779948c045041434b948c055f6172677394298c075f6b7761726773947d9475622e\",\n  \"log_to_file\": [\n    null,\n    null\n  ],\n  \"max_failures\": 0,\n  \"_last_result\": {\n    \"score\": 0.877154435245733,\n    \"time_this_iter_s\": 16.346220016479492,\n    \"done\": true,\n    \"training_iteration\": 1,\n    \"trial_id\": \"a158585c\",\n    \"date\": \"2025-01-23_02-40-24\",\n    \"timestamp\": 1737596424,\n    \"time_total_s\": 16.346220016479492,\n    \"pid\": 30685,\n    \"hostname\": \"MacBook-Pro.ds19.agh.edu.pl\",\n    \"node_ip\": \"127.0.0.1\",\n    \"config\": {\n      \"Gradient_LR\": 0.012508766443324926,\n      \"Gradient_Momentum\": 0.9373352987270256,\n      \"Gradient_Steps\": 370,\n      \"Threshold\": 10,\n      \"merge_threshold\": 6,\n      \"number_of_points\": 700,\n      \"outliers_multiplier\": 3.0817999426925327\n    },\n    \"time_since_restore\": 16.346220016479492,\n    \"iterations_since_restore\": 1,\n    \"experiment_tag\": \"19_Gradient_LR=0.0125,Gradient_Momentum=0.9373,Gradient_Steps=370,Threshold=10,merge_threshold=6,number_of_points=700,outliers_multiplier=3.0818\"\n  },\n  \"_default_result_or_future\": null,\n  \"last_update_time\": 1737596424.5632331,\n  \"metric_analysis\": {\n    \"score\": {\n      \"max\": 0.877154435245733,\n      \"min\": 0.877154435245733,\n      \"avg\": 0.877154435245733,\n      \"last\": 0.877154435245733,\n      \"last-5-avg\": 0.877154435245733,\n      \"last-10-avg\": 0.877154435245733\n    },\n    \"time_this_iter_s\": {\n      \"max\": 16.346220016479492,\n      \"min\": 16.346220016479492,\n      \"avg\": 16.346220016479492,\n      \"last\": 16.346220016479492,\n      \"last-5-avg\": 16.346220016479492,\n      \"last-10-avg\": 16.346220016479492\n    },\n    \"done\": {\n      \"max\": false,\n      \"min\": false,\n      \"avg\": false,\n      \"last\": false,\n      \"last-5-avg\": false,\n      \"last-10-avg\": false\n    },\n    \"training_iteration\": {\n      \"max\": 1,\n      \"min\": 1,\n      \"avg\": 1,\n      \"last\": 1,\n      \"last-5-avg\": 1,\n      \"last-10-avg\": 1\n    },\n    \"time_total_s\": {\n      \"max\": 16.346220016479492,\n      \"min\": 16.346220016479492,\n      \"avg\": 16.346220016479492,\n      \"last\": 16.346220016479492,\n      \"last-5-avg\": 16.346220016479492,\n      \"last-10-avg\": 16.346220016479492\n    },\n    \"time_since_restore\": {\n      \"max\": 16.346220016479492,\n      \"min\": 16.346220016479492,\n      \"avg\": 16.346220016479492,\n      \"last\": 16.346220016479492,\n      \"last-5-avg\": 16.346220016479492,\n      \"last-10-avg\": 16.346220016479492\n    },\n    \"iterations_since_restore\": {\n      \"max\": 1,\n      \"min\": 1,\n      \"avg\": 1,\n      \"last\": 1,\n      \"last-5-avg\": 1,\n      \"last-10-avg\": 1\n    }\n  },\n  \"n_steps\": [\n    5,\n    10\n  ],\n  \"metric_n_steps\": {\n    \"score\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059589000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452948c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02663894898887945294284b038c013c944e4e4e4affffffff4affffffff4b0074946243082a7f9d2da611ec3f9486945294612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059589000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452948c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02663894898887945294284b038c013c944e4e4e4affffffff4affffffff4b0074946243082a7f9d2da611ec3f9486945294612e\"\n      }\n    },\n    \"time_this_iter_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529447403058a1e0000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529447403058a1e0000000612e\"\n      }\n    },\n    \"done\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059522000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529489612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059522000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529489612e\"\n      }\n    },\n    \"training_iteration\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b01612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b01612e\"\n      }\n    },\n    \"time_total_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529447403058a1e0000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529447403058a1e0000000612e\"\n      }\n    },\n    \"time_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529447403058a1e0000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529447403058a1e0000000612e\"\n      }\n    },\n    \"iterations_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b01612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b01612e\"\n      }\n    }\n  },\n  \"export_formats\": [],\n  \"status\": \"TERMINATED\",\n  \"start_time\": 1737596408.211327,\n  \"relative_logdir\": \"lambda_a158585c_19_Gradient_LR=0.0125,Gradient_Momentum=0.9373,Gradient_Steps=370,Threshold=10,merge_threshold=6,number_of_points=_2025-01-23_02-40-05\",\n  \"runner\": null,\n  \"last_debug\": 0,\n  \"error_filename\": null,\n  \"pickled_error_filename\": null,\n  \"trial_name_creator\": null,\n  \"trial_dirname_creator\": null,\n  \"custom_trial_name\": null,\n  \"custom_dirname\": null,\n  \"experiment_dir_name\": \"test\",\n  \"saving_to\": null,\n  \"checkpoint_config\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"800595ff000000000000008c0e7261792e6169722e636f6e666967948c10436865636b706f696e74436f6e6669679493942981947d94288c0b6e756d5f746f5f6b656570944b038c1a636865636b706f696e745f73636f72655f617474726962757465948c12747261696e696e675f697465726174696f6e948c16636865636b706f696e745f73636f72655f6f72646572948c036d6178948c14636865636b706f696e745f6672657175656e6379944b008c11636865636b706f696e745f61745f656e64944e8c1a5f636865636b706f696e745f6b6565705f616c6c5f72616e6b7394898c1f5f636865636b706f696e745f75706c6f61645f66726f6d5f776f726b657273948975622e\"\n  },\n  \"checkpoint_manager\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"800595d2020000000000008c257261792e74756e652e657865637574696f6e2e636865636b706f696e745f6d616e61676572948c125f436865636b706f696e744d616e616765729493942981947d94288c145f636865636b706f696e745f7374726174656779948c0e7261792e6169722e636f6e666967948c10436865636b706f696e74436f6e6669679493942981947d94288c0b6e756d5f746f5f6b656570944b038c1a636865636b706f696e745f73636f72655f617474726962757465948c12747261696e696e675f697465726174696f6e948c16636865636b706f696e745f73636f72655f6f72646572948c036d6178948c14636865636b706f696e745f6672657175656e6379944b008c11636865636b706f696e745f61745f656e64944e8c1a5f636865636b706f696e745f6b6565705f616c6c5f72616e6b7394898c1f5f636865636b706f696e745f75706c6f61645f66726f6d5f776f726b657273948975628c155f6c61746573745f636865636b706f696e745f6964944b008c1a5f746f705f7065727369737465645f636865636b706f696e7473945d948c1a5f626573745f7065727369737465645f636865636b706f696e74944e8c1c5f6c61746573745f7065727369737465645f636865636b706f696e74944e8c195f6c61746573745f6d656d6f72795f636865636b706f696e74948c247261792e6169722e5f696e7465726e616c2e636865636b706f696e745f6d616e61676572948c125f547261636b6564436865636b706f696e749493942981947d94288c0b6469725f6f725f64617461944e8c026964944affffffff8c0c73746f726167655f6d6f646594681a8c11436865636b706f696e7453746f726167659493944b01859452948c0472616e6b944b008c076d657472696373947d948c076e6f64655f6970944e8c155f6d6f76655f696e73746561645f6f665f636f7079948875628c185f636865636b706f696e74735f746f5f636c65616e5f7570948f9475622e\"\n  },\n  \"restore_path\": null,\n  \"restoring_from\": null,\n  \"num_failures\": 0,\n  \"num_restore_failures\": 0,\n  \"results\": \"80054e2e\",\n  \"best_result\": \"80054e2e\",\n  \"param_config\": \"80054e2e\",\n  \"extra_arg\": \"80054e2e\",\n  \"_state_json\": null,\n  \"_state_valid\": false,\n  \"_resources\": \"80054e2e\"\n}",
    "{\n  \"stub\": false,\n  \"trainable_name\": \"lambda\",\n  \"trial_id\": \"dc1d340a\",\n  \"sync_config\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059593000000000000008c0f7261792e74756e652e73796e636572948c0a53796e63436f6e6669679493942981947d94288c0a75706c6f61645f646972944e8c0673796e636572948c046175746f948c0b73796e635f706572696f64944d2c018c0c73796e635f74696d656f7574944d08078c0e73796e635f61727469666163747394888c1273796e635f6f6e5f636865636b706f696e74948875622e\"\n  },\n  \"_orig_experiment_path\": \"/Users/kuba/ijcai_ski/ray_results/test\",\n  \"_orig_experiment_dir_name\": \"test\",\n  \"_local_experiment_path\": \"/Users/kuba/ijcai_ski/ray_results/test\",\n  \"_remote_experiment_path\": null,\n  \"config\": {\n    \"Gradient_LR\": 0.04098462949063312,\n    \"Gradient_Momentum\": 0.997727232559919,\n    \"Gradient_Steps\": 240,\n    \"Threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308000000000000000094869452942e\"\n    },\n    \"merge_threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b0074946243080a0000000000000094869452942e\"\n    },\n    \"number_of_points\": 200,\n    \"outliers_multiplier\": 4.258006593048278\n  },\n  \"_Trial__unresolved_config\": {\n    \"Gradient_LR\": 0.04098462949063312,\n    \"Gradient_Momentum\": 0.997727232559919,\n    \"Gradient_Steps\": 240,\n    \"Threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308000000000000000094869452942e\"\n    },\n    \"merge_threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b0074946243080a0000000000000094869452942e\"\n    },\n    \"number_of_points\": 200,\n    \"outliers_multiplier\": 4.258006593048278\n  },\n  \"evaluated_params\": {\n    \"Gradient_LR\": 0.04098462949063312,\n    \"Gradient_Momentum\": 0.997727232559919,\n    \"Gradient_Steps\": 240,\n    \"Threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308000000000000000094869452942e\"\n    },\n    \"merge_threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b0074946243080a0000000000000094869452942e\"\n    },\n    \"number_of_points\": 200,\n    \"outliers_multiplier\": 4.258006593048278\n  },\n  \"experiment_tag\": \"22_Gradient_LR=0.0410,Gradient_Momentum=0.9977,Gradient_Steps=240,Threshold=0,merge_threshold=10,number_of_points=200,outliers_multiplier=4.2580\",\n  \"location\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059546000000000000008c197261792e74756e652e6578706572696d656e742e747269616c948c095f4c6f636174696f6e9493942981947d94288c08686f73746e616d65944e8c03706964944e75622e\"\n  },\n  \"stopping_criterion\": {},\n  \"_setup_default_resource\": true,\n  \"_default_placement_group_factory\": \"800595aa000000000000008c237261792e74756e652e657865637574696f6e2e706c6163656d656e745f67726f757073948c15506c6163656d656e7447726f7570466163746f72799493942981947d94288c085f62756e646c6573945d947d948c0343505594473ff000000000000073618c155f686561645f62756e646c655f69735f656d70747994898c095f7374726174656779948c045041434b948c055f6172677394298c075f6b7761726773947d9475622e\",\n  \"placement_group_factory\": \"800595aa000000000000008c237261792e74756e652e657865637574696f6e2e706c6163656d656e745f67726f757073948c15506c6163656d656e7447726f7570466163746f72799493942981947d94288c085f62756e646c6573945d947d948c0343505594473ff000000000000073618c155f686561645f62756e646c655f69735f656d70747994898c095f7374726174656779948c045041434b948c055f6172677394298c075f6b7761726773947d9475622e\",\n  \"log_to_file\": [\n    null,\n    null\n  ],\n  \"max_failures\": 0,\n  \"_last_result\": {\n    \"score\": 0.6174560825910322,\n    \"time_this_iter_s\": 13.668535947799683,\n    \"done\": true,\n    \"training_iteration\": 1,\n    \"trial_id\": \"dc1d340a\",\n    \"date\": \"2025-01-23_02-40-25\",\n    \"timestamp\": 1737596425,\n    \"time_total_s\": 13.668535947799683,\n    \"pid\": 30590,\n    \"hostname\": \"MacBook-Pro.ds19.agh.edu.pl\",\n    \"node_ip\": \"127.0.0.1\",\n    \"config\": {\n      \"Gradient_LR\": 0.04098462949063312,\n      \"Gradient_Momentum\": 0.997727232559919,\n      \"Gradient_Steps\": 240,\n      \"Threshold\": 0,\n      \"merge_threshold\": 10,\n      \"number_of_points\": 200,\n      \"outliers_multiplier\": 4.258006593048278\n    },\n    \"time_since_restore\": 13.668535947799683,\n    \"iterations_since_restore\": 1,\n    \"experiment_tag\": \"22_Gradient_LR=0.0410,Gradient_Momentum=0.9977,Gradient_Steps=240,Threshold=0,merge_threshold=10,number_of_points=200,outliers_multiplier=4.2580\"\n  },\n  \"_default_result_or_future\": null,\n  \"last_update_time\": 1737596425.6868842,\n  \"metric_analysis\": {\n    \"score\": {\n      \"max\": 0.6174560825910322,\n      \"min\": 0.6174560825910322,\n      \"avg\": 0.6174560825910322,\n      \"last\": 0.6174560825910322,\n      \"last-5-avg\": 0.6174560825910322,\n      \"last-10-avg\": 0.6174560825910322\n    },\n    \"time_this_iter_s\": {\n      \"max\": 13.668535947799683,\n      \"min\": 13.668535947799683,\n      \"avg\": 13.668535947799683,\n      \"last\": 13.668535947799683,\n      \"last-5-avg\": 13.668535947799683,\n      \"last-10-avg\": 13.668535947799683\n    },\n    \"done\": {\n      \"max\": false,\n      \"min\": false,\n      \"avg\": false,\n      \"last\": false,\n      \"last-5-avg\": false,\n      \"last-10-avg\": false\n    },\n    \"training_iteration\": {\n      \"max\": 1,\n      \"min\": 1,\n      \"avg\": 1,\n      \"last\": 1,\n      \"last-5-avg\": 1,\n      \"last-10-avg\": 1\n    },\n    \"time_total_s\": {\n      \"max\": 13.668535947799683,\n      \"min\": 13.668535947799683,\n      \"avg\": 13.668535947799683,\n      \"last\": 13.668535947799683,\n      \"last-5-avg\": 13.668535947799683,\n      \"last-10-avg\": 13.668535947799683\n    },\n    \"time_since_restore\": {\n      \"max\": 13.668535947799683,\n      \"min\": 13.668535947799683,\n      \"avg\": 13.668535947799683,\n      \"last\": 13.668535947799683,\n      \"last-5-avg\": 13.668535947799683,\n      \"last-10-avg\": 13.668535947799683\n    },\n    \"iterations_since_restore\": {\n      \"max\": 1,\n      \"min\": 1,\n      \"avg\": 1,\n      \"last\": 1,\n      \"last-5-avg\": 1,\n      \"last-10-avg\": 1\n    }\n  },\n  \"n_steps\": [\n    5,\n    10\n  ],\n  \"metric_n_steps\": {\n    \"score\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059589000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452948c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02663894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308763b2e4233c2e33f9486945294612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059589000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452948c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02663894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308763b2e4233c2e33f9486945294612e\"\n      }\n    },\n    \"time_this_iter_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529447402b564a58000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529447402b564a58000000612e\"\n      }\n    },\n    \"done\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059522000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529489612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059522000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529489612e\"\n      }\n    },\n    \"training_iteration\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b01612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b01612e\"\n      }\n    },\n    \"time_total_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529447402b564a58000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529447402b564a58000000612e\"\n      }\n    },\n    \"time_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529447402b564a58000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529447402b564a58000000612e\"\n      }\n    },\n    \"iterations_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b01612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b01612e\"\n      }\n    }\n  },\n  \"export_formats\": [],\n  \"status\": \"TERMINATED\",\n  \"start_time\": 1737596412.013505,\n  \"relative_logdir\": \"lambda_dc1d340a_22_Gradient_LR=0.0410,Gradient_Momentum=0.9977,Gradient_Steps=240,Threshold=0,merge_threshold=10,number_of_points=_2025-01-23_02-40-09\",\n  \"runner\": null,\n  \"last_debug\": 0,\n  \"error_filename\": null,\n  \"pickled_error_filename\": null,\n  \"trial_name_creator\": null,\n  \"trial_dirname_creator\": null,\n  \"custom_trial_name\": null,\n  \"custom_dirname\": null,\n  \"experiment_dir_name\": \"test\",\n  \"saving_to\": null,\n  \"checkpoint_config\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"800595ff000000000000008c0e7261792e6169722e636f6e666967948c10436865636b706f696e74436f6e6669679493942981947d94288c0b6e756d5f746f5f6b656570944b038c1a636865636b706f696e745f73636f72655f617474726962757465948c12747261696e696e675f697465726174696f6e948c16636865636b706f696e745f73636f72655f6f72646572948c036d6178948c14636865636b706f696e745f6672657175656e6379944b008c11636865636b706f696e745f61745f656e64944e8c1a5f636865636b706f696e745f6b6565705f616c6c5f72616e6b7394898c1f5f636865636b706f696e745f75706c6f61645f66726f6d5f776f726b657273948975622e\"\n  },\n  \"checkpoint_manager\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"800595d2020000000000008c257261792e74756e652e657865637574696f6e2e636865636b706f696e745f6d616e61676572948c125f436865636b706f696e744d616e616765729493942981947d94288c145f636865636b706f696e745f7374726174656779948c0e7261792e6169722e636f6e666967948c10436865636b706f696e74436f6e6669679493942981947d94288c0b6e756d5f746f5f6b656570944b038c1a636865636b706f696e745f73636f72655f617474726962757465948c12747261696e696e675f697465726174696f6e948c16636865636b706f696e745f73636f72655f6f72646572948c036d6178948c14636865636b706f696e745f6672657175656e6379944b008c11636865636b706f696e745f61745f656e64944e8c1a5f636865636b706f696e745f6b6565705f616c6c5f72616e6b7394898c1f5f636865636b706f696e745f75706c6f61645f66726f6d5f776f726b657273948975628c155f6c61746573745f636865636b706f696e745f6964944b008c1a5f746f705f7065727369737465645f636865636b706f696e7473945d948c1a5f626573745f7065727369737465645f636865636b706f696e74944e8c1c5f6c61746573745f7065727369737465645f636865636b706f696e74944e8c195f6c61746573745f6d656d6f72795f636865636b706f696e74948c247261792e6169722e5f696e7465726e616c2e636865636b706f696e745f6d616e61676572948c125f547261636b6564436865636b706f696e749493942981947d94288c0b6469725f6f725f64617461944e8c026964944affffffff8c0c73746f726167655f6d6f646594681a8c11436865636b706f696e7453746f726167659493944b01859452948c0472616e6b944b008c076d657472696373947d948c076e6f64655f6970944e8c155f6d6f76655f696e73746561645f6f665f636f7079948875628c185f636865636b706f696e74735f746f5f636c65616e5f7570948f9475622e\"\n  },\n  \"restore_path\": null,\n  \"restoring_from\": null,\n  \"num_failures\": 0,\n  \"num_restore_failures\": 0,\n  \"results\": \"80054e2e\",\n  \"best_result\": \"80054e2e\",\n  \"param_config\": \"80054e2e\",\n  \"extra_arg\": \"80054e2e\",\n  \"_state_json\": null,\n  \"_state_valid\": false,\n  \"_resources\": \"80054e2e\"\n}",
    "{\n  \"stub\": false,\n  \"trainable_name\": \"lambda\",\n  \"trial_id\": \"61832fd5\",\n  \"sync_config\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059593000000000000008c0f7261792e74756e652e73796e636572948c0a53796e63436f6e6669679493942981947d94288c0a75706c6f61645f646972944e8c0673796e636572948c046175746f948c0b73796e635f706572696f64944d2c018c0c73796e635f74696d656f7574944d08078c0e73796e635f61727469666163747394888c1273796e635f6f6e5f636865636b706f696e74948875622e\"\n  },\n  \"_orig_experiment_path\": \"/Users/kuba/ijcai_ski/ray_results/test\",\n  \"_orig_experiment_dir_name\": \"test\",\n  \"_local_experiment_path\": \"/Users/kuba/ijcai_ski/ray_results/test\",\n  \"_remote_experiment_path\": null,\n  \"config\": {\n    \"Gradient_LR\": 0.035264496447493035,\n    \"Gradient_Momentum\": 0.9547666470861575,\n    \"Gradient_Steps\": 220,\n    \"Threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308080000000000000094869452942e\"\n    },\n    \"merge_threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308130000000000000094869452942e\"\n    },\n    \"number_of_points\": 800,\n    \"outliers_multiplier\": 2.552145544132814\n  },\n  \"_Trial__unresolved_config\": {\n    \"Gradient_LR\": 0.035264496447493035,\n    \"Gradient_Momentum\": 0.9547666470861575,\n    \"Gradient_Steps\": 220,\n    \"Threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308080000000000000094869452942e\"\n    },\n    \"merge_threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308130000000000000094869452942e\"\n    },\n    \"number_of_points\": 800,\n    \"outliers_multiplier\": 2.552145544132814\n  },\n  \"evaluated_params\": {\n    \"Gradient_LR\": 0.035264496447493035,\n    \"Gradient_Momentum\": 0.9547666470861575,\n    \"Gradient_Steps\": 220,\n    \"Threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308080000000000000094869452942e\"\n    },\n    \"merge_threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308130000000000000094869452942e\"\n    },\n    \"number_of_points\": 800,\n    \"outliers_multiplier\": 2.552145544132814\n  },\n  \"experiment_tag\": \"18_Gradient_LR=0.0353,Gradient_Momentum=0.9548,Gradient_Steps=220,Threshold=8,merge_threshold=19,number_of_points=800,outliers_multiplier=2.5521\",\n  \"location\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059546000000000000008c197261792e74756e652e6578706572696d656e742e747269616c948c095f4c6f636174696f6e9493942981947d94288c08686f73746e616d65944e8c03706964944e75622e\"\n  },\n  \"stopping_criterion\": {},\n  \"_setup_default_resource\": true,\n  \"_default_placement_group_factory\": \"800595aa000000000000008c237261792e74756e652e657865637574696f6e2e706c6163656d656e745f67726f757073948c15506c6163656d656e7447726f7570466163746f72799493942981947d94288c085f62756e646c6573945d947d948c0343505594473ff000000000000073618c155f686561645f62756e646c655f69735f656d70747994898c095f7374726174656779948c045041434b948c055f6172677394298c075f6b7761726773947d9475622e\",\n  \"placement_group_factory\": \"800595aa000000000000008c237261792e74756e652e657865637574696f6e2e706c6163656d656e745f67726f757073948c15506c6163656d656e7447726f7570466163746f72799493942981947d94288c085f62756e646c6573945d947d948c0343505594473ff000000000000073618c155f686561645f62756e646c655f69735f656d70747994898c095f7374726174656779948c045041434b948c055f6172677394298c075f6b7761726773947d9475622e\",\n  \"log_to_file\": [\n    null,\n    null\n  ],\n  \"max_failures\": 0,\n  \"_last_result\": {\n    \"score\": 0.8657938593931076,\n    \"time_this_iter_s\": 12.8282151222229,\n    \"done\": true,\n    \"training_iteration\": 1,\n    \"trial_id\": \"61832fd5\",\n    \"date\": \"2025-01-23_02-40-18\",\n    \"timestamp\": 1737596418,\n    \"time_total_s\": 12.8282151222229,\n    \"pid\": 30510,\n    \"hostname\": \"MacBook-Pro.ds19.agh.edu.pl\",\n    \"node_ip\": \"127.0.0.1\",\n    \"config\": {\n      \"Gradient_LR\": 0.035264496447493035,\n      \"Gradient_Momentum\": 0.9547666470861575,\n      \"Gradient_Steps\": 220,\n      \"Threshold\": 8,\n      \"merge_threshold\": 19,\n      \"number_of_points\": 800,\n      \"outliers_multiplier\": 2.552145544132814\n    },\n    \"time_since_restore\": 12.8282151222229,\n    \"iterations_since_restore\": 1,\n    \"experiment_tag\": \"18_Gradient_LR=0.0353,Gradient_Momentum=0.9548,Gradient_Steps=220,Threshold=8,merge_threshold=19,number_of_points=800,outliers_multiplier=2.5521\"\n  },\n  \"_default_result_or_future\": null,\n  \"last_update_time\": 1737596418.406534,\n  \"metric_analysis\": {\n    \"score\": {\n      \"max\": 0.8657938593931076,\n      \"min\": 0.8657938593931076,\n      \"avg\": 0.8657938593931076,\n      \"last\": 0.8657938593931076,\n      \"last-5-avg\": 0.8657938593931076,\n      \"last-10-avg\": 0.8657938593931076\n    },\n    \"time_this_iter_s\": {\n      \"max\": 12.8282151222229,\n      \"min\": 12.8282151222229,\n      \"avg\": 12.8282151222229,\n      \"last\": 12.8282151222229,\n      \"last-5-avg\": 12.8282151222229,\n      \"last-10-avg\": 12.8282151222229\n    },\n    \"done\": {\n      \"max\": false,\n      \"min\": false,\n      \"avg\": false,\n      \"last\": false,\n      \"last-5-avg\": false,\n      \"last-10-avg\": false\n    },\n    \"training_iteration\": {\n      \"max\": 1,\n      \"min\": 1,\n      \"avg\": 1,\n      \"last\": 1,\n      \"last-5-avg\": 1,\n      \"last-10-avg\": 1\n    },\n    \"time_total_s\": {\n      \"max\": 12.8282151222229,\n      \"min\": 12.8282151222229,\n      \"avg\": 12.8282151222229,\n      \"last\": 12.8282151222229,\n      \"last-5-avg\": 12.8282151222229,\n      \"last-10-avg\": 12.8282151222229\n    },\n    \"time_since_restore\": {\n      \"max\": 12.8282151222229,\n      \"min\": 12.8282151222229,\n      \"avg\": 12.8282151222229,\n      \"last\": 12.8282151222229,\n      \"last-5-avg\": 12.8282151222229,\n      \"last-10-avg\": 12.8282151222229\n    },\n    \"iterations_since_restore\": {\n      \"max\": 1,\n      \"min\": 1,\n      \"avg\": 1,\n      \"last\": 1,\n      \"last-5-avg\": 1,\n      \"last-10-avg\": 1\n    }\n  },\n  \"n_steps\": [\n    5,\n    10\n  ],\n  \"metric_n_steps\": {\n    \"score\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059589000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452948c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02663894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308fe78e55295b4eb3f9486945294612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059589000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452948c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02663894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308fe78e55295b4eb3f9486945294612e\"\n      }\n    },\n    \"time_this_iter_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294474029a80bd0000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294474029a80bd0000000612e\"\n      }\n    },\n    \"done\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059522000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529489612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059522000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529489612e\"\n      }\n    },\n    \"training_iteration\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b01612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b01612e\"\n      }\n    },\n    \"time_total_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294474029a80bd0000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294474029a80bd0000000612e\"\n      }\n    },\n    \"time_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294474029a80bd0000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294474029a80bd0000000612e\"\n      }\n    },\n    \"iterations_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b01612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b01612e\"\n      }\n    }\n  },\n  \"export_formats\": [],\n  \"status\": \"TERMINATED\",\n  \"start_time\": 1737596405.571479,\n  \"relative_logdir\": \"lambda_61832fd5_18_Gradient_LR=0.0353,Gradient_Momentum=0.9548,Gradient_Steps=220,Threshold=8,merge_threshold=19,number_of_points=_2025-01-23_02-40-05\",\n  \"runner\": null,\n  \"last_debug\": 0,\n  \"error_filename\": null,\n  \"pickled_error_filename\": null,\n  \"trial_name_creator\": null,\n  \"trial_dirname_creator\": null,\n  \"custom_trial_name\": null,\n  \"custom_dirname\": null,\n  \"experiment_dir_name\": \"test\",\n  \"saving_to\": null,\n  \"checkpoint_config\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"800595ff000000000000008c0e7261792e6169722e636f6e666967948c10436865636b706f696e74436f6e6669679493942981947d94288c0b6e756d5f746f5f6b656570944b038c1a636865636b706f696e745f73636f72655f617474726962757465948c12747261696e696e675f697465726174696f6e948c16636865636b706f696e745f73636f72655f6f72646572948c036d6178948c14636865636b706f696e745f6672657175656e6379944b008c11636865636b706f696e745f61745f656e64944e8c1a5f636865636b706f696e745f6b6565705f616c6c5f72616e6b7394898c1f5f636865636b706f696e745f75706c6f61645f66726f6d5f776f726b657273948975622e\"\n  },\n  \"checkpoint_manager\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"800595d2020000000000008c257261792e74756e652e657865637574696f6e2e636865636b706f696e745f6d616e61676572948c125f436865636b706f696e744d616e616765729493942981947d94288c145f636865636b706f696e745f7374726174656779948c0e7261792e6169722e636f6e666967948c10436865636b706f696e74436f6e6669679493942981947d94288c0b6e756d5f746f5f6b656570944b038c1a636865636b706f696e745f73636f72655f617474726962757465948c12747261696e696e675f697465726174696f6e948c16636865636b706f696e745f73636f72655f6f72646572948c036d6178948c14636865636b706f696e745f6672657175656e6379944b008c11636865636b706f696e745f61745f656e64944e8c1a5f636865636b706f696e745f6b6565705f616c6c5f72616e6b7394898c1f5f636865636b706f696e745f75706c6f61645f66726f6d5f776f726b657273948975628c155f6c61746573745f636865636b706f696e745f6964944b008c1a5f746f705f7065727369737465645f636865636b706f696e7473945d948c1a5f626573745f7065727369737465645f636865636b706f696e74944e8c1c5f6c61746573745f7065727369737465645f636865636b706f696e74944e8c195f6c61746573745f6d656d6f72795f636865636b706f696e74948c247261792e6169722e5f696e7465726e616c2e636865636b706f696e745f6d616e61676572948c125f547261636b6564436865636b706f696e749493942981947d94288c0b6469725f6f725f64617461944e8c026964944affffffff8c0c73746f726167655f6d6f646594681a8c11436865636b706f696e7453746f726167659493944b01859452948c0472616e6b944b008c076d657472696373947d948c076e6f64655f6970944e8c155f6d6f76655f696e73746561645f6f665f636f7079948875628c185f636865636b706f696e74735f746f5f636c65616e5f7570948f9475622e\"\n  },\n  \"restore_path\": null,\n  \"restoring_from\": null,\n  \"num_failures\": 0,\n  \"num_restore_failures\": 0,\n  \"results\": \"80054e2e\",\n  \"best_result\": \"80054e2e\",\n  \"param_config\": \"80054e2e\",\n  \"extra_arg\": \"80054e2e\",\n  \"_state_json\": null,\n  \"_state_valid\": false,\n  \"_resources\": \"80054e2e\"\n}",
    "{\n  \"stub\": false,\n  \"trainable_name\": \"lambda\",\n  \"trial_id\": \"79beb910\",\n  \"sync_config\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059593000000000000008c0f7261792e74756e652e73796e636572948c0a53796e63436f6e6669679493942981947d94288c0a75706c6f61645f646972944e8c0673796e636572948c046175746f948c0b73796e635f706572696f64944d2c018c0c73796e635f74696d656f7574944d08078c0e73796e635f61727469666163747394888c1273796e635f6f6e5f636865636b706f696e74948875622e\"\n  },\n  \"_orig_experiment_path\": \"/Users/kuba/ijcai_ski/ray_results/test\",\n  \"_orig_experiment_dir_name\": \"test\",\n  \"_local_experiment_path\": \"/Users/kuba/ijcai_ski/ray_results/test\",\n  \"_remote_experiment_path\": null,\n  \"config\": {\n    \"Gradient_LR\": 0.044069078932960803,\n    \"Gradient_Momentum\": 0.9903863423580542,\n    \"Gradient_Steps\": 240,\n    \"Threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b0074946243080f0000000000000094869452942e\"\n    },\n    \"merge_threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308080000000000000094869452942e\"\n    },\n    \"number_of_points\": 100,\n    \"outliers_multiplier\": 3.6279593594893442\n  },\n  \"_Trial__unresolved_config\": {\n    \"Gradient_LR\": 0.044069078932960803,\n    \"Gradient_Momentum\": 0.9903863423580542,\n    \"Gradient_Steps\": 240,\n    \"Threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b0074946243080f0000000000000094869452942e\"\n    },\n    \"merge_threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308080000000000000094869452942e\"\n    },\n    \"number_of_points\": 100,\n    \"outliers_multiplier\": 3.6279593594893442\n  },\n  \"evaluated_params\": {\n    \"Gradient_LR\": 0.044069078932960803,\n    \"Gradient_Momentum\": 0.9903863423580542,\n    \"Gradient_Steps\": 240,\n    \"Threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b0074946243080f0000000000000094869452942e\"\n    },\n    \"merge_threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308080000000000000094869452942e\"\n    },\n    \"number_of_points\": 100,\n    \"outliers_multiplier\": 3.6279593594893442\n  },\n  \"experiment_tag\": \"25_Gradient_LR=0.0441,Gradient_Momentum=0.9904,Gradient_Steps=240,Threshold=15,merge_threshold=8,number_of_points=100,outliers_multiplier=3.6280\",\n  \"location\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059546000000000000008c197261792e74756e652e6578706572696d656e742e747269616c948c095f4c6f636174696f6e9493942981947d94288c08686f73746e616d65944e8c03706964944e75622e\"\n  },\n  \"stopping_criterion\": {},\n  \"_setup_default_resource\": true,\n  \"_default_placement_group_factory\": \"800595aa000000000000008c237261792e74756e652e657865637574696f6e2e706c6163656d656e745f67726f757073948c15506c6163656d656e7447726f7570466163746f72799493942981947d94288c085f62756e646c6573945d947d948c0343505594473ff000000000000073618c155f686561645f62756e646c655f69735f656d70747994898c095f7374726174656779948c045041434b948c055f6172677394298c075f6b7761726773947d9475622e\",\n  \"placement_group_factory\": \"800595aa000000000000008c237261792e74756e652e657865637574696f6e2e706c6163656d656e745f67726f757073948c15506c6163656d656e7447726f7570466163746f72799493942981947d94288c085f62756e646c6573945d947d948c0343505594473ff000000000000073618c155f686561645f62756e646c655f69735f656d70747994898c095f7374726174656779948c045041434b948c055f6172677394298c075f6b7761726773947d9475622e\",\n  \"log_to_file\": [\n    null,\n    null\n  ],\n  \"max_failures\": 0,\n  \"_last_result\": {\n    \"score\": 0.853024641854932,\n    \"time_this_iter_s\": 13.245805740356445,\n    \"done\": true,\n    \"training_iteration\": 1,\n    \"trial_id\": \"79beb910\",\n    \"date\": \"2025-01-23_02-40-31\",\n    \"timestamp\": 1737596431,\n    \"time_total_s\": 13.245805740356445,\n    \"pid\": 30510,\n    \"hostname\": \"MacBook-Pro.ds19.agh.edu.pl\",\n    \"node_ip\": \"127.0.0.1\",\n    \"config\": {\n      \"Gradient_LR\": 0.044069078932960803,\n      \"Gradient_Momentum\": 0.9903863423580542,\n      \"Gradient_Steps\": 240,\n      \"Threshold\": 15,\n      \"merge_threshold\": 8,\n      \"number_of_points\": 100,\n      \"outliers_multiplier\": 3.6279593594893442\n    },\n    \"time_since_restore\": 13.245805740356445,\n    \"iterations_since_restore\": 1,\n    \"experiment_tag\": \"25_Gradient_LR=0.0441,Gradient_Momentum=0.9904,Gradient_Steps=240,Threshold=15,merge_threshold=8,number_of_points=100,outliers_multiplier=3.6280\"\n  },\n  \"_default_result_or_future\": null,\n  \"last_update_time\": 1737596431.68641,\n  \"metric_analysis\": {\n    \"score\": {\n      \"max\": 0.853024641854932,\n      \"min\": 0.853024641854932,\n      \"avg\": 0.853024641854932,\n      \"last\": 0.853024641854932,\n      \"last-5-avg\": 0.853024641854932,\n      \"last-10-avg\": 0.853024641854932\n    },\n    \"time_this_iter_s\": {\n      \"max\": 13.245805740356445,\n      \"min\": 13.245805740356445,\n      \"avg\": 13.245805740356445,\n      \"last\": 13.245805740356445,\n      \"last-5-avg\": 13.245805740356445,\n      \"last-10-avg\": 13.245805740356445\n    },\n    \"done\": {\n      \"max\": false,\n      \"min\": false,\n      \"avg\": false,\n      \"last\": false,\n      \"last-5-avg\": false,\n      \"last-10-avg\": false\n    },\n    \"training_iteration\": {\n      \"max\": 1,\n      \"min\": 1,\n      \"avg\": 1,\n      \"last\": 1,\n      \"last-5-avg\": 1,\n      \"last-10-avg\": 1\n    },\n    \"time_total_s\": {\n      \"max\": 13.245805740356445,\n      \"min\": 13.245805740356445,\n      \"avg\": 13.245805740356445,\n      \"last\": 13.245805740356445,\n      \"last-5-avg\": 13.245805740356445,\n      \"last-10-avg\": 13.245805740356445\n    },\n    \"time_since_restore\": {\n      \"max\": 13.245805740356445,\n      \"min\": 13.245805740356445,\n      \"avg\": 13.245805740356445,\n      \"last\": 13.245805740356445,\n      \"last-5-avg\": 13.245805740356445,\n      \"last-10-avg\": 13.245805740356445\n    },\n    \"iterations_since_restore\": {\n      \"max\": 1,\n      \"min\": 1,\n      \"avg\": 1,\n      \"last\": 1,\n      \"last-5-avg\": 1,\n      \"last-10-avg\": 1\n    }\n  },\n  \"n_steps\": [\n    5,\n    10\n  ],\n  \"metric_n_steps\": {\n    \"score\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059589000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452948c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02663894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308955e6e55fa4beb3f9486945294612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059589000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452948c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02663894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308955e6e55fa4beb3f9486945294612e\"\n      }\n    },\n    \"time_this_iter_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529447402a7dda40000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529447402a7dda40000000612e\"\n      }\n    },\n    \"done\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059522000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529489612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059522000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529489612e\"\n      }\n    },\n    \"training_iteration\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b01612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b01612e\"\n      }\n    },\n    \"time_total_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529447402a7dda40000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529447402a7dda40000000612e\"\n      }\n    },\n    \"time_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529447402a7dda40000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529447402a7dda40000000612e\"\n      }\n    },\n    \"iterations_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b01612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b01612e\"\n      }\n    }\n  },\n  \"export_formats\": [],\n  \"status\": \"TERMINATED\",\n  \"start_time\": 1737596418.419609,\n  \"relative_logdir\": \"lambda_79beb910_25_Gradient_LR=0.0441,Gradient_Momentum=0.9904,Gradient_Steps=240,Threshold=15,merge_threshold=8,number_of_points=_2025-01-23_02-40-16\",\n  \"runner\": null,\n  \"last_debug\": 0,\n  \"error_filename\": null,\n  \"pickled_error_filename\": null,\n  \"trial_name_creator\": null,\n  \"trial_dirname_creator\": null,\n  \"custom_trial_name\": null,\n  \"custom_dirname\": null,\n  \"experiment_dir_name\": \"test\",\n  \"saving_to\": null,\n  \"checkpoint_config\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"800595ff000000000000008c0e7261792e6169722e636f6e666967948c10436865636b706f696e74436f6e6669679493942981947d94288c0b6e756d5f746f5f6b656570944b038c1a636865636b706f696e745f73636f72655f617474726962757465948c12747261696e696e675f697465726174696f6e948c16636865636b706f696e745f73636f72655f6f72646572948c036d6178948c14636865636b706f696e745f6672657175656e6379944b008c11636865636b706f696e745f61745f656e64944e8c1a5f636865636b706f696e745f6b6565705f616c6c5f72616e6b7394898c1f5f636865636b706f696e745f75706c6f61645f66726f6d5f776f726b657273948975622e\"\n  },\n  \"checkpoint_manager\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"800595d2020000000000008c257261792e74756e652e657865637574696f6e2e636865636b706f696e745f6d616e61676572948c125f436865636b706f696e744d616e616765729493942981947d94288c145f636865636b706f696e745f7374726174656779948c0e7261792e6169722e636f6e666967948c10436865636b706f696e74436f6e6669679493942981947d94288c0b6e756d5f746f5f6b656570944b038c1a636865636b706f696e745f73636f72655f617474726962757465948c12747261696e696e675f697465726174696f6e948c16636865636b706f696e745f73636f72655f6f72646572948c036d6178948c14636865636b706f696e745f6672657175656e6379944b008c11636865636b706f696e745f61745f656e64944e8c1a5f636865636b706f696e745f6b6565705f616c6c5f72616e6b7394898c1f5f636865636b706f696e745f75706c6f61645f66726f6d5f776f726b657273948975628c155f6c61746573745f636865636b706f696e745f6964944b008c1a5f746f705f7065727369737465645f636865636b706f696e7473945d948c1a5f626573745f7065727369737465645f636865636b706f696e74944e8c1c5f6c61746573745f7065727369737465645f636865636b706f696e74944e8c195f6c61746573745f6d656d6f72795f636865636b706f696e74948c247261792e6169722e5f696e7465726e616c2e636865636b706f696e745f6d616e61676572948c125f547261636b6564436865636b706f696e749493942981947d94288c0b6469725f6f725f64617461944e8c026964944affffffff8c0c73746f726167655f6d6f646594681a8c11436865636b706f696e7453746f726167659493944b01859452948c0472616e6b944b008c076d657472696373947d948c076e6f64655f6970944e8c155f6d6f76655f696e73746561645f6f665f636f7079948875628c185f636865636b706f696e74735f746f5f636c65616e5f7570948f9475622e\"\n  },\n  \"restore_path\": null,\n  \"restoring_from\": null,\n  \"num_failures\": 0,\n  \"num_restore_failures\": 0,\n  \"results\": \"80054e2e\",\n  \"best_result\": \"80054e2e\",\n  \"param_config\": \"80054e2e\",\n  \"extra_arg\": \"80054e2e\",\n  \"_state_json\": null,\n  \"_state_valid\": false,\n  \"_resources\": \"80054e2e\"\n}",
    "{\n  \"stub\": false,\n  \"trainable_name\": \"lambda\",\n  \"trial_id\": \"e363bba5\",\n  \"sync_config\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059593000000000000008c0f7261792e74756e652e73796e636572948c0a53796e63436f6e6669679493942981947d94288c0a75706c6f61645f646972944e8c0673796e636572948c046175746f948c0b73796e635f706572696f64944d2c018c0c73796e635f74696d656f7574944d08078c0e73796e635f61727469666163747394888c1273796e635f6f6e5f636865636b706f696e74948875622e\"\n  },\n  \"_orig_experiment_path\": \"/Users/kuba/ijcai_ski/ray_results/test\",\n  \"_orig_experiment_dir_name\": \"test\",\n  \"_local_experiment_path\": \"/Users/kuba/ijcai_ski/ray_results/test\",\n  \"_remote_experiment_path\": null,\n  \"config\": {\n    \"Gradient_LR\": 0.022056516892803416,\n    \"Gradient_Momentum\": 0.9713021055955122,\n    \"Gradient_Steps\": 100,\n    \"Threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308120000000000000094869452942e\"\n    },\n    \"merge_threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b0074946243080d0000000000000094869452942e\"\n    },\n    \"number_of_points\": 300,\n    \"outliers_multiplier\": 4.2036420443465055\n  },\n  \"_Trial__unresolved_config\": {\n    \"Gradient_LR\": 0.022056516892803416,\n    \"Gradient_Momentum\": 0.9713021055955122,\n    \"Gradient_Steps\": 100,\n    \"Threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308120000000000000094869452942e\"\n    },\n    \"merge_threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b0074946243080d0000000000000094869452942e\"\n    },\n    \"number_of_points\": 300,\n    \"outliers_multiplier\": 4.2036420443465055\n  },\n  \"evaluated_params\": {\n    \"Gradient_LR\": 0.022056516892803416,\n    \"Gradient_Momentum\": 0.9713021055955122,\n    \"Gradient_Steps\": 100,\n    \"Threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308120000000000000094869452942e\"\n    },\n    \"merge_threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b0074946243080d0000000000000094869452942e\"\n    },\n    \"number_of_points\": 300,\n    \"outliers_multiplier\": 4.2036420443465055\n  },\n  \"experiment_tag\": \"24_Gradient_LR=0.0221,Gradient_Momentum=0.9713,Gradient_Steps=100,Threshold=18,merge_threshold=13,number_of_points=300,outliers_multiplier=4.2036\",\n  \"location\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059546000000000000008c197261792e74756e652e6578706572696d656e742e747269616c948c095f4c6f636174696f6e9493942981947d94288c08686f73746e616d65944e8c03706964944e75622e\"\n  },\n  \"stopping_criterion\": {},\n  \"_setup_default_resource\": true,\n  \"_default_placement_group_factory\": \"800595aa000000000000008c237261792e74756e652e657865637574696f6e2e706c6163656d656e745f67726f757073948c15506c6163656d656e7447726f7570466163746f72799493942981947d94288c085f62756e646c6573945d947d948c0343505594473ff000000000000073618c155f686561645f62756e646c655f69735f656d70747994898c095f7374726174656779948c045041434b948c055f6172677394298c075f6b7761726773947d9475622e\",\n  \"placement_group_factory\": \"800595aa000000000000008c237261792e74756e652e657865637574696f6e2e706c6163656d656e745f67726f757073948c15506c6163656d656e7447726f7570466163746f72799493942981947d94288c085f62756e646c6573945d947d948c0343505594473ff000000000000073618c155f686561645f62756e646c655f69735f656d70747994898c095f7374726174656779948c045041434b948c055f6172677394298c075f6b7761726773947d9475622e\",\n  \"log_to_file\": [\n    null,\n    null\n  ],\n  \"max_failures\": 0,\n  \"_last_result\": {\n    \"score\": 0.8304103317810798,\n    \"time_this_iter_s\": 10.338128089904785,\n    \"done\": true,\n    \"training_iteration\": 1,\n    \"trial_id\": \"e363bba5\",\n    \"date\": \"2025-01-23_02-40-27\",\n    \"timestamp\": 1737596427,\n    \"time_total_s\": 10.338128089904785,\n    \"pid\": 30566,\n    \"hostname\": \"MacBook-Pro.ds19.agh.edu.pl\",\n    \"node_ip\": \"127.0.0.1\",\n    \"config\": {\n      \"Gradient_LR\": 0.022056516892803416,\n      \"Gradient_Momentum\": 0.9713021055955122,\n      \"Gradient_Steps\": 100,\n      \"Threshold\": 18,\n      \"merge_threshold\": 13,\n      \"number_of_points\": 300,\n      \"outliers_multiplier\": 4.2036420443465055\n    },\n    \"time_since_restore\": 10.338128089904785,\n    \"iterations_since_restore\": 1,\n    \"experiment_tag\": \"24_Gradient_LR=0.0221,Gradient_Momentum=0.9713,Gradient_Steps=100,Threshold=18,merge_threshold=13,number_of_points=300,outliers_multiplier=4.2036\"\n  },\n  \"_default_result_or_future\": null,\n  \"last_update_time\": 1737596427.0353122,\n  \"metric_analysis\": {\n    \"score\": {\n      \"max\": 0.8304103317810798,\n      \"min\": 0.8304103317810798,\n      \"avg\": 0.8304103317810798,\n      \"last\": 0.8304103317810798,\n      \"last-5-avg\": 0.8304103317810798,\n      \"last-10-avg\": 0.8304103317810798\n    },\n    \"time_this_iter_s\": {\n      \"max\": 10.338128089904785,\n      \"min\": 10.338128089904785,\n      \"avg\": 10.338128089904785,\n      \"last\": 10.338128089904785,\n      \"last-5-avg\": 10.338128089904785,\n      \"last-10-avg\": 10.338128089904785\n    },\n    \"done\": {\n      \"max\": false,\n      \"min\": false,\n      \"avg\": false,\n      \"last\": false,\n      \"last-5-avg\": false,\n      \"last-10-avg\": false\n    },\n    \"training_iteration\": {\n      \"max\": 1,\n      \"min\": 1,\n      \"avg\": 1,\n      \"last\": 1,\n      \"last-5-avg\": 1,\n      \"last-10-avg\": 1\n    },\n    \"time_total_s\": {\n      \"max\": 10.338128089904785,\n      \"min\": 10.338128089904785,\n      \"avg\": 10.338128089904785,\n      \"last\": 10.338128089904785,\n      \"last-5-avg\": 10.338128089904785,\n      \"last-10-avg\": 10.338128089904785\n    },\n    \"time_since_restore\": {\n      \"max\": 10.338128089904785,\n      \"min\": 10.338128089904785,\n      \"avg\": 10.338128089904785,\n      \"last\": 10.338128089904785,\n      \"last-5-avg\": 10.338128089904785,\n      \"last-10-avg\": 10.338128089904785\n    },\n    \"iterations_since_restore\": {\n      \"max\": 1,\n      \"min\": 1,\n      \"avg\": 1,\n      \"last\": 1,\n      \"last-5-avg\": 1,\n      \"last-10-avg\": 1\n    }\n  },\n  \"n_steps\": [\n    5,\n    10\n  ],\n  \"metric_n_steps\": {\n    \"score\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059589000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452948c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02663894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308f25328b0b892ea3f9486945294612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059589000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452948c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02663894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308f25328b0b892ea3f9486945294612e\"\n      }\n    },\n    \"time_this_iter_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294474024ad1f20000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294474024ad1f20000000612e\"\n      }\n    },\n    \"done\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059522000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529489612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059522000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529489612e\"\n      }\n    },\n    \"training_iteration\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b01612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b01612e\"\n      }\n    },\n    \"time_total_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294474024ad1f20000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294474024ad1f20000000612e\"\n      }\n    },\n    \"time_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294474024ad1f20000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294474024ad1f20000000612e\"\n      }\n    },\n    \"iterations_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b01612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b01612e\"\n      }\n    }\n  },\n  \"export_formats\": [],\n  \"status\": \"TERMINATED\",\n  \"start_time\": 1737596416.686016,\n  \"relative_logdir\": \"lambda_e363bba5_24_Gradient_LR=0.0221,Gradient_Momentum=0.9713,Gradient_Steps=100,Threshold=18,merge_threshold=13,number_of_points_2025-01-23_02-40-16\",\n  \"runner\": null,\n  \"last_debug\": 0,\n  \"error_filename\": null,\n  \"pickled_error_filename\": null,\n  \"trial_name_creator\": null,\n  \"trial_dirname_creator\": null,\n  \"custom_trial_name\": null,\n  \"custom_dirname\": null,\n  \"experiment_dir_name\": \"test\",\n  \"saving_to\": null,\n  \"checkpoint_config\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"800595ff000000000000008c0e7261792e6169722e636f6e666967948c10436865636b706f696e74436f6e6669679493942981947d94288c0b6e756d5f746f5f6b656570944b038c1a636865636b706f696e745f73636f72655f617474726962757465948c12747261696e696e675f697465726174696f6e948c16636865636b706f696e745f73636f72655f6f72646572948c036d6178948c14636865636b706f696e745f6672657175656e6379944b008c11636865636b706f696e745f61745f656e64944e8c1a5f636865636b706f696e745f6b6565705f616c6c5f72616e6b7394898c1f5f636865636b706f696e745f75706c6f61645f66726f6d5f776f726b657273948975622e\"\n  },\n  \"checkpoint_manager\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"800595d2020000000000008c257261792e74756e652e657865637574696f6e2e636865636b706f696e745f6d616e61676572948c125f436865636b706f696e744d616e616765729493942981947d94288c145f636865636b706f696e745f7374726174656779948c0e7261792e6169722e636f6e666967948c10436865636b706f696e74436f6e6669679493942981947d94288c0b6e756d5f746f5f6b656570944b038c1a636865636b706f696e745f73636f72655f617474726962757465948c12747261696e696e675f697465726174696f6e948c16636865636b706f696e745f73636f72655f6f72646572948c036d6178948c14636865636b706f696e745f6672657175656e6379944b008c11636865636b706f696e745f61745f656e64944e8c1a5f636865636b706f696e745f6b6565705f616c6c5f72616e6b7394898c1f5f636865636b706f696e745f75706c6f61645f66726f6d5f776f726b657273948975628c155f6c61746573745f636865636b706f696e745f6964944b008c1a5f746f705f7065727369737465645f636865636b706f696e7473945d948c1a5f626573745f7065727369737465645f636865636b706f696e74944e8c1c5f6c61746573745f7065727369737465645f636865636b706f696e74944e8c195f6c61746573745f6d656d6f72795f636865636b706f696e74948c247261792e6169722e5f696e7465726e616c2e636865636b706f696e745f6d616e61676572948c125f547261636b6564436865636b706f696e749493942981947d94288c0b6469725f6f725f64617461944e8c026964944affffffff8c0c73746f726167655f6d6f646594681a8c11436865636b706f696e7453746f726167659493944b01859452948c0472616e6b944b008c076d657472696373947d948c076e6f64655f6970944e8c155f6d6f76655f696e73746561645f6f665f636f7079948875628c185f636865636b706f696e74735f746f5f636c65616e5f7570948f9475622e\"\n  },\n  \"restore_path\": null,\n  \"restoring_from\": null,\n  \"num_failures\": 0,\n  \"num_restore_failures\": 0,\n  \"results\": \"80054e2e\",\n  \"best_result\": \"80054e2e\",\n  \"param_config\": \"80054e2e\",\n  \"extra_arg\": \"80054e2e\",\n  \"_state_json\": null,\n  \"_state_valid\": false,\n  \"_resources\": \"80054e2e\"\n}",
    "{\n  \"stub\": false,\n  \"trainable_name\": \"lambda\",\n  \"trial_id\": \"a3e43489\",\n  \"sync_config\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059593000000000000008c0f7261792e74756e652e73796e636572948c0a53796e63436f6e6669679493942981947d94288c0a75706c6f61645f646972944e8c0673796e636572948c046175746f948c0b73796e635f706572696f64944d2c018c0c73796e635f74696d656f7574944d08078c0e73796e635f61727469666163747394888c1273796e635f6f6e5f636865636b706f696e74948875622e\"\n  },\n  \"_orig_experiment_path\": \"/Users/kuba/ijcai_ski/ray_results/test\",\n  \"_orig_experiment_dir_name\": \"test\",\n  \"_local_experiment_path\": \"/Users/kuba/ijcai_ski/ray_results/test\",\n  \"_remote_experiment_path\": null,\n  \"config\": {\n    \"Gradient_LR\": 0.04475581877080746,\n    \"Gradient_Momentum\": 0.9708908123827051,\n    \"Gradient_Steps\": 380,\n    \"Threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308100000000000000094869452942e\"\n    },\n    \"merge_threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b0074946243080f0000000000000094869452942e\"\n    },\n    \"number_of_points\": 300,\n    \"outliers_multiplier\": 3.711779398239135\n  },\n  \"_Trial__unresolved_config\": {\n    \"Gradient_LR\": 0.04475581877080746,\n    \"Gradient_Momentum\": 0.9708908123827051,\n    \"Gradient_Steps\": 380,\n    \"Threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308100000000000000094869452942e\"\n    },\n    \"merge_threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b0074946243080f0000000000000094869452942e\"\n    },\n    \"number_of_points\": 300,\n    \"outliers_multiplier\": 3.711779398239135\n  },\n  \"evaluated_params\": {\n    \"Gradient_LR\": 0.04475581877080746,\n    \"Gradient_Momentum\": 0.9708908123827051,\n    \"Gradient_Steps\": 380,\n    \"Threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308100000000000000094869452942e\"\n    },\n    \"merge_threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b0074946243080f0000000000000094869452942e\"\n    },\n    \"number_of_points\": 300,\n    \"outliers_multiplier\": 3.711779398239135\n  },\n  \"experiment_tag\": \"28_Gradient_LR=0.0448,Gradient_Momentum=0.9709,Gradient_Steps=380,Threshold=16,merge_threshold=15,number_of_points=300,outliers_multiplier=3.7118\",\n  \"location\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059546000000000000008c197261792e74756e652e6578706572696d656e742e747269616c948c095f4c6f636174696f6e9493942981947d94288c08686f73746e616d65944e8c03706964944e75622e\"\n  },\n  \"stopping_criterion\": {},\n  \"_setup_default_resource\": true,\n  \"_default_placement_group_factory\": \"800595aa000000000000008c237261792e74756e652e657865637574696f6e2e706c6163656d656e745f67726f757073948c15506c6163656d656e7447726f7570466163746f72799493942981947d94288c085f62756e646c6573945d947d948c0343505594473ff000000000000073618c155f686561645f62756e646c655f69735f656d70747994898c095f7374726174656779948c045041434b948c055f6172677394298c075f6b7761726773947d9475622e\",\n  \"placement_group_factory\": \"800595aa000000000000008c237261792e74756e652e657865637574696f6e2e706c6163656d656e745f67726f757073948c15506c6163656d656e7447726f7570466163746f72799493942981947d94288c085f62756e646c6573945d947d948c0343505594473ff000000000000073618c155f686561645f62756e646c655f69735f656d70747994898c095f7374726174656779948c045041434b948c055f6172677394298c075f6b7761726773947d9475622e\",\n  \"log_to_file\": [\n    null,\n    null\n  ],\n  \"max_failures\": 0,\n  \"_last_result\": {\n    \"score\": 0.8392376690869693,\n    \"time_this_iter_s\": 12.785298824310303,\n    \"done\": true,\n    \"training_iteration\": 1,\n    \"trial_id\": \"a3e43489\",\n    \"date\": \"2025-01-23_02-40-37\",\n    \"timestamp\": 1737596437,\n    \"time_total_s\": 12.785298824310303,\n    \"pid\": 30685,\n    \"hostname\": \"MacBook-Pro.ds19.agh.edu.pl\",\n    \"node_ip\": \"127.0.0.1\",\n    \"config\": {\n      \"Gradient_LR\": 0.04475581877080746,\n      \"Gradient_Momentum\": 0.9708908123827051,\n      \"Gradient_Steps\": 380,\n      \"Threshold\": 16,\n      \"merge_threshold\": 15,\n      \"number_of_points\": 300,\n      \"outliers_multiplier\": 3.711779398239135\n    },\n    \"time_since_restore\": 12.785298824310303,\n    \"iterations_since_restore\": 1,\n    \"experiment_tag\": \"28_Gradient_LR=0.0448,Gradient_Momentum=0.9709,Gradient_Steps=380,Threshold=16,merge_threshold=15,number_of_points=300,outliers_multiplier=3.7118\"\n  },\n  \"_default_result_or_future\": null,\n  \"last_update_time\": 1737596437.3987021,\n  \"metric_analysis\": {\n    \"score\": {\n      \"max\": 0.8392376690869693,\n      \"min\": 0.8392376690869693,\n      \"avg\": 0.8392376690869693,\n      \"last\": 0.8392376690869693,\n      \"last-5-avg\": 0.8392376690869693,\n      \"last-10-avg\": 0.8392376690869693\n    },\n    \"time_this_iter_s\": {\n      \"max\": 12.785298824310303,\n      \"min\": 12.785298824310303,\n      \"avg\": 12.785298824310303,\n      \"last\": 12.785298824310303,\n      \"last-5-avg\": 12.785298824310303,\n      \"last-10-avg\": 12.785298824310303\n    },\n    \"done\": {\n      \"max\": false,\n      \"min\": false,\n      \"avg\": false,\n      \"last\": false,\n      \"last-5-avg\": false,\n      \"last-10-avg\": false\n    },\n    \"training_iteration\": {\n      \"max\": 1,\n      \"min\": 1,\n      \"avg\": 1,\n      \"last\": 1,\n      \"last-5-avg\": 1,\n      \"last-10-avg\": 1\n    },\n    \"time_total_s\": {\n      \"max\": 12.785298824310303,\n      \"min\": 12.785298824310303,\n      \"avg\": 12.785298824310303,\n      \"last\": 12.785298824310303,\n      \"last-5-avg\": 12.785298824310303,\n      \"last-10-avg\": 12.785298824310303\n    },\n    \"time_since_restore\": {\n      \"max\": 12.785298824310303,\n      \"min\": 12.785298824310303,\n      \"avg\": 12.785298824310303,\n      \"last\": 12.785298824310303,\n      \"last-5-avg\": 12.785298824310303,\n      \"last-10-avg\": 12.785298824310303\n    },\n    \"iterations_since_restore\": {\n      \"max\": 1,\n      \"min\": 1,\n      \"avg\": 1,\n      \"last\": 1,\n      \"last-5-avg\": 1,\n      \"last-10-avg\": 1\n    }\n  },\n  \"n_steps\": [\n    5,\n    10\n  ],\n  \"metric_n_steps\": {\n    \"score\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059589000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452948c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02663894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308fd97c9f408dbea3f9486945294612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059589000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452948c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02663894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308fd97c9f408dbea3f9486945294612e\"\n      }\n    },\n    \"time_this_iter_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944740299212b0000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944740299212b0000000612e\"\n      }\n    },\n    \"done\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059522000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529489612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059522000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529489612e\"\n      }\n    },\n    \"training_iteration\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b01612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b01612e\"\n      }\n    },\n    \"time_total_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944740299212b0000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944740299212b0000000612e\"\n      }\n    },\n    \"time_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944740299212b0000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944740299212b0000000612e\"\n      }\n    },\n    \"iterations_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b01612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b01612e\"\n      }\n    }\n  },\n  \"export_formats\": [],\n  \"status\": \"TERMINATED\",\n  \"start_time\": 1737596424.603928,\n  \"relative_logdir\": \"lambda_a3e43489_28_Gradient_LR=0.0448,Gradient_Momentum=0.9709,Gradient_Steps=380,Threshold=16,merge_threshold=15,number_of_points_2025-01-23_02-40-22\",\n  \"runner\": null,\n  \"last_debug\": 0,\n  \"error_filename\": null,\n  \"pickled_error_filename\": null,\n  \"trial_name_creator\": null,\n  \"trial_dirname_creator\": null,\n  \"custom_trial_name\": null,\n  \"custom_dirname\": null,\n  \"experiment_dir_name\": \"test\",\n  \"saving_to\": null,\n  \"checkpoint_config\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"800595ff000000000000008c0e7261792e6169722e636f6e666967948c10436865636b706f696e74436f6e6669679493942981947d94288c0b6e756d5f746f5f6b656570944b038c1a636865636b706f696e745f73636f72655f617474726962757465948c12747261696e696e675f697465726174696f6e948c16636865636b706f696e745f73636f72655f6f72646572948c036d6178948c14636865636b706f696e745f6672657175656e6379944b008c11636865636b706f696e745f61745f656e64944e8c1a5f636865636b706f696e745f6b6565705f616c6c5f72616e6b7394898c1f5f636865636b706f696e745f75706c6f61645f66726f6d5f776f726b657273948975622e\"\n  },\n  \"checkpoint_manager\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"800595d2020000000000008c257261792e74756e652e657865637574696f6e2e636865636b706f696e745f6d616e61676572948c125f436865636b706f696e744d616e616765729493942981947d94288c145f636865636b706f696e745f7374726174656779948c0e7261792e6169722e636f6e666967948c10436865636b706f696e74436f6e6669679493942981947d94288c0b6e756d5f746f5f6b656570944b038c1a636865636b706f696e745f73636f72655f617474726962757465948c12747261696e696e675f697465726174696f6e948c16636865636b706f696e745f73636f72655f6f72646572948c036d6178948c14636865636b706f696e745f6672657175656e6379944b008c11636865636b706f696e745f61745f656e64944e8c1a5f636865636b706f696e745f6b6565705f616c6c5f72616e6b7394898c1f5f636865636b706f696e745f75706c6f61645f66726f6d5f776f726b657273948975628c155f6c61746573745f636865636b706f696e745f6964944b008c1a5f746f705f7065727369737465645f636865636b706f696e7473945d948c1a5f626573745f7065727369737465645f636865636b706f696e74944e8c1c5f6c61746573745f7065727369737465645f636865636b706f696e74944e8c195f6c61746573745f6d656d6f72795f636865636b706f696e74948c247261792e6169722e5f696e7465726e616c2e636865636b706f696e745f6d616e61676572948c125f547261636b6564436865636b706f696e749493942981947d94288c0b6469725f6f725f64617461944e8c026964944affffffff8c0c73746f726167655f6d6f646594681a8c11436865636b706f696e7453746f726167659493944b01859452948c0472616e6b944b008c076d657472696373947d948c076e6f64655f6970944e8c155f6d6f76655f696e73746561645f6f665f636f7079948875628c185f636865636b706f696e74735f746f5f636c65616e5f7570948f9475622e\"\n  },\n  \"restore_path\": null,\n  \"restoring_from\": null,\n  \"num_failures\": 0,\n  \"num_restore_failures\": 0,\n  \"results\": \"80054e2e\",\n  \"best_result\": \"80054e2e\",\n  \"param_config\": \"80054e2e\",\n  \"extra_arg\": \"80054e2e\",\n  \"_state_json\": null,\n  \"_state_valid\": false,\n  \"_resources\": \"80054e2e\"\n}",
    "{\n  \"stub\": false,\n  \"trainable_name\": \"lambda\",\n  \"trial_id\": \"41ec8c77\",\n  \"sync_config\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059593000000000000008c0f7261792e74756e652e73796e636572948c0a53796e63436f6e6669679493942981947d94288c0a75706c6f61645f646972944e8c0673796e636572948c046175746f948c0b73796e635f706572696f64944d2c018c0c73796e635f74696d656f7574944d08078c0e73796e635f61727469666163747394888c1273796e635f6f6e5f636865636b706f696e74948875622e\"\n  },\n  \"_orig_experiment_path\": \"/Users/kuba/ijcai_ski/ray_results/test\",\n  \"_orig_experiment_dir_name\": \"test\",\n  \"_local_experiment_path\": \"/Users/kuba/ijcai_ski/ray_results/test\",\n  \"_remote_experiment_path\": null,\n  \"config\": {\n    \"Gradient_LR\": 0.09766901474175475,\n    \"Gradient_Momentum\": 0.9987512457041473,\n    \"Gradient_Steps\": 280,\n    \"Threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b0074946243080b0000000000000094869452942e\"\n    },\n    \"merge_threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b0074946243080a0000000000000094869452942e\"\n    },\n    \"number_of_points\": 900,\n    \"outliers_multiplier\": 4.360599092057763\n  },\n  \"_Trial__unresolved_config\": {\n    \"Gradient_LR\": 0.09766901474175475,\n    \"Gradient_Momentum\": 0.9987512457041473,\n    \"Gradient_Steps\": 280,\n    \"Threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b0074946243080b0000000000000094869452942e\"\n    },\n    \"merge_threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b0074946243080a0000000000000094869452942e\"\n    },\n    \"number_of_points\": 900,\n    \"outliers_multiplier\": 4.360599092057763\n  },\n  \"evaluated_params\": {\n    \"Gradient_LR\": 0.09766901474175475,\n    \"Gradient_Momentum\": 0.9987512457041473,\n    \"Gradient_Steps\": 280,\n    \"Threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b0074946243080b0000000000000094869452942e\"\n    },\n    \"merge_threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b0074946243080a0000000000000094869452942e\"\n    },\n    \"number_of_points\": 900,\n    \"outliers_multiplier\": 4.360599092057763\n  },\n  \"experiment_tag\": \"27_Gradient_LR=0.0977,Gradient_Momentum=0.9988,Gradient_Steps=280,Threshold=11,merge_threshold=10,number_of_points=900,outliers_multiplier=4.3606\",\n  \"location\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059546000000000000008c197261792e74756e652e6578706572696d656e742e747269616c948c095f4c6f636174696f6e9493942981947d94288c08686f73746e616d65944e8c03706964944e75622e\"\n  },\n  \"stopping_criterion\": {},\n  \"_setup_default_resource\": true,\n  \"_default_placement_group_factory\": \"800595aa000000000000008c237261792e74756e652e657865637574696f6e2e706c6163656d656e745f67726f757073948c15506c6163656d656e7447726f7570466163746f72799493942981947d94288c085f62756e646c6573945d947d948c0343505594473ff000000000000073618c155f686561645f62756e646c655f69735f656d70747994898c095f7374726174656779948c045041434b948c055f6172677394298c075f6b7761726773947d9475622e\",\n  \"placement_group_factory\": \"800595aa000000000000008c237261792e74756e652e657865637574696f6e2e706c6163656d656e745f67726f757073948c15506c6163656d656e7447726f7570466163746f72799493942981947d94288c085f62756e646c6573945d947d948c0343505594473ff000000000000073618c155f686561645f62756e646c655f69735f656d70747994898c095f7374726174656779948c045041434b948c055f6172677394298c075f6b7761726773947d9475622e\",\n  \"log_to_file\": [\n    null,\n    null\n  ],\n  \"max_failures\": 0,\n  \"_last_result\": {\n    \"score\": 0.2909595354797482,\n    \"time_this_iter_s\": 10.523498058319092,\n    \"done\": true,\n    \"training_iteration\": 1,\n    \"trial_id\": \"41ec8c77\",\n    \"date\": \"2025-01-23_02-40-32\",\n    \"timestamp\": 1737596432,\n    \"time_total_s\": 10.523498058319092,\n    \"pid\": 30525,\n    \"hostname\": \"MacBook-Pro.ds19.agh.edu.pl\",\n    \"node_ip\": \"127.0.0.1\",\n    \"config\": {\n      \"Gradient_LR\": 0.09766901474175475,\n      \"Gradient_Momentum\": 0.9987512457041473,\n      \"Gradient_Steps\": 280,\n      \"Threshold\": 11,\n      \"merge_threshold\": 10,\n      \"number_of_points\": 900,\n      \"outliers_multiplier\": 4.360599092057763\n    },\n    \"time_since_restore\": 10.523498058319092,\n    \"iterations_since_restore\": 1,\n    \"experiment_tag\": \"27_Gradient_LR=0.0977,Gradient_Momentum=0.9988,Gradient_Steps=280,Threshold=11,merge_threshold=10,number_of_points=900,outliers_multiplier=4.3606\"\n  },\n  \"_default_result_or_future\": null,\n  \"last_update_time\": 1737596432.5761669,\n  \"metric_analysis\": {\n    \"score\": {\n      \"max\": 0.2909595354797482,\n      \"min\": 0.2909595354797482,\n      \"avg\": 0.2909595354797482,\n      \"last\": 0.2909595354797482,\n      \"last-5-avg\": 0.2909595354797482,\n      \"last-10-avg\": 0.2909595354797482\n    },\n    \"time_this_iter_s\": {\n      \"max\": 10.523498058319092,\n      \"min\": 10.523498058319092,\n      \"avg\": 10.523498058319092,\n      \"last\": 10.523498058319092,\n      \"last-5-avg\": 10.523498058319092,\n      \"last-10-avg\": 10.523498058319092\n    },\n    \"done\": {\n      \"max\": false,\n      \"min\": false,\n      \"avg\": false,\n      \"last\": false,\n      \"last-5-avg\": false,\n      \"last-10-avg\": false\n    },\n    \"training_iteration\": {\n      \"max\": 1,\n      \"min\": 1,\n      \"avg\": 1,\n      \"last\": 1,\n      \"last-5-avg\": 1,\n      \"last-10-avg\": 1\n    },\n    \"time_total_s\": {\n      \"max\": 10.523498058319092,\n      \"min\": 10.523498058319092,\n      \"avg\": 10.523498058319092,\n      \"last\": 10.523498058319092,\n      \"last-5-avg\": 10.523498058319092,\n      \"last-10-avg\": 10.523498058319092\n    },\n    \"time_since_restore\": {\n      \"max\": 10.523498058319092,\n      \"min\": 10.523498058319092,\n      \"avg\": 10.523498058319092,\n      \"last\": 10.523498058319092,\n      \"last-5-avg\": 10.523498058319092,\n      \"last-10-avg\": 10.523498058319092\n    },\n    \"iterations_since_restore\": {\n      \"max\": 1,\n      \"min\": 1,\n      \"avg\": 1,\n      \"last\": 1,\n      \"last-5-avg\": 1,\n      \"last-10-avg\": 1\n    }\n  },\n  \"n_steps\": [\n    5,\n    10\n  ],\n  \"metric_n_steps\": {\n    \"score\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059589000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452948c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02663894898887945294284b038c013c944e4e4e4affffffff4affffffff4b0074946243085a1256be149fd23f9486945294612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059589000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452948c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02663894898887945294284b038c013c944e4e4e4affffffff4affffffff4b0074946243085a1256be149fd23f9486945294612e\"\n      }\n    },\n    \"time_this_iter_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944740250c07f0000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944740250c07f0000000612e\"\n      }\n    },\n    \"done\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059522000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529489612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059522000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529489612e\"\n      }\n    },\n    \"training_iteration\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b01612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b01612e\"\n      }\n    },\n    \"time_total_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944740250c07f0000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944740250c07f0000000612e\"\n      }\n    },\n    \"time_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944740250c07f0000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944740250c07f0000000612e\"\n      }\n    },\n    \"iterations_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b01612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b01612e\"\n      }\n    }\n  },\n  \"export_formats\": [],\n  \"status\": \"TERMINATED\",\n  \"start_time\": 1737596422.039438,\n  \"relative_logdir\": \"lambda_41ec8c77_27_Gradient_LR=0.0977,Gradient_Momentum=0.9988,Gradient_Steps=280,Threshold=11,merge_threshold=10,number_of_points_2025-01-23_02-40-21\",\n  \"runner\": null,\n  \"last_debug\": 0,\n  \"error_filename\": null,\n  \"pickled_error_filename\": null,\n  \"trial_name_creator\": null,\n  \"trial_dirname_creator\": null,\n  \"custom_trial_name\": null,\n  \"custom_dirname\": null,\n  \"experiment_dir_name\": \"test\",\n  \"saving_to\": null,\n  \"checkpoint_config\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"800595ff000000000000008c0e7261792e6169722e636f6e666967948c10436865636b706f696e74436f6e6669679493942981947d94288c0b6e756d5f746f5f6b656570944b038c1a636865636b706f696e745f73636f72655f617474726962757465948c12747261696e696e675f697465726174696f6e948c16636865636b706f696e745f73636f72655f6f72646572948c036d6178948c14636865636b706f696e745f6672657175656e6379944b008c11636865636b706f696e745f61745f656e64944e8c1a5f636865636b706f696e745f6b6565705f616c6c5f72616e6b7394898c1f5f636865636b706f696e745f75706c6f61645f66726f6d5f776f726b657273948975622e\"\n  },\n  \"checkpoint_manager\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"800595d2020000000000008c257261792e74756e652e657865637574696f6e2e636865636b706f696e745f6d616e61676572948c125f436865636b706f696e744d616e616765729493942981947d94288c145f636865636b706f696e745f7374726174656779948c0e7261792e6169722e636f6e666967948c10436865636b706f696e74436f6e6669679493942981947d94288c0b6e756d5f746f5f6b656570944b038c1a636865636b706f696e745f73636f72655f617474726962757465948c12747261696e696e675f697465726174696f6e948c16636865636b706f696e745f73636f72655f6f72646572948c036d6178948c14636865636b706f696e745f6672657175656e6379944b008c11636865636b706f696e745f61745f656e64944e8c1a5f636865636b706f696e745f6b6565705f616c6c5f72616e6b7394898c1f5f636865636b706f696e745f75706c6f61645f66726f6d5f776f726b657273948975628c155f6c61746573745f636865636b706f696e745f6964944b008c1a5f746f705f7065727369737465645f636865636b706f696e7473945d948c1a5f626573745f7065727369737465645f636865636b706f696e74944e8c1c5f6c61746573745f7065727369737465645f636865636b706f696e74944e8c195f6c61746573745f6d656d6f72795f636865636b706f696e74948c247261792e6169722e5f696e7465726e616c2e636865636b706f696e745f6d616e61676572948c125f547261636b6564436865636b706f696e749493942981947d94288c0b6469725f6f725f64617461944e8c026964944affffffff8c0c73746f726167655f6d6f646594681a8c11436865636b706f696e7453746f726167659493944b01859452948c0472616e6b944b008c076d657472696373947d948c076e6f64655f6970944e8c155f6d6f76655f696e73746561645f6f665f636f7079948875628c185f636865636b706f696e74735f746f5f636c65616e5f7570948f9475622e\"\n  },\n  \"restore_path\": null,\n  \"restoring_from\": null,\n  \"num_failures\": 0,\n  \"num_restore_failures\": 0,\n  \"results\": \"80054e2e\",\n  \"best_result\": \"80054e2e\",\n  \"param_config\": \"80054e2e\",\n  \"extra_arg\": \"80054e2e\",\n  \"_state_json\": null,\n  \"_state_valid\": false,\n  \"_resources\": \"80054e2e\"\n}",
    "{\n  \"stub\": false,\n  \"trainable_name\": \"lambda\",\n  \"trial_id\": \"bbc023e5\",\n  \"sync_config\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059593000000000000008c0f7261792e74756e652e73796e636572948c0a53796e63436f6e6669679493942981947d94288c0a75706c6f61645f646972944e8c0673796e636572948c046175746f948c0b73796e635f706572696f64944d2c018c0c73796e635f74696d656f7574944d08078c0e73796e635f61727469666163747394888c1273796e635f6f6e5f636865636b706f696e74948875622e\"\n  },\n  \"_orig_experiment_path\": \"/Users/kuba/ijcai_ski/ray_results/test\",\n  \"_orig_experiment_dir_name\": \"test\",\n  \"_local_experiment_path\": \"/Users/kuba/ijcai_ski/ray_results/test\",\n  \"_remote_experiment_path\": null,\n  \"config\": {\n    \"Gradient_LR\": 0.06726589280771597,\n    \"Gradient_Momentum\": 0.982526544494107,\n    \"Gradient_Steps\": 270,\n    \"Threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308110000000000000094869452942e\"\n    },\n    \"merge_threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b0074946243080e0000000000000094869452942e\"\n    },\n    \"number_of_points\": 200,\n    \"outliers_multiplier\": 3.978038331664771\n  },\n  \"_Trial__unresolved_config\": {\n    \"Gradient_LR\": 0.06726589280771597,\n    \"Gradient_Momentum\": 0.982526544494107,\n    \"Gradient_Steps\": 270,\n    \"Threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308110000000000000094869452942e\"\n    },\n    \"merge_threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b0074946243080e0000000000000094869452942e\"\n    },\n    \"number_of_points\": 200,\n    \"outliers_multiplier\": 3.978038331664771\n  },\n  \"evaluated_params\": {\n    \"Gradient_LR\": 0.06726589280771597,\n    \"Gradient_Momentum\": 0.982526544494107,\n    \"Gradient_Steps\": 270,\n    \"Threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308110000000000000094869452942e\"\n    },\n    \"merge_threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b0074946243080e0000000000000094869452942e\"\n    },\n    \"number_of_points\": 200,\n    \"outliers_multiplier\": 3.978038331664771\n  },\n  \"experiment_tag\": \"26_Gradient_LR=0.0673,Gradient_Momentum=0.9825,Gradient_Steps=270,Threshold=17,merge_threshold=14,number_of_points=200,outliers_multiplier=3.9780\",\n  \"location\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059546000000000000008c197261792e74756e652e6578706572696d656e742e747269616c948c095f4c6f636174696f6e9493942981947d94288c08686f73746e616d65944e8c03706964944e75622e\"\n  },\n  \"stopping_criterion\": {},\n  \"_setup_default_resource\": true,\n  \"_default_placement_group_factory\": \"800595aa000000000000008c237261792e74756e652e657865637574696f6e2e706c6163656d656e745f67726f757073948c15506c6163656d656e7447726f7570466163746f72799493942981947d94288c085f62756e646c6573945d947d948c0343505594473ff000000000000073618c155f686561645f62756e646c655f69735f656d70747994898c095f7374726174656779948c045041434b948c055f6172677394298c075f6b7761726773947d9475622e\",\n  \"placement_group_factory\": \"800595aa000000000000008c237261792e74756e652e657865637574696f6e2e706c6163656d656e745f67726f757073948c15506c6163656d656e7447726f7570466163746f72799493942981947d94288c085f62756e646c6573945d947d948c0343505594473ff000000000000073618c155f686561645f62756e646c655f69735f656d70747994898c095f7374726174656779948c045041434b948c055f6172677394298c075f6b7761726773947d9475622e\",\n  \"log_to_file\": [\n    null,\n    null\n  ],\n  \"max_failures\": 0,\n  \"_last_result\": {\n    \"score\": 0.8083759298212648,\n    \"time_this_iter_s\": 12.28614592552185,\n    \"done\": true,\n    \"training_iteration\": 1,\n    \"trial_id\": \"bbc023e5\",\n    \"date\": \"2025-01-23_02-40-33\",\n    \"timestamp\": 1737596433,\n    \"time_total_s\": 12.28614592552185,\n    \"pid\": 30798,\n    \"hostname\": \"MacBook-Pro.ds19.agh.edu.pl\",\n    \"node_ip\": \"127.0.0.1\",\n    \"config\": {\n      \"Gradient_LR\": 0.06726589280771597,\n      \"Gradient_Momentum\": 0.982526544494107,\n      \"Gradient_Steps\": 270,\n      \"Threshold\": 17,\n      \"merge_threshold\": 14,\n      \"number_of_points\": 200,\n      \"outliers_multiplier\": 3.978038331664771\n    },\n    \"time_since_restore\": 12.28614592552185,\n    \"iterations_since_restore\": 1,\n    \"experiment_tag\": \"26_Gradient_LR=0.0673,Gradient_Momentum=0.9825,Gradient_Steps=270,Threshold=17,merge_threshold=14,number_of_points=200,outliers_multiplier=3.9780\"\n  },\n  \"_default_result_or_future\": null,\n  \"last_update_time\": 1737596433.967861,\n  \"metric_analysis\": {\n    \"score\": {\n      \"max\": 0.8083759298212648,\n      \"min\": 0.8083759298212648,\n      \"avg\": 0.8083759298212648,\n      \"last\": 0.8083759298212648,\n      \"last-5-avg\": 0.8083759298212648,\n      \"last-10-avg\": 0.8083759298212648\n    },\n    \"time_this_iter_s\": {\n      \"max\": 12.28614592552185,\n      \"min\": 12.28614592552185,\n      \"avg\": 12.28614592552185,\n      \"last\": 12.28614592552185,\n      \"last-5-avg\": 12.28614592552185,\n      \"last-10-avg\": 12.28614592552185\n    },\n    \"done\": {\n      \"max\": false,\n      \"min\": false,\n      \"avg\": false,\n      \"last\": false,\n      \"last-5-avg\": false,\n      \"last-10-avg\": false\n    },\n    \"training_iteration\": {\n      \"max\": 1,\n      \"min\": 1,\n      \"avg\": 1,\n      \"last\": 1,\n      \"last-5-avg\": 1,\n      \"last-10-avg\": 1\n    },\n    \"time_total_s\": {\n      \"max\": 12.28614592552185,\n      \"min\": 12.28614592552185,\n      \"avg\": 12.28614592552185,\n      \"last\": 12.28614592552185,\n      \"last-5-avg\": 12.28614592552185,\n      \"last-10-avg\": 12.28614592552185\n    },\n    \"time_since_restore\": {\n      \"max\": 12.28614592552185,\n      \"min\": 12.28614592552185,\n      \"avg\": 12.28614592552185,\n      \"last\": 12.28614592552185,\n      \"last-5-avg\": 12.28614592552185,\n      \"last-10-avg\": 12.28614592552185\n    },\n    \"iterations_since_restore\": {\n      \"max\": 1,\n      \"min\": 1,\n      \"avg\": 1,\n      \"last\": 1,\n      \"last-5-avg\": 1,\n      \"last-10-avg\": 1\n    }\n  },\n  \"n_steps\": [\n    5,\n    10\n  ],\n  \"metric_n_steps\": {\n    \"score\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059589000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452948c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02663894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308ed96ae3237dee93f9486945294612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059589000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452948c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02663894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308ed96ae3237dee93f9486945294612e\"\n      }\n    },\n    \"time_this_iter_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944740289281b8000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944740289281b8000000612e\"\n      }\n    },\n    \"done\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059522000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529489612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059522000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529489612e\"\n      }\n    },\n    \"training_iteration\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b01612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b01612e\"\n      }\n    },\n    \"time_total_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944740289281b8000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944740289281b8000000612e\"\n      }\n    },\n    \"time_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944740289281b8000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944740289281b8000000612e\"\n      }\n    },\n    \"iterations_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b01612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b01612e\"\n      }\n    }\n  },\n  \"export_formats\": [],\n  \"status\": \"TERMINATED\",\n  \"start_time\": 1737596421.669662,\n  \"relative_logdir\": \"lambda_bbc023e5_26_Gradient_LR=0.0673,Gradient_Momentum=0.9825,Gradient_Steps=270,Threshold=17,merge_threshold=14,number_of_points_2025-01-23_02-40-18\",\n  \"runner\": null,\n  \"last_debug\": 0,\n  \"error_filename\": null,\n  \"pickled_error_filename\": null,\n  \"trial_name_creator\": null,\n  \"trial_dirname_creator\": null,\n  \"custom_trial_name\": null,\n  \"custom_dirname\": null,\n  \"experiment_dir_name\": \"test\",\n  \"saving_to\": null,\n  \"checkpoint_config\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"800595ff000000000000008c0e7261792e6169722e636f6e666967948c10436865636b706f696e74436f6e6669679493942981947d94288c0b6e756d5f746f5f6b656570944b038c1a636865636b706f696e745f73636f72655f617474726962757465948c12747261696e696e675f697465726174696f6e948c16636865636b706f696e745f73636f72655f6f72646572948c036d6178948c14636865636b706f696e745f6672657175656e6379944b008c11636865636b706f696e745f61745f656e64944e8c1a5f636865636b706f696e745f6b6565705f616c6c5f72616e6b7394898c1f5f636865636b706f696e745f75706c6f61645f66726f6d5f776f726b657273948975622e\"\n  },\n  \"checkpoint_manager\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"800595d2020000000000008c257261792e74756e652e657865637574696f6e2e636865636b706f696e745f6d616e61676572948c125f436865636b706f696e744d616e616765729493942981947d94288c145f636865636b706f696e745f7374726174656779948c0e7261792e6169722e636f6e666967948c10436865636b706f696e74436f6e6669679493942981947d94288c0b6e756d5f746f5f6b656570944b038c1a636865636b706f696e745f73636f72655f617474726962757465948c12747261696e696e675f697465726174696f6e948c16636865636b706f696e745f73636f72655f6f72646572948c036d6178948c14636865636b706f696e745f6672657175656e6379944b008c11636865636b706f696e745f61745f656e64944e8c1a5f636865636b706f696e745f6b6565705f616c6c5f72616e6b7394898c1f5f636865636b706f696e745f75706c6f61645f66726f6d5f776f726b657273948975628c155f6c61746573745f636865636b706f696e745f6964944b008c1a5f746f705f7065727369737465645f636865636b706f696e7473945d948c1a5f626573745f7065727369737465645f636865636b706f696e74944e8c1c5f6c61746573745f7065727369737465645f636865636b706f696e74944e8c195f6c61746573745f6d656d6f72795f636865636b706f696e74948c247261792e6169722e5f696e7465726e616c2e636865636b706f696e745f6d616e61676572948c125f547261636b6564436865636b706f696e749493942981947d94288c0b6469725f6f725f64617461944e8c026964944affffffff8c0c73746f726167655f6d6f646594681a8c11436865636b706f696e7453746f726167659493944b01859452948c0472616e6b944b008c076d657472696373947d948c076e6f64655f6970944e8c155f6d6f76655f696e73746561645f6f665f636f7079948875628c185f636865636b706f696e74735f746f5f636c65616e5f7570948f9475622e\"\n  },\n  \"restore_path\": null,\n  \"restoring_from\": null,\n  \"num_failures\": 0,\n  \"num_restore_failures\": 0,\n  \"results\": \"80054e2e\",\n  \"best_result\": \"80054e2e\",\n  \"param_config\": \"80054e2e\",\n  \"extra_arg\": \"80054e2e\",\n  \"_state_json\": null,\n  \"_state_valid\": false,\n  \"_resources\": \"80054e2e\"\n}",
    "{\n  \"stub\": false,\n  \"trainable_name\": \"lambda\",\n  \"trial_id\": \"8b670176\",\n  \"sync_config\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059593000000000000008c0f7261792e74756e652e73796e636572948c0a53796e63436f6e6669679493942981947d94288c0a75706c6f61645f646972944e8c0673796e636572948c046175746f948c0b73796e635f706572696f64944d2c018c0c73796e635f74696d656f7574944d08078c0e73796e635f61727469666163747394888c1273796e635f6f6e5f636865636b706f696e74948875622e\"\n  },\n  \"_orig_experiment_path\": \"/Users/kuba/ijcai_ski/ray_results/test\",\n  \"_orig_experiment_dir_name\": \"test\",\n  \"_local_experiment_path\": \"/Users/kuba/ijcai_ski/ray_results/test\",\n  \"_remote_experiment_path\": null,\n  \"config\": {\n    \"Gradient_LR\": 0.060897913047483536,\n    \"Gradient_Momentum\": 0.9644943085796399,\n    \"Gradient_Steps\": 190,\n    \"Threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308000000000000000094869452942e\"\n    },\n    \"merge_threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b0074946243080a0000000000000094869452942e\"\n    },\n    \"number_of_points\": 1000,\n    \"outliers_multiplier\": 3.4078003194818205\n  },\n  \"_Trial__unresolved_config\": {\n    \"Gradient_LR\": 0.060897913047483536,\n    \"Gradient_Momentum\": 0.9644943085796399,\n    \"Gradient_Steps\": 190,\n    \"Threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308000000000000000094869452942e\"\n    },\n    \"merge_threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b0074946243080a0000000000000094869452942e\"\n    },\n    \"number_of_points\": 1000,\n    \"outliers_multiplier\": 3.4078003194818205\n  },\n  \"evaluated_params\": {\n    \"Gradient_LR\": 0.060897913047483536,\n    \"Gradient_Momentum\": 0.9644943085796399,\n    \"Gradient_Steps\": 190,\n    \"Threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308000000000000000094869452942e\"\n    },\n    \"merge_threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b0074946243080a0000000000000094869452942e\"\n    },\n    \"number_of_points\": 1000,\n    \"outliers_multiplier\": 3.4078003194818205\n  },\n  \"experiment_tag\": \"32_Gradient_LR=0.0609,Gradient_Momentum=0.9645,Gradient_Steps=190,Threshold=0,merge_threshold=10,number_of_points=1000,outliers_multiplier=3.4078\",\n  \"location\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059546000000000000008c197261792e74756e652e6578706572696d656e742e747269616c948c095f4c6f636174696f6e9493942981947d94288c08686f73746e616d65944e8c03706964944e75622e\"\n  },\n  \"stopping_criterion\": {},\n  \"_setup_default_resource\": true,\n  \"_default_placement_group_factory\": \"800595aa000000000000008c237261792e74756e652e657865637574696f6e2e706c6163656d656e745f67726f757073948c15506c6163656d656e7447726f7570466163746f72799493942981947d94288c085f62756e646c6573945d947d948c0343505594473ff000000000000073618c155f686561645f62756e646c655f69735f656d70747994898c095f7374726174656779948c045041434b948c055f6172677394298c075f6b7761726773947d9475622e\",\n  \"placement_group_factory\": \"800595aa000000000000008c237261792e74756e652e657865637574696f6e2e706c6163656d656e745f67726f757073948c15506c6163656d656e7447726f7570466163746f72799493942981947d94288c085f62756e646c6573945d947d948c0343505594473ff000000000000073618c155f686561645f62756e646c655f69735f656d70747994898c095f7374726174656779948c045041434b948c055f6172677394298c075f6b7761726773947d9475622e\",\n  \"log_to_file\": [\n    null,\n    null\n  ],\n  \"max_failures\": 0,\n  \"_last_result\": {\n    \"score\": 0.9284635955274462,\n    \"time_this_iter_s\": 13.009290933609009,\n    \"done\": true,\n    \"training_iteration\": 1,\n    \"trial_id\": \"8b670176\",\n    \"date\": \"2025-01-23_02-40-42\",\n    \"timestamp\": 1737596442,\n    \"time_total_s\": 13.009290933609009,\n    \"pid\": 30487,\n    \"hostname\": \"MacBook-Pro.ds19.agh.edu.pl\",\n    \"node_ip\": \"127.0.0.1\",\n    \"config\": {\n      \"Gradient_LR\": 0.060897913047483536,\n      \"Gradient_Momentum\": 0.9644943085796399,\n      \"Gradient_Steps\": 190,\n      \"Threshold\": 0,\n      \"merge_threshold\": 10,\n      \"number_of_points\": 1000,\n      \"outliers_multiplier\": 3.4078003194818205\n    },\n    \"time_since_restore\": 13.009290933609009,\n    \"iterations_since_restore\": 1,\n    \"experiment_tag\": \"32_Gradient_LR=0.0609,Gradient_Momentum=0.9645,Gradient_Steps=190,Threshold=0,merge_threshold=10,number_of_points=1000,outliers_multiplier=3.4078\"\n  },\n  \"_default_result_or_future\": null,\n  \"last_update_time\": 1737596442.700713,\n  \"metric_analysis\": {\n    \"score\": {\n      \"max\": 0.9284635955274462,\n      \"min\": 0.9284635955274462,\n      \"avg\": 0.9284635955274462,\n      \"last\": 0.9284635955274462,\n      \"last-5-avg\": 0.9284635955274462,\n      \"last-10-avg\": 0.9284635955274462\n    },\n    \"time_this_iter_s\": {\n      \"max\": 13.009290933609009,\n      \"min\": 13.009290933609009,\n      \"avg\": 13.009290933609009,\n      \"last\": 13.009290933609009,\n      \"last-5-avg\": 13.009290933609009,\n      \"last-10-avg\": 13.009290933609009\n    },\n    \"done\": {\n      \"max\": false,\n      \"min\": false,\n      \"avg\": false,\n      \"last\": false,\n      \"last-5-avg\": false,\n      \"last-10-avg\": false\n    },\n    \"training_iteration\": {\n      \"max\": 1,\n      \"min\": 1,\n      \"avg\": 1,\n      \"last\": 1,\n      \"last-5-avg\": 1,\n      \"last-10-avg\": 1\n    },\n    \"time_total_s\": {\n      \"max\": 13.009290933609009,\n      \"min\": 13.009290933609009,\n      \"avg\": 13.009290933609009,\n      \"last\": 13.009290933609009,\n      \"last-5-avg\": 13.009290933609009,\n      \"last-10-avg\": 13.009290933609009\n    },\n    \"time_since_restore\": {\n      \"max\": 13.009290933609009,\n      \"min\": 13.009290933609009,\n      \"avg\": 13.009290933609009,\n      \"last\": 13.009290933609009,\n      \"last-5-avg\": 13.009290933609009,\n      \"last-10-avg\": 13.009290933609009\n    },\n    \"iterations_since_restore\": {\n      \"max\": 1,\n      \"min\": 1,\n      \"avg\": 1,\n      \"last\": 1,\n      \"last-5-avg\": 1,\n      \"last-10-avg\": 1\n    }\n  },\n  \"n_steps\": [\n    5,\n    10\n  ],\n  \"metric_n_steps\": {\n    \"score\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059589000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452948c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02663894898887945294284b038c013c944e4e4e4affffffff4affffffff4b0074946243087b244a49f9b5ed3f9486945294612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059589000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452948c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02663894898887945294284b038c013c944e4e4e4affffffff4affffffff4b0074946243087b244a49f9b5ed3f9486945294612e\"\n      }\n    },\n    \"time_this_iter_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529447402a04c1c8000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529447402a04c1c8000000612e\"\n      }\n    },\n    \"done\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059522000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529489612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059522000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529489612e\"\n      }\n    },\n    \"training_iteration\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b01612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b01612e\"\n      }\n    },\n    \"time_total_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529447402a04c1c8000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529447402a04c1c8000000612e\"\n      }\n    },\n    \"time_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529447402a04c1c8000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529447402a04c1c8000000612e\"\n      }\n    },\n    \"iterations_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b01612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b01612e\"\n      }\n    }\n  },\n  \"export_formats\": [],\n  \"status\": \"TERMINATED\",\n  \"start_time\": 1737596429.683223,\n  \"relative_logdir\": \"lambda_8b670176_32_Gradient_LR=0.0609,Gradient_Momentum=0.9645,Gradient_Steps=190,Threshold=0,merge_threshold=10,number_of_points=_2025-01-23_02-40-27\",\n  \"runner\": null,\n  \"last_debug\": 0,\n  \"error_filename\": null,\n  \"pickled_error_filename\": null,\n  \"trial_name_creator\": null,\n  \"trial_dirname_creator\": null,\n  \"custom_trial_name\": null,\n  \"custom_dirname\": null,\n  \"experiment_dir_name\": \"test\",\n  \"saving_to\": null,\n  \"checkpoint_config\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"800595ff000000000000008c0e7261792e6169722e636f6e666967948c10436865636b706f696e74436f6e6669679493942981947d94288c0b6e756d5f746f5f6b656570944b038c1a636865636b706f696e745f73636f72655f617474726962757465948c12747261696e696e675f697465726174696f6e948c16636865636b706f696e745f73636f72655f6f72646572948c036d6178948c14636865636b706f696e745f6672657175656e6379944b008c11636865636b706f696e745f61745f656e64944e8c1a5f636865636b706f696e745f6b6565705f616c6c5f72616e6b7394898c1f5f636865636b706f696e745f75706c6f61645f66726f6d5f776f726b657273948975622e\"\n  },\n  \"checkpoint_manager\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"800595d2020000000000008c257261792e74756e652e657865637574696f6e2e636865636b706f696e745f6d616e61676572948c125f436865636b706f696e744d616e616765729493942981947d94288c145f636865636b706f696e745f7374726174656779948c0e7261792e6169722e636f6e666967948c10436865636b706f696e74436f6e6669679493942981947d94288c0b6e756d5f746f5f6b656570944b038c1a636865636b706f696e745f73636f72655f617474726962757465948c12747261696e696e675f697465726174696f6e948c16636865636b706f696e745f73636f72655f6f72646572948c036d6178948c14636865636b706f696e745f6672657175656e6379944b008c11636865636b706f696e745f61745f656e64944e8c1a5f636865636b706f696e745f6b6565705f616c6c5f72616e6b7394898c1f5f636865636b706f696e745f75706c6f61645f66726f6d5f776f726b657273948975628c155f6c61746573745f636865636b706f696e745f6964944b008c1a5f746f705f7065727369737465645f636865636b706f696e7473945d948c1a5f626573745f7065727369737465645f636865636b706f696e74944e8c1c5f6c61746573745f7065727369737465645f636865636b706f696e74944e8c195f6c61746573745f6d656d6f72795f636865636b706f696e74948c247261792e6169722e5f696e7465726e616c2e636865636b706f696e745f6d616e61676572948c125f547261636b6564436865636b706f696e749493942981947d94288c0b6469725f6f725f64617461944e8c026964944affffffff8c0c73746f726167655f6d6f646594681a8c11436865636b706f696e7453746f726167659493944b01859452948c0472616e6b944b008c076d657472696373947d948c076e6f64655f6970944e8c155f6d6f76655f696e73746561645f6f665f636f7079948875628c185f636865636b706f696e74735f746f5f636c65616e5f7570948f9475622e\"\n  },\n  \"restore_path\": null,\n  \"restoring_from\": null,\n  \"num_failures\": 0,\n  \"num_restore_failures\": 0,\n  \"results\": \"80054e2e\",\n  \"best_result\": \"80054e2e\",\n  \"param_config\": \"80054e2e\",\n  \"extra_arg\": \"80054e2e\",\n  \"_state_json\": null,\n  \"_state_valid\": false,\n  \"_resources\": \"80054e2e\"\n}",
    "{\n  \"stub\": false,\n  \"trainable_name\": \"lambda\",\n  \"trial_id\": \"a9601769\",\n  \"sync_config\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059593000000000000008c0f7261792e74756e652e73796e636572948c0a53796e63436f6e6669679493942981947d94288c0a75706c6f61645f646972944e8c0673796e636572948c046175746f948c0b73796e635f706572696f64944d2c018c0c73796e635f74696d656f7574944d08078c0e73796e635f61727469666163747394888c1273796e635f6f6e5f636865636b706f696e74948875622e\"\n  },\n  \"_orig_experiment_path\": \"/Users/kuba/ijcai_ski/ray_results/test\",\n  \"_orig_experiment_dir_name\": \"test\",\n  \"_local_experiment_path\": \"/Users/kuba/ijcai_ski/ray_results/test\",\n  \"_remote_experiment_path\": null,\n  \"config\": {\n    \"Gradient_LR\": 0.04916446535800787,\n    \"Gradient_Momentum\": 0.9512882915615946,\n    \"Gradient_Steps\": 440,\n    \"Threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308010000000000000094869452942e\"\n    },\n    \"merge_threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b0074946243080c0000000000000094869452942e\"\n    },\n    \"number_of_points\": 600,\n    \"outliers_multiplier\": 2.2276086894827403\n  },\n  \"_Trial__unresolved_config\": {\n    \"Gradient_LR\": 0.04916446535800787,\n    \"Gradient_Momentum\": 0.9512882915615946,\n    \"Gradient_Steps\": 440,\n    \"Threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308010000000000000094869452942e\"\n    },\n    \"merge_threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b0074946243080c0000000000000094869452942e\"\n    },\n    \"number_of_points\": 600,\n    \"outliers_multiplier\": 2.2276086894827403\n  },\n  \"evaluated_params\": {\n    \"Gradient_LR\": 0.04916446535800787,\n    \"Gradient_Momentum\": 0.9512882915615946,\n    \"Gradient_Steps\": 440,\n    \"Threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308010000000000000094869452942e\"\n    },\n    \"merge_threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b0074946243080c0000000000000094869452942e\"\n    },\n    \"number_of_points\": 600,\n    \"outliers_multiplier\": 2.2276086894827403\n  },\n  \"experiment_tag\": \"36_Gradient_LR=0.0492,Gradient_Momentum=0.9513,Gradient_Steps=440,Threshold=1,merge_threshold=12,number_of_points=600,outliers_multiplier=2.2276\",\n  \"location\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059546000000000000008c197261792e74756e652e6578706572696d656e742e747269616c948c095f4c6f636174696f6e9493942981947d94288c08686f73746e616d65944e8c03706964944e75622e\"\n  },\n  \"stopping_criterion\": {},\n  \"_setup_default_resource\": true,\n  \"_default_placement_group_factory\": \"800595aa000000000000008c237261792e74756e652e657865637574696f6e2e706c6163656d656e745f67726f757073948c15506c6163656d656e7447726f7570466163746f72799493942981947d94288c085f62756e646c6573945d947d948c0343505594473ff000000000000073618c155f686561645f62756e646c655f69735f656d70747994898c095f7374726174656779948c045041434b948c055f6172677394298c075f6b7761726773947d9475622e\",\n  \"placement_group_factory\": \"800595aa000000000000008c237261792e74756e652e657865637574696f6e2e706c6163656d656e745f67726f757073948c15506c6163656d656e7447726f7570466163746f72799493942981947d94288c085f62756e646c6573945d947d948c0343505594473ff000000000000073618c155f686561645f62756e646c655f69735f656d70747994898c095f7374726174656779948c045041434b948c055f6172677394298c075f6b7761726773947d9475622e\",\n  \"log_to_file\": [\n    null,\n    null\n  ],\n  \"max_failures\": 0,\n  \"_last_result\": {\n    \"score\": 0.895022161651033,\n    \"time_this_iter_s\": 16.10281491279602,\n    \"done\": true,\n    \"training_iteration\": 1,\n    \"trial_id\": \"a9601769\",\n    \"date\": \"2025-01-23_02-40-53\",\n    \"timestamp\": 1737596453,\n    \"time_total_s\": 16.10281491279602,\n    \"pid\": 30685,\n    \"hostname\": \"MacBook-Pro.ds19.agh.edu.pl\",\n    \"node_ip\": \"127.0.0.1\",\n    \"config\": {\n      \"Gradient_LR\": 0.04916446535800787,\n      \"Gradient_Momentum\": 0.9512882915615946,\n      \"Gradient_Steps\": 440,\n      \"Threshold\": 1,\n      \"merge_threshold\": 12,\n      \"number_of_points\": 600,\n      \"outliers_multiplier\": 2.2276086894827403\n    },\n    \"time_since_restore\": 16.10281491279602,\n    \"iterations_since_restore\": 1,\n    \"experiment_tag\": \"36_Gradient_LR=0.0492,Gradient_Momentum=0.9513,Gradient_Steps=440,Threshold=1,merge_threshold=12,number_of_points=600,outliers_multiplier=2.2276\"\n  },\n  \"_default_result_or_future\": null,\n  \"last_update_time\": 1737596453.532615,\n  \"metric_analysis\": {\n    \"score\": {\n      \"max\": 0.895022161651033,\n      \"min\": 0.895022161651033,\n      \"avg\": 0.895022161651033,\n      \"last\": 0.895022161651033,\n      \"last-5-avg\": 0.895022161651033,\n      \"last-10-avg\": 0.895022161651033\n    },\n    \"time_this_iter_s\": {\n      \"max\": 16.10281491279602,\n      \"min\": 16.10281491279602,\n      \"avg\": 16.10281491279602,\n      \"last\": 16.10281491279602,\n      \"last-5-avg\": 16.10281491279602,\n      \"last-10-avg\": 16.10281491279602\n    },\n    \"done\": {\n      \"max\": false,\n      \"min\": false,\n      \"avg\": false,\n      \"last\": false,\n      \"last-5-avg\": false,\n      \"last-10-avg\": false\n    },\n    \"training_iteration\": {\n      \"max\": 1,\n      \"min\": 1,\n      \"avg\": 1,\n      \"last\": 1,\n      \"last-5-avg\": 1,\n      \"last-10-avg\": 1\n    },\n    \"time_total_s\": {\n      \"max\": 16.10281491279602,\n      \"min\": 16.10281491279602,\n      \"avg\": 16.10281491279602,\n      \"last\": 16.10281491279602,\n      \"last-5-avg\": 16.10281491279602,\n      \"last-10-avg\": 16.10281491279602\n    },\n    \"time_since_restore\": {\n      \"max\": 16.10281491279602,\n      \"min\": 16.10281491279602,\n      \"avg\": 16.10281491279602,\n      \"last\": 16.10281491279602,\n      \"last-5-avg\": 16.10281491279602,\n      \"last-10-avg\": 16.10281491279602\n    },\n    \"iterations_since_restore\": {\n      \"max\": 1,\n      \"min\": 1,\n      \"avg\": 1,\n      \"last\": 1,\n      \"last-5-avg\": 1,\n      \"last-10-avg\": 1\n    }\n  },\n  \"n_steps\": [\n    5,\n    10\n  ],\n  \"metric_n_steps\": {\n    \"score\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059589000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452948c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02663894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308b0902f8405a4ec3f9486945294612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059589000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452948c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02663894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308b0902f8405a4ec3f9486945294612e\"\n      }\n    },\n    \"time_this_iter_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944740301a5214000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944740301a5214000000612e\"\n      }\n    },\n    \"done\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059522000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529489612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059522000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529489612e\"\n      }\n    },\n    \"training_iteration\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b01612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b01612e\"\n      }\n    },\n    \"time_total_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944740301a5214000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944740301a5214000000612e\"\n      }\n    },\n    \"time_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944740301a5214000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944740301a5214000000612e\"\n      }\n    },\n    \"iterations_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b01612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b01612e\"\n      }\n    }\n  },\n  \"export_formats\": [],\n  \"status\": \"TERMINATED\",\n  \"start_time\": 1737596437.414232,\n  \"relative_logdir\": \"lambda_a9601769_36_Gradient_LR=0.0492,Gradient_Momentum=0.9513,Gradient_Steps=440,Threshold=1,merge_threshold=12,number_of_points=_2025-01-23_02-40-34\",\n  \"runner\": null,\n  \"last_debug\": 0,\n  \"error_filename\": null,\n  \"pickled_error_filename\": null,\n  \"trial_name_creator\": null,\n  \"trial_dirname_creator\": null,\n  \"custom_trial_name\": null,\n  \"custom_dirname\": null,\n  \"experiment_dir_name\": \"test\",\n  \"saving_to\": null,\n  \"checkpoint_config\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"800595ff000000000000008c0e7261792e6169722e636f6e666967948c10436865636b706f696e74436f6e6669679493942981947d94288c0b6e756d5f746f5f6b656570944b038c1a636865636b706f696e745f73636f72655f617474726962757465948c12747261696e696e675f697465726174696f6e948c16636865636b706f696e745f73636f72655f6f72646572948c036d6178948c14636865636b706f696e745f6672657175656e6379944b008c11636865636b706f696e745f61745f656e64944e8c1a5f636865636b706f696e745f6b6565705f616c6c5f72616e6b7394898c1f5f636865636b706f696e745f75706c6f61645f66726f6d5f776f726b657273948975622e\"\n  },\n  \"checkpoint_manager\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"800595d2020000000000008c257261792e74756e652e657865637574696f6e2e636865636b706f696e745f6d616e61676572948c125f436865636b706f696e744d616e616765729493942981947d94288c145f636865636b706f696e745f7374726174656779948c0e7261792e6169722e636f6e666967948c10436865636b706f696e74436f6e6669679493942981947d94288c0b6e756d5f746f5f6b656570944b038c1a636865636b706f696e745f73636f72655f617474726962757465948c12747261696e696e675f697465726174696f6e948c16636865636b706f696e745f73636f72655f6f72646572948c036d6178948c14636865636b706f696e745f6672657175656e6379944b008c11636865636b706f696e745f61745f656e64944e8c1a5f636865636b706f696e745f6b6565705f616c6c5f72616e6b7394898c1f5f636865636b706f696e745f75706c6f61645f66726f6d5f776f726b657273948975628c155f6c61746573745f636865636b706f696e745f6964944b008c1a5f746f705f7065727369737465645f636865636b706f696e7473945d948c1a5f626573745f7065727369737465645f636865636b706f696e74944e8c1c5f6c61746573745f7065727369737465645f636865636b706f696e74944e8c195f6c61746573745f6d656d6f72795f636865636b706f696e74948c247261792e6169722e5f696e7465726e616c2e636865636b706f696e745f6d616e61676572948c125f547261636b6564436865636b706f696e749493942981947d94288c0b6469725f6f725f64617461944e8c026964944affffffff8c0c73746f726167655f6d6f646594681a8c11436865636b706f696e7453746f726167659493944b01859452948c0472616e6b944b008c076d657472696373947d948c076e6f64655f6970944e8c155f6d6f76655f696e73746561645f6f665f636f7079948875628c185f636865636b706f696e74735f746f5f636c65616e5f7570948f9475622e\"\n  },\n  \"restore_path\": null,\n  \"restoring_from\": null,\n  \"num_failures\": 0,\n  \"num_restore_failures\": 0,\n  \"results\": \"80054e2e\",\n  \"best_result\": \"80054e2e\",\n  \"param_config\": \"80054e2e\",\n  \"extra_arg\": \"80054e2e\",\n  \"_state_json\": null,\n  \"_state_valid\": false,\n  \"_resources\": \"80054e2e\"\n}",
    "{\n  \"stub\": false,\n  \"trainable_name\": \"lambda\",\n  \"trial_id\": \"c858d5e1\",\n  \"sync_config\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059593000000000000008c0f7261792e74756e652e73796e636572948c0a53796e63436f6e6669679493942981947d94288c0a75706c6f61645f646972944e8c0673796e636572948c046175746f948c0b73796e635f706572696f64944d2c018c0c73796e635f74696d656f7574944d08078c0e73796e635f61727469666163747394888c1273796e635f6f6e5f636865636b706f696e74948875622e\"\n  },\n  \"_orig_experiment_path\": \"/Users/kuba/ijcai_ski/ray_results/test\",\n  \"_orig_experiment_dir_name\": \"test\",\n  \"_local_experiment_path\": \"/Users/kuba/ijcai_ski/ray_results/test\",\n  \"_remote_experiment_path\": null,\n  \"config\": {\n    \"Gradient_LR\": 0.03177519969381148,\n    \"Gradient_Momentum\": 0.9911102674021119,\n    \"Gradient_Steps\": 330,\n    \"Threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308020000000000000094869452942e\"\n    },\n    \"merge_threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b0074946243080f0000000000000094869452942e\"\n    },\n    \"number_of_points\": 1000,\n    \"outliers_multiplier\": 2.5323501814680847\n  },\n  \"_Trial__unresolved_config\": {\n    \"Gradient_LR\": 0.03177519969381148,\n    \"Gradient_Momentum\": 0.9911102674021119,\n    \"Gradient_Steps\": 330,\n    \"Threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308020000000000000094869452942e\"\n    },\n    \"merge_threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b0074946243080f0000000000000094869452942e\"\n    },\n    \"number_of_points\": 1000,\n    \"outliers_multiplier\": 2.5323501814680847\n  },\n  \"evaluated_params\": {\n    \"Gradient_LR\": 0.03177519969381148,\n    \"Gradient_Momentum\": 0.9911102674021119,\n    \"Gradient_Steps\": 330,\n    \"Threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308020000000000000094869452942e\"\n    },\n    \"merge_threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b0074946243080f0000000000000094869452942e\"\n    },\n    \"number_of_points\": 1000,\n    \"outliers_multiplier\": 2.5323501814680847\n  },\n  \"experiment_tag\": \"30_Gradient_LR=0.0318,Gradient_Momentum=0.9911,Gradient_Steps=330,Threshold=2,merge_threshold=15,number_of_points=1000,outliers_multiplier=2.5324\",\n  \"location\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059546000000000000008c197261792e74756e652e6578706572696d656e742e747269616c948c095f4c6f636174696f6e9493942981947d94288c08686f73746e616d65944e8c03706964944e75622e\"\n  },\n  \"stopping_criterion\": {},\n  \"_setup_default_resource\": true,\n  \"_default_placement_group_factory\": \"800595aa000000000000008c237261792e74756e652e657865637574696f6e2e706c6163656d656e745f67726f757073948c15506c6163656d656e7447726f7570466163746f72799493942981947d94288c085f62756e646c6573945d947d948c0343505594473ff000000000000073618c155f686561645f62756e646c655f69735f656d70747994898c095f7374726174656779948c045041434b948c055f6172677394298c075f6b7761726773947d9475622e\",\n  \"placement_group_factory\": \"800595aa000000000000008c237261792e74756e652e657865637574696f6e2e706c6163656d656e745f67726f757073948c15506c6163656d656e7447726f7570466163746f72799493942981947d94288c085f62756e646c6573945d947d948c0343505594473ff000000000000073618c155f686561645f62756e646c655f69735f656d70747994898c095f7374726174656779948c045041434b948c055f6172677394298c075f6b7761726773947d9475622e\",\n  \"log_to_file\": [\n    null,\n    null\n  ],\n  \"max_failures\": 0,\n  \"_last_result\": {\n    \"score\": 0.8751364873055228,\n    \"time_this_iter_s\": 13.640964984893799,\n    \"done\": true,\n    \"training_iteration\": 1,\n    \"trial_id\": \"c858d5e1\",\n    \"date\": \"2025-01-23_02-40-39\",\n    \"timestamp\": 1737596439,\n    \"time_total_s\": 13.640964984893799,\n    \"pid\": 30590,\n    \"hostname\": \"MacBook-Pro.ds19.agh.edu.pl\",\n    \"node_ip\": \"127.0.0.1\",\n    \"config\": {\n      \"Gradient_LR\": 0.03177519969381148,\n      \"Gradient_Momentum\": 0.9911102674021119,\n      \"Gradient_Steps\": 330,\n      \"Threshold\": 2,\n      \"merge_threshold\": 15,\n      \"number_of_points\": 1000,\n      \"outliers_multiplier\": 2.5323501814680847\n    },\n    \"time_since_restore\": 13.640964984893799,\n    \"iterations_since_restore\": 1,\n    \"experiment_tag\": \"30_Gradient_LR=0.0318,Gradient_Momentum=0.9911,Gradient_Steps=330,Threshold=2,merge_threshold=15,number_of_points=1000,outliers_multiplier=2.5324\"\n  },\n  \"_default_result_or_future\": null,\n  \"last_update_time\": 1737596439.3617861,\n  \"metric_analysis\": {\n    \"score\": {\n      \"max\": 0.8751364873055228,\n      \"min\": 0.8751364873055228,\n      \"avg\": 0.8751364873055228,\n      \"last\": 0.8751364873055228,\n      \"last-5-avg\": 0.8751364873055228,\n      \"last-10-avg\": 0.8751364873055228\n    },\n    \"time_this_iter_s\": {\n      \"max\": 13.640964984893799,\n      \"min\": 13.640964984893799,\n      \"avg\": 13.640964984893799,\n      \"last\": 13.640964984893799,\n      \"last-5-avg\": 13.640964984893799,\n      \"last-10-avg\": 13.640964984893799\n    },\n    \"done\": {\n      \"max\": false,\n      \"min\": false,\n      \"avg\": false,\n      \"last\": false,\n      \"last-5-avg\": false,\n      \"last-10-avg\": false\n    },\n    \"training_iteration\": {\n      \"max\": 1,\n      \"min\": 1,\n      \"avg\": 1,\n      \"last\": 1,\n      \"last-5-avg\": 1,\n      \"last-10-avg\": 1\n    },\n    \"time_total_s\": {\n      \"max\": 13.640964984893799,\n      \"min\": 13.640964984893799,\n      \"avg\": 13.640964984893799,\n      \"last\": 13.640964984893799,\n      \"last-5-avg\": 13.640964984893799,\n      \"last-10-avg\": 13.640964984893799\n    },\n    \"time_since_restore\": {\n      \"max\": 13.640964984893799,\n      \"min\": 13.640964984893799,\n      \"avg\": 13.640964984893799,\n      \"last\": 13.640964984893799,\n      \"last-5-avg\": 13.640964984893799,\n      \"last-10-avg\": 13.640964984893799\n    },\n    \"iterations_since_restore\": {\n      \"max\": 1,\n      \"min\": 1,\n      \"avg\": 1,\n      \"last\": 1,\n      \"last-5-avg\": 1,\n      \"last-10-avg\": 1\n    }\n  },\n  \"n_steps\": [\n    5,\n    10\n  ],\n  \"metric_n_steps\": {\n    \"score\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059589000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452948c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02663894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308eb6e103c1e01ec3f9486945294612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059589000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452948c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02663894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308eb6e103c1e01ec3f9486945294612e\"\n      }\n    },\n    \"time_this_iter_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529447402b482c90000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529447402b482c90000000612e\"\n      }\n    },\n    \"done\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059522000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529489612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059522000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529489612e\"\n      }\n    },\n    \"training_iteration\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b01612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b01612e\"\n      }\n    },\n    \"time_total_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529447402b482c90000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529447402b482c90000000612e\"\n      }\n    },\n    \"time_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529447402b482c90000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529447402b482c90000000612e\"\n      }\n    },\n    \"iterations_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b01612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b01612e\"\n      }\n    }\n  },\n  \"export_formats\": [],\n  \"status\": \"TERMINATED\",\n  \"start_time\": 1737596425.7013361,\n  \"relative_logdir\": \"lambda_c858d5e1_30_Gradient_LR=0.0318,Gradient_Momentum=0.9911,Gradient_Steps=330,Threshold=2,merge_threshold=15,number_of_points=_2025-01-23_02-40-25\",\n  \"runner\": null,\n  \"last_debug\": 0,\n  \"error_filename\": null,\n  \"pickled_error_filename\": null,\n  \"trial_name_creator\": null,\n  \"trial_dirname_creator\": null,\n  \"custom_trial_name\": null,\n  \"custom_dirname\": null,\n  \"experiment_dir_name\": \"test\",\n  \"saving_to\": null,\n  \"checkpoint_config\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"800595ff000000000000008c0e7261792e6169722e636f6e666967948c10436865636b706f696e74436f6e6669679493942981947d94288c0b6e756d5f746f5f6b656570944b038c1a636865636b706f696e745f73636f72655f617474726962757465948c12747261696e696e675f697465726174696f6e948c16636865636b706f696e745f73636f72655f6f72646572948c036d6178948c14636865636b706f696e745f6672657175656e6379944b008c11636865636b706f696e745f61745f656e64944e8c1a5f636865636b706f696e745f6b6565705f616c6c5f72616e6b7394898c1f5f636865636b706f696e745f75706c6f61645f66726f6d5f776f726b657273948975622e\"\n  },\n  \"checkpoint_manager\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"800595d2020000000000008c257261792e74756e652e657865637574696f6e2e636865636b706f696e745f6d616e61676572948c125f436865636b706f696e744d616e616765729493942981947d94288c145f636865636b706f696e745f7374726174656779948c0e7261792e6169722e636f6e666967948c10436865636b706f696e74436f6e6669679493942981947d94288c0b6e756d5f746f5f6b656570944b038c1a636865636b706f696e745f73636f72655f617474726962757465948c12747261696e696e675f697465726174696f6e948c16636865636b706f696e745f73636f72655f6f72646572948c036d6178948c14636865636b706f696e745f6672657175656e6379944b008c11636865636b706f696e745f61745f656e64944e8c1a5f636865636b706f696e745f6b6565705f616c6c5f72616e6b7394898c1f5f636865636b706f696e745f75706c6f61645f66726f6d5f776f726b657273948975628c155f6c61746573745f636865636b706f696e745f6964944b008c1a5f746f705f7065727369737465645f636865636b706f696e7473945d948c1a5f626573745f7065727369737465645f636865636b706f696e74944e8c1c5f6c61746573745f7065727369737465645f636865636b706f696e74944e8c195f6c61746573745f6d656d6f72795f636865636b706f696e74948c247261792e6169722e5f696e7465726e616c2e636865636b706f696e745f6d616e61676572948c125f547261636b6564436865636b706f696e749493942981947d94288c0b6469725f6f725f64617461944e8c026964944affffffff8c0c73746f726167655f6d6f646594681a8c11436865636b706f696e7453746f726167659493944b01859452948c0472616e6b944b008c076d657472696373947d948c076e6f64655f6970944e8c155f6d6f76655f696e73746561645f6f665f636f7079948875628c185f636865636b706f696e74735f746f5f636c65616e5f7570948f9475622e\"\n  },\n  \"restore_path\": null,\n  \"restoring_from\": null,\n  \"num_failures\": 0,\n  \"num_restore_failures\": 0,\n  \"results\": \"80054e2e\",\n  \"best_result\": \"80054e2e\",\n  \"param_config\": \"80054e2e\",\n  \"extra_arg\": \"80054e2e\",\n  \"_state_json\": null,\n  \"_state_valid\": false,\n  \"_resources\": \"80054e2e\"\n}",
    "{\n  \"stub\": false,\n  \"trainable_name\": \"lambda\",\n  \"trial_id\": \"98c5fd6c\",\n  \"sync_config\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059593000000000000008c0f7261792e74756e652e73796e636572948c0a53796e63436f6e6669679493942981947d94288c0a75706c6f61645f646972944e8c0673796e636572948c046175746f948c0b73796e635f706572696f64944d2c018c0c73796e635f74696d656f7574944d08078c0e73796e635f61727469666163747394888c1273796e635f6f6e5f636865636b706f696e74948875622e\"\n  },\n  \"_orig_experiment_path\": \"/Users/kuba/ijcai_ski/ray_results/test\",\n  \"_orig_experiment_dir_name\": \"test\",\n  \"_local_experiment_path\": \"/Users/kuba/ijcai_ski/ray_results/test\",\n  \"_remote_experiment_path\": null,\n  \"config\": {\n    \"Gradient_LR\": 0.01739245466460087,\n    \"Gradient_Momentum\": 0.9739079366745553,\n    \"Gradient_Steps\": 480,\n    \"Threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308020000000000000094869452942e\"\n    },\n    \"merge_threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308120000000000000094869452942e\"\n    },\n    \"number_of_points\": 900,\n    \"outliers_multiplier\": 4.049636828474132\n  },\n  \"_Trial__unresolved_config\": {\n    \"Gradient_LR\": 0.01739245466460087,\n    \"Gradient_Momentum\": 0.9739079366745553,\n    \"Gradient_Steps\": 480,\n    \"Threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308020000000000000094869452942e\"\n    },\n    \"merge_threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308120000000000000094869452942e\"\n    },\n    \"number_of_points\": 900,\n    \"outliers_multiplier\": 4.049636828474132\n  },\n  \"evaluated_params\": {\n    \"Gradient_LR\": 0.01739245466460087,\n    \"Gradient_Momentum\": 0.9739079366745553,\n    \"Gradient_Steps\": 480,\n    \"Threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308020000000000000094869452942e\"\n    },\n    \"merge_threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308120000000000000094869452942e\"\n    },\n    \"number_of_points\": 900,\n    \"outliers_multiplier\": 4.049636828474132\n  },\n  \"experiment_tag\": \"31_Gradient_LR=0.0174,Gradient_Momentum=0.9739,Gradient_Steps=480,Threshold=2,merge_threshold=18,number_of_points=900,outliers_multiplier=4.0496\",\n  \"location\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059546000000000000008c197261792e74756e652e6578706572696d656e742e747269616c948c095f4c6f636174696f6e9493942981947d94288c08686f73746e616d65944e8c03706964944e75622e\"\n  },\n  \"stopping_criterion\": {},\n  \"_setup_default_resource\": true,\n  \"_default_placement_group_factory\": \"800595aa000000000000008c237261792e74756e652e657865637574696f6e2e706c6163656d656e745f67726f757073948c15506c6163656d656e7447726f7570466163746f72799493942981947d94288c085f62756e646c6573945d947d948c0343505594473ff000000000000073618c155f686561645f62756e646c655f69735f656d70747994898c095f7374726174656779948c045041434b948c055f6172677394298c075f6b7761726773947d9475622e\",\n  \"placement_group_factory\": \"800595aa000000000000008c237261792e74756e652e657865637574696f6e2e706c6163656d656e745f67726f757073948c15506c6163656d656e7447726f7570466163746f72799493942981947d94288c085f62756e646c6573945d947d948c0343505594473ff000000000000073618c155f686561645f62756e646c655f69735f656d70747994898c095f7374726174656779948c045041434b948c055f6172677394298c075f6b7761726773947d9475622e\",\n  \"log_to_file\": [\n    null,\n    null\n  ],\n  \"max_failures\": 0,\n  \"_last_result\": {\n    \"score\": 0.8965919904335093,\n    \"time_this_iter_s\": 17.358771324157715,\n    \"done\": true,\n    \"training_iteration\": 1,\n    \"trial_id\": \"98c5fd6c\",\n    \"date\": \"2025-01-23_02-40-44\",\n    \"timestamp\": 1737596444,\n    \"time_total_s\": 17.358771324157715,\n    \"pid\": 30566,\n    \"hostname\": \"MacBook-Pro.ds19.agh.edu.pl\",\n    \"node_ip\": \"127.0.0.1\",\n    \"config\": {\n      \"Gradient_LR\": 0.01739245466460087,\n      \"Gradient_Momentum\": 0.9739079366745553,\n      \"Gradient_Steps\": 480,\n      \"Threshold\": 2,\n      \"merge_threshold\": 18,\n      \"number_of_points\": 900,\n      \"outliers_multiplier\": 4.049636828474132\n    },\n    \"time_since_restore\": 17.358771324157715,\n    \"iterations_since_restore\": 1,\n    \"experiment_tag\": \"31_Gradient_LR=0.0174,Gradient_Momentum=0.9739,Gradient_Steps=480,Threshold=2,merge_threshold=18,number_of_points=900,outliers_multiplier=4.0496\"\n  },\n  \"_default_result_or_future\": null,\n  \"last_update_time\": 1737596444.4142919,\n  \"metric_analysis\": {\n    \"score\": {\n      \"max\": 0.8965919904335093,\n      \"min\": 0.8965919904335093,\n      \"avg\": 0.8965919904335093,\n      \"last\": 0.8965919904335093,\n      \"last-5-avg\": 0.8965919904335093,\n      \"last-10-avg\": 0.8965919904335093\n    },\n    \"time_this_iter_s\": {\n      \"max\": 17.358771324157715,\n      \"min\": 17.358771324157715,\n      \"avg\": 17.358771324157715,\n      \"last\": 17.358771324157715,\n      \"last-5-avg\": 17.358771324157715,\n      \"last-10-avg\": 17.358771324157715\n    },\n    \"done\": {\n      \"max\": false,\n      \"min\": false,\n      \"avg\": false,\n      \"last\": false,\n      \"last-5-avg\": false,\n      \"last-10-avg\": false\n    },\n    \"training_iteration\": {\n      \"max\": 1,\n      \"min\": 1,\n      \"avg\": 1,\n      \"last\": 1,\n      \"last-5-avg\": 1,\n      \"last-10-avg\": 1\n    },\n    \"time_total_s\": {\n      \"max\": 17.358771324157715,\n      \"min\": 17.358771324157715,\n      \"avg\": 17.358771324157715,\n      \"last\": 17.358771324157715,\n      \"last-5-avg\": 17.358771324157715,\n      \"last-10-avg\": 17.358771324157715\n    },\n    \"time_since_restore\": {\n      \"max\": 17.358771324157715,\n      \"min\": 17.358771324157715,\n      \"avg\": 17.358771324157715,\n      \"last\": 17.358771324157715,\n      \"last-5-avg\": 17.358771324157715,\n      \"last-10-avg\": 17.358771324157715\n    },\n    \"iterations_since_restore\": {\n      \"max\": 1,\n      \"min\": 1,\n      \"avg\": 1,\n      \"last\": 1,\n      \"last-5-avg\": 1,\n      \"last-10-avg\": 1\n    }\n  },\n  \"n_steps\": [\n    5,\n    10\n  ],\n  \"metric_n_steps\": {\n    \"score\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059589000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452948c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02663894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308188f98afe1b0ec3f9486945294612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059589000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452948c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02663894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308188f98afe1b0ec3f9486945294612e\"\n      }\n    },\n    \"time_this_iter_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944740315bd870000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944740315bd870000000612e\"\n      }\n    },\n    \"done\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059522000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529489612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059522000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529489612e\"\n      }\n    },\n    \"training_iteration\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b01612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b01612e\"\n      }\n    },\n    \"time_total_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944740315bd870000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944740315bd870000000612e\"\n      }\n    },\n    \"time_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944740315bd870000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944740315bd870000000612e\"\n      }\n    },\n    \"iterations_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b01612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b01612e\"\n      }\n    }\n  },\n  \"export_formats\": [],\n  \"status\": \"TERMINATED\",\n  \"start_time\": 1737596427.0432498,\n  \"relative_logdir\": \"lambda_98c5fd6c_31_Gradient_LR=0.0174,Gradient_Momentum=0.9739,Gradient_Steps=480,Threshold=2,merge_threshold=18,number_of_points=_2025-01-23_02-40-25\",\n  \"runner\": null,\n  \"last_debug\": 0,\n  \"error_filename\": null,\n  \"pickled_error_filename\": null,\n  \"trial_name_creator\": null,\n  \"trial_dirname_creator\": null,\n  \"custom_trial_name\": null,\n  \"custom_dirname\": null,\n  \"experiment_dir_name\": \"test\",\n  \"saving_to\": null,\n  \"checkpoint_config\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"800595ff000000000000008c0e7261792e6169722e636f6e666967948c10436865636b706f696e74436f6e6669679493942981947d94288c0b6e756d5f746f5f6b656570944b038c1a636865636b706f696e745f73636f72655f617474726962757465948c12747261696e696e675f697465726174696f6e948c16636865636b706f696e745f73636f72655f6f72646572948c036d6178948c14636865636b706f696e745f6672657175656e6379944b008c11636865636b706f696e745f61745f656e64944e8c1a5f636865636b706f696e745f6b6565705f616c6c5f72616e6b7394898c1f5f636865636b706f696e745f75706c6f61645f66726f6d5f776f726b657273948975622e\"\n  },\n  \"checkpoint_manager\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"800595d2020000000000008c257261792e74756e652e657865637574696f6e2e636865636b706f696e745f6d616e61676572948c125f436865636b706f696e744d616e616765729493942981947d94288c145f636865636b706f696e745f7374726174656779948c0e7261792e6169722e636f6e666967948c10436865636b706f696e74436f6e6669679493942981947d94288c0b6e756d5f746f5f6b656570944b038c1a636865636b706f696e745f73636f72655f617474726962757465948c12747261696e696e675f697465726174696f6e948c16636865636b706f696e745f73636f72655f6f72646572948c036d6178948c14636865636b706f696e745f6672657175656e6379944b008c11636865636b706f696e745f61745f656e64944e8c1a5f636865636b706f696e745f6b6565705f616c6c5f72616e6b7394898c1f5f636865636b706f696e745f75706c6f61645f66726f6d5f776f726b657273948975628c155f6c61746573745f636865636b706f696e745f6964944b008c1a5f746f705f7065727369737465645f636865636b706f696e7473945d948c1a5f626573745f7065727369737465645f636865636b706f696e74944e8c1c5f6c61746573745f7065727369737465645f636865636b706f696e74944e8c195f6c61746573745f6d656d6f72795f636865636b706f696e74948c247261792e6169722e5f696e7465726e616c2e636865636b706f696e745f6d616e61676572948c125f547261636b6564436865636b706f696e749493942981947d94288c0b6469725f6f725f64617461944e8c026964944affffffff8c0c73746f726167655f6d6f646594681a8c11436865636b706f696e7453746f726167659493944b01859452948c0472616e6b944b008c076d657472696373947d948c076e6f64655f6970944e8c155f6d6f76655f696e73746561645f6f665f636f7079948875628c185f636865636b706f696e74735f746f5f636c65616e5f7570948f9475622e\"\n  },\n  \"restore_path\": null,\n  \"restoring_from\": null,\n  \"num_failures\": 0,\n  \"num_restore_failures\": 0,\n  \"results\": \"80054e2e\",\n  \"best_result\": \"80054e2e\",\n  \"param_config\": \"80054e2e\",\n  \"extra_arg\": \"80054e2e\",\n  \"_state_json\": null,\n  \"_state_valid\": false,\n  \"_resources\": \"80054e2e\"\n}",
    "{\n  \"stub\": false,\n  \"trainable_name\": \"lambda\",\n  \"trial_id\": \"33a748e8\",\n  \"sync_config\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059593000000000000008c0f7261792e74756e652e73796e636572948c0a53796e63436f6e6669679493942981947d94288c0a75706c6f61645f646972944e8c0673796e636572948c046175746f948c0b73796e635f706572696f64944d2c018c0c73796e635f74696d656f7574944d08078c0e73796e635f61727469666163747394888c1273796e635f6f6e5f636865636b706f696e74948875622e\"\n  },\n  \"_orig_experiment_path\": \"/Users/kuba/ijcai_ski/ray_results/test\",\n  \"_orig_experiment_dir_name\": \"test\",\n  \"_local_experiment_path\": \"/Users/kuba/ijcai_ski/ray_results/test\",\n  \"_remote_experiment_path\": null,\n  \"config\": {\n    \"Gradient_LR\": 0.026258525808669872,\n    \"Gradient_Momentum\": 0.9286051071059666,\n    \"Gradient_Steps\": 490,\n    \"Threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308070000000000000094869452942e\"\n    },\n    \"merge_threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b0074946243080b0000000000000094869452942e\"\n    },\n    \"number_of_points\": 1000,\n    \"outliers_multiplier\": 4.357374899125173\n  },\n  \"_Trial__unresolved_config\": {\n    \"Gradient_LR\": 0.026258525808669872,\n    \"Gradient_Momentum\": 0.9286051071059666,\n    \"Gradient_Steps\": 490,\n    \"Threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308070000000000000094869452942e\"\n    },\n    \"merge_threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b0074946243080b0000000000000094869452942e\"\n    },\n    \"number_of_points\": 1000,\n    \"outliers_multiplier\": 4.357374899125173\n  },\n  \"evaluated_params\": {\n    \"Gradient_LR\": 0.026258525808669872,\n    \"Gradient_Momentum\": 0.9286051071059666,\n    \"Gradient_Steps\": 490,\n    \"Threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308070000000000000094869452942e\"\n    },\n    \"merge_threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b0074946243080b0000000000000094869452942e\"\n    },\n    \"number_of_points\": 1000,\n    \"outliers_multiplier\": 4.357374899125173\n  },\n  \"experiment_tag\": \"34_Gradient_LR=0.0263,Gradient_Momentum=0.9286,Gradient_Steps=490,Threshold=7,merge_threshold=11,number_of_points=1000,outliers_multiplier=4.3574\",\n  \"location\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059546000000000000008c197261792e74756e652e6578706572696d656e742e747269616c948c095f4c6f636174696f6e9493942981947d94288c08686f73746e616d65944e8c03706964944e75622e\"\n  },\n  \"stopping_criterion\": {},\n  \"_setup_default_resource\": true,\n  \"_default_placement_group_factory\": \"800595aa000000000000008c237261792e74756e652e657865637574696f6e2e706c6163656d656e745f67726f757073948c15506c6163656d656e7447726f7570466163746f72799493942981947d94288c085f62756e646c6573945d947d948c0343505594473ff000000000000073618c155f686561645f62756e646c655f69735f656d70747994898c095f7374726174656779948c045041434b948c055f6172677394298c075f6b7761726773947d9475622e\",\n  \"placement_group_factory\": \"800595aa000000000000008c237261792e74756e652e657865637574696f6e2e706c6163656d656e745f67726f757073948c15506c6163656d656e7447726f7570466163746f72799493942981947d94288c085f62756e646c6573945d947d948c0343505594473ff000000000000073618c155f686561645f62756e646c655f69735f656d70747994898c095f7374726174656779948c045041434b948c055f6172677394298c075f6b7761726773947d9475622e\",\n  \"log_to_file\": [\n    null,\n    null\n  ],\n  \"max_failures\": 0,\n  \"_last_result\": {\n    \"score\": 0.8809973676545599,\n    \"time_this_iter_s\": 17.177611827850342,\n    \"done\": true,\n    \"training_iteration\": 1,\n    \"trial_id\": \"33a748e8\",\n    \"date\": \"2025-01-23_02-40-49\",\n    \"timestamp\": 1737596449,\n    \"time_total_s\": 17.177611827850342,\n    \"pid\": 30525,\n    \"hostname\": \"MacBook-Pro.ds19.agh.edu.pl\",\n    \"node_ip\": \"127.0.0.1\",\n    \"config\": {\n      \"Gradient_LR\": 0.026258525808669872,\n      \"Gradient_Momentum\": 0.9286051071059666,\n      \"Gradient_Steps\": 490,\n      \"Threshold\": 7,\n      \"merge_threshold\": 11,\n      \"number_of_points\": 1000,\n      \"outliers_multiplier\": 4.357374899125173\n    },\n    \"time_since_restore\": 17.177611827850342,\n    \"iterations_since_restore\": 1,\n    \"experiment_tag\": \"34_Gradient_LR=0.0263,Gradient_Momentum=0.9286,Gradient_Steps=490,Threshold=7,merge_threshold=11,number_of_points=1000,outliers_multiplier=4.3574\"\n  },\n  \"_default_result_or_future\": null,\n  \"last_update_time\": 1737596449.775719,\n  \"metric_analysis\": {\n    \"score\": {\n      \"max\": 0.8809973676545599,\n      \"min\": 0.8809973676545599,\n      \"avg\": 0.8809973676545599,\n      \"last\": 0.8809973676545599,\n      \"last-5-avg\": 0.8809973676545599,\n      \"last-10-avg\": 0.8809973676545599\n    },\n    \"time_this_iter_s\": {\n      \"max\": 17.177611827850342,\n      \"min\": 17.177611827850342,\n      \"avg\": 17.177611827850342,\n      \"last\": 17.177611827850342,\n      \"last-5-avg\": 17.177611827850342,\n      \"last-10-avg\": 17.177611827850342\n    },\n    \"done\": {\n      \"max\": false,\n      \"min\": false,\n      \"avg\": false,\n      \"last\": false,\n      \"last-5-avg\": false,\n      \"last-10-avg\": false\n    },\n    \"training_iteration\": {\n      \"max\": 1,\n      \"min\": 1,\n      \"avg\": 1,\n      \"last\": 1,\n      \"last-5-avg\": 1,\n      \"last-10-avg\": 1\n    },\n    \"time_total_s\": {\n      \"max\": 17.177611827850342,\n      \"min\": 17.177611827850342,\n      \"avg\": 17.177611827850342,\n      \"last\": 17.177611827850342,\n      \"last-5-avg\": 17.177611827850342,\n      \"last-10-avg\": 17.177611827850342\n    },\n    \"time_since_restore\": {\n      \"max\": 17.177611827850342,\n      \"min\": 17.177611827850342,\n      \"avg\": 17.177611827850342,\n      \"last\": 17.177611827850342,\n      \"last-5-avg\": 17.177611827850342,\n      \"last-10-avg\": 17.177611827850342\n    },\n    \"iterations_since_restore\": {\n      \"max\": 1,\n      \"min\": 1,\n      \"avg\": 1,\n      \"last\": 1,\n      \"last-5-avg\": 1,\n      \"last-10-avg\": 1\n    }\n  },\n  \"n_steps\": [\n    5,\n    10\n  ],\n  \"metric_n_steps\": {\n    \"score\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059589000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452948c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02663894898887945294284b038c013c944e4e4e4affffffff4affffffff4b00749462430890073e642131ec3f9486945294612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059589000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452948c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02663894898887945294284b038c013c944e4e4e4affffffff4affffffff4b00749462430890073e642131ec3f9486945294612e\"\n      }\n    },\n    \"time_this_iter_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944740312d77f8000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944740312d77f8000000612e\"\n      }\n    },\n    \"done\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059522000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529489612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059522000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529489612e\"\n      }\n    },\n    \"training_iteration\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b01612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b01612e\"\n      }\n    },\n    \"time_total_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944740312d77f8000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944740312d77f8000000612e\"\n      }\n    },\n    \"time_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944740312d77f8000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944740312d77f8000000612e\"\n      }\n    },\n    \"iterations_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b01612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b01612e\"\n      }\n    }\n  },\n  \"export_formats\": [],\n  \"status\": \"TERMINATED\",\n  \"start_time\": 1737596432.585557,\n  \"relative_logdir\": \"lambda_33a748e8_34_Gradient_LR=0.0263,Gradient_Momentum=0.9286,Gradient_Steps=490,Threshold=7,merge_threshold=11,number_of_points=_2025-01-23_02-40-31\",\n  \"runner\": null,\n  \"last_debug\": 0,\n  \"error_filename\": null,\n  \"pickled_error_filename\": null,\n  \"trial_name_creator\": null,\n  \"trial_dirname_creator\": null,\n  \"custom_trial_name\": null,\n  \"custom_dirname\": null,\n  \"experiment_dir_name\": \"test\",\n  \"saving_to\": null,\n  \"checkpoint_config\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"800595ff000000000000008c0e7261792e6169722e636f6e666967948c10436865636b706f696e74436f6e6669679493942981947d94288c0b6e756d5f746f5f6b656570944b038c1a636865636b706f696e745f73636f72655f617474726962757465948c12747261696e696e675f697465726174696f6e948c16636865636b706f696e745f73636f72655f6f72646572948c036d6178948c14636865636b706f696e745f6672657175656e6379944b008c11636865636b706f696e745f61745f656e64944e8c1a5f636865636b706f696e745f6b6565705f616c6c5f72616e6b7394898c1f5f636865636b706f696e745f75706c6f61645f66726f6d5f776f726b657273948975622e\"\n  },\n  \"checkpoint_manager\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"800595d2020000000000008c257261792e74756e652e657865637574696f6e2e636865636b706f696e745f6d616e61676572948c125f436865636b706f696e744d616e616765729493942981947d94288c145f636865636b706f696e745f7374726174656779948c0e7261792e6169722e636f6e666967948c10436865636b706f696e74436f6e6669679493942981947d94288c0b6e756d5f746f5f6b656570944b038c1a636865636b706f696e745f73636f72655f617474726962757465948c12747261696e696e675f697465726174696f6e948c16636865636b706f696e745f73636f72655f6f72646572948c036d6178948c14636865636b706f696e745f6672657175656e6379944b008c11636865636b706f696e745f61745f656e64944e8c1a5f636865636b706f696e745f6b6565705f616c6c5f72616e6b7394898c1f5f636865636b706f696e745f75706c6f61645f66726f6d5f776f726b657273948975628c155f6c61746573745f636865636b706f696e745f6964944b008c1a5f746f705f7065727369737465645f636865636b706f696e7473945d948c1a5f626573745f7065727369737465645f636865636b706f696e74944e8c1c5f6c61746573745f7065727369737465645f636865636b706f696e74944e8c195f6c61746573745f6d656d6f72795f636865636b706f696e74948c247261792e6169722e5f696e7465726e616c2e636865636b706f696e745f6d616e61676572948c125f547261636b6564436865636b706f696e749493942981947d94288c0b6469725f6f725f64617461944e8c026964944affffffff8c0c73746f726167655f6d6f646594681a8c11436865636b706f696e7453746f726167659493944b01859452948c0472616e6b944b008c076d657472696373947d948c076e6f64655f6970944e8c155f6d6f76655f696e73746561645f6f665f636f7079948875628c185f636865636b706f696e74735f746f5f636c65616e5f7570948f9475622e\"\n  },\n  \"restore_path\": null,\n  \"restoring_from\": null,\n  \"num_failures\": 0,\n  \"num_restore_failures\": 0,\n  \"results\": \"80054e2e\",\n  \"best_result\": \"80054e2e\",\n  \"param_config\": \"80054e2e\",\n  \"extra_arg\": \"80054e2e\",\n  \"_state_json\": null,\n  \"_state_valid\": false,\n  \"_resources\": \"80054e2e\"\n}",
    "{\n  \"stub\": false,\n  \"trainable_name\": \"lambda\",\n  \"trial_id\": \"f06b3fe5\",\n  \"sync_config\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059593000000000000008c0f7261792e74756e652e73796e636572948c0a53796e63436f6e6669679493942981947d94288c0a75706c6f61645f646972944e8c0673796e636572948c046175746f948c0b73796e635f706572696f64944d2c018c0c73796e635f74696d656f7574944d08078c0e73796e635f61727469666163747394888c1273796e635f6f6e5f636865636b706f696e74948875622e\"\n  },\n  \"_orig_experiment_path\": \"/Users/kuba/ijcai_ski/ray_results/test\",\n  \"_orig_experiment_dir_name\": \"test\",\n  \"_local_experiment_path\": \"/Users/kuba/ijcai_ski/ray_results/test\",\n  \"_remote_experiment_path\": null,\n  \"config\": {\n    \"Gradient_LR\": 0.023231390742639316,\n    \"Gradient_Momentum\": 0.9480017649570477,\n    \"Gradient_Steps\": 300,\n    \"Threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308070000000000000094869452942e\"\n    },\n    \"merge_threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b0074946243080d0000000000000094869452942e\"\n    },\n    \"number_of_points\": 600,\n    \"outliers_multiplier\": 4.970158764100548\n  },\n  \"_Trial__unresolved_config\": {\n    \"Gradient_LR\": 0.023231390742639316,\n    \"Gradient_Momentum\": 0.9480017649570477,\n    \"Gradient_Steps\": 300,\n    \"Threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308070000000000000094869452942e\"\n    },\n    \"merge_threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b0074946243080d0000000000000094869452942e\"\n    },\n    \"number_of_points\": 600,\n    \"outliers_multiplier\": 4.970158764100548\n  },\n  \"evaluated_params\": {\n    \"Gradient_LR\": 0.023231390742639316,\n    \"Gradient_Momentum\": 0.9480017649570477,\n    \"Gradient_Steps\": 300,\n    \"Threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308070000000000000094869452942e\"\n    },\n    \"merge_threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b0074946243080d0000000000000094869452942e\"\n    },\n    \"number_of_points\": 600,\n    \"outliers_multiplier\": 4.970158764100548\n  },\n  \"experiment_tag\": \"29_Gradient_LR=0.0232,Gradient_Momentum=0.9480,Gradient_Steps=300,Threshold=7,merge_threshold=13,number_of_points=600,outliers_multiplier=4.9702\",\n  \"location\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059546000000000000008c197261792e74756e652e6578706572696d656e742e747269616c948c095f4c6f636174696f6e9493942981947d94288c08686f73746e616d65944e8c03706964944e75622e\"\n  },\n  \"stopping_criterion\": {},\n  \"_setup_default_resource\": true,\n  \"_default_placement_group_factory\": \"800595aa000000000000008c237261792e74756e652e657865637574696f6e2e706c6163656d656e745f67726f757073948c15506c6163656d656e7447726f7570466163746f72799493942981947d94288c085f62756e646c6573945d947d948c0343505594473ff000000000000073618c155f686561645f62756e646c655f69735f656d70747994898c095f7374726174656779948c045041434b948c055f6172677394298c075f6b7761726773947d9475622e\",\n  \"placement_group_factory\": \"800595aa000000000000008c237261792e74756e652e657865637574696f6e2e706c6163656d656e745f67726f757073948c15506c6163656d656e7447726f7570466163746f72799493942981947d94288c085f62756e646c6573945d947d948c0343505594473ff000000000000073618c155f686561645f62756e646c655f69735f656d70747994898c095f7374726174656779948c045041434b948c055f6172677394298c075f6b7761726773947d9475622e\",\n  \"log_to_file\": [\n    null,\n    null\n  ],\n  \"max_failures\": 0,\n  \"_last_result\": {\n    \"score\": 0.8836570122245908,\n    \"time_this_iter_s\": 12.701223134994507,\n    \"done\": true,\n    \"training_iteration\": 1,\n    \"trial_id\": \"f06b3fe5\",\n    \"date\": \"2025-01-23_02-40-37\",\n    \"timestamp\": 1737596437,\n    \"time_total_s\": 12.701223134994507,\n    \"pid\": 30619,\n    \"hostname\": \"MacBook-Pro.ds19.agh.edu.pl\",\n    \"node_ip\": \"127.0.0.1\",\n    \"config\": {\n      \"Gradient_LR\": 0.023231390742639316,\n      \"Gradient_Momentum\": 0.9480017649570477,\n      \"Gradient_Steps\": 300,\n      \"Threshold\": 7,\n      \"merge_threshold\": 13,\n      \"number_of_points\": 600,\n      \"outliers_multiplier\": 4.970158764100548\n    },\n    \"time_since_restore\": 12.701223134994507,\n    \"iterations_since_restore\": 1,\n    \"experiment_tag\": \"29_Gradient_LR=0.0232,Gradient_Momentum=0.9480,Gradient_Steps=300,Threshold=7,merge_threshold=13,number_of_points=600,outliers_multiplier=4.9702\"\n  },\n  \"_default_result_or_future\": null,\n  \"last_update_time\": 1737596437.8391948,\n  \"metric_analysis\": {\n    \"score\": {\n      \"max\": 0.8836570122245908,\n      \"min\": 0.8836570122245908,\n      \"avg\": 0.8836570122245908,\n      \"last\": 0.8836570122245908,\n      \"last-5-avg\": 0.8836570122245908,\n      \"last-10-avg\": 0.8836570122245908\n    },\n    \"time_this_iter_s\": {\n      \"max\": 12.701223134994507,\n      \"min\": 12.701223134994507,\n      \"avg\": 12.701223134994507,\n      \"last\": 12.701223134994507,\n      \"last-5-avg\": 12.701223134994507,\n      \"last-10-avg\": 12.701223134994507\n    },\n    \"done\": {\n      \"max\": false,\n      \"min\": false,\n      \"avg\": false,\n      \"last\": false,\n      \"last-5-avg\": false,\n      \"last-10-avg\": false\n    },\n    \"training_iteration\": {\n      \"max\": 1,\n      \"min\": 1,\n      \"avg\": 1,\n      \"last\": 1,\n      \"last-5-avg\": 1,\n      \"last-10-avg\": 1\n    },\n    \"time_total_s\": {\n      \"max\": 12.701223134994507,\n      \"min\": 12.701223134994507,\n      \"avg\": 12.701223134994507,\n      \"last\": 12.701223134994507,\n      \"last-5-avg\": 12.701223134994507,\n      \"last-10-avg\": 12.701223134994507\n    },\n    \"time_since_restore\": {\n      \"max\": 12.701223134994507,\n      \"min\": 12.701223134994507,\n      \"avg\": 12.701223134994507,\n      \"last\": 12.701223134994507,\n      \"last-5-avg\": 12.701223134994507,\n      \"last-10-avg\": 12.701223134994507\n    },\n    \"iterations_since_restore\": {\n      \"max\": 1,\n      \"min\": 1,\n      \"avg\": 1,\n      \"last\": 1,\n      \"last-5-avg\": 1,\n      \"last-10-avg\": 1\n    }\n  },\n  \"n_steps\": [\n    5,\n    10\n  ],\n  \"metric_n_steps\": {\n    \"score\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059589000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452948c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02663894898887945294284b038c013c944e4e4e4affffffff4affffffff4b00749462430892570c12eb46ec3f9486945294612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059589000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452948c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02663894898887945294284b038c013c944e4e4e4affffffff4affffffff4b00749462430892570c12eb46ec3f9486945294612e\"\n      }\n    },\n    \"time_this_iter_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944740296706b8000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944740296706b8000000612e\"\n      }\n    },\n    \"done\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059522000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529489612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059522000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529489612e\"\n      }\n    },\n    \"training_iteration\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b01612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b01612e\"\n      }\n    },\n    \"time_total_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944740296706b8000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944740296706b8000000612e\"\n      }\n    },\n    \"time_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944740296706b8000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944740296706b8000000612e\"\n      }\n    },\n    \"iterations_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b01612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b01612e\"\n      }\n    }\n  },\n  \"export_formats\": [],\n  \"status\": \"TERMINATED\",\n  \"start_time\": 1737596425.128474,\n  \"relative_logdir\": \"lambda_f06b3fe5_29_Gradient_LR=0.0232,Gradient_Momentum=0.9480,Gradient_Steps=300,Threshold=7,merge_threshold=13,number_of_points=_2025-01-23_02-40-24\",\n  \"runner\": null,\n  \"last_debug\": 0,\n  \"error_filename\": null,\n  \"pickled_error_filename\": null,\n  \"trial_name_creator\": null,\n  \"trial_dirname_creator\": null,\n  \"custom_trial_name\": null,\n  \"custom_dirname\": null,\n  \"experiment_dir_name\": \"test\",\n  \"saving_to\": null,\n  \"checkpoint_config\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"800595ff000000000000008c0e7261792e6169722e636f6e666967948c10436865636b706f696e74436f6e6669679493942981947d94288c0b6e756d5f746f5f6b656570944b038c1a636865636b706f696e745f73636f72655f617474726962757465948c12747261696e696e675f697465726174696f6e948c16636865636b706f696e745f73636f72655f6f72646572948c036d6178948c14636865636b706f696e745f6672657175656e6379944b008c11636865636b706f696e745f61745f656e64944e8c1a5f636865636b706f696e745f6b6565705f616c6c5f72616e6b7394898c1f5f636865636b706f696e745f75706c6f61645f66726f6d5f776f726b657273948975622e\"\n  },\n  \"checkpoint_manager\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"800595d2020000000000008c257261792e74756e652e657865637574696f6e2e636865636b706f696e745f6d616e61676572948c125f436865636b706f696e744d616e616765729493942981947d94288c145f636865636b706f696e745f7374726174656779948c0e7261792e6169722e636f6e666967948c10436865636b706f696e74436f6e6669679493942981947d94288c0b6e756d5f746f5f6b656570944b038c1a636865636b706f696e745f73636f72655f617474726962757465948c12747261696e696e675f697465726174696f6e948c16636865636b706f696e745f73636f72655f6f72646572948c036d6178948c14636865636b706f696e745f6672657175656e6379944b008c11636865636b706f696e745f61745f656e64944e8c1a5f636865636b706f696e745f6b6565705f616c6c5f72616e6b7394898c1f5f636865636b706f696e745f75706c6f61645f66726f6d5f776f726b657273948975628c155f6c61746573745f636865636b706f696e745f6964944b008c1a5f746f705f7065727369737465645f636865636b706f696e7473945d948c1a5f626573745f7065727369737465645f636865636b706f696e74944e8c1c5f6c61746573745f7065727369737465645f636865636b706f696e74944e8c195f6c61746573745f6d656d6f72795f636865636b706f696e74948c247261792e6169722e5f696e7465726e616c2e636865636b706f696e745f6d616e61676572948c125f547261636b6564436865636b706f696e749493942981947d94288c0b6469725f6f725f64617461944e8c026964944affffffff8c0c73746f726167655f6d6f646594681a8c11436865636b706f696e7453746f726167659493944b01859452948c0472616e6b944b008c076d657472696373947d948c076e6f64655f6970944e8c155f6d6f76655f696e73746561645f6f665f636f7079948875628c185f636865636b706f696e74735f746f5f636c65616e5f7570948f9475622e\"\n  },\n  \"restore_path\": null,\n  \"restoring_from\": null,\n  \"num_failures\": 0,\n  \"num_restore_failures\": 0,\n  \"results\": \"80054e2e\",\n  \"best_result\": \"80054e2e\",\n  \"param_config\": \"80054e2e\",\n  \"extra_arg\": \"80054e2e\",\n  \"_state_json\": null,\n  \"_state_valid\": false,\n  \"_resources\": \"80054e2e\"\n}",
    "{\n  \"stub\": false,\n  \"trainable_name\": \"lambda\",\n  \"trial_id\": \"eaa141d8\",\n  \"sync_config\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059593000000000000008c0f7261792e74756e652e73796e636572948c0a53796e63436f6e6669679493942981947d94288c0a75706c6f61645f646972944e8c0673796e636572948c046175746f948c0b73796e635f706572696f64944d2c018c0c73796e635f74696d656f7574944d08078c0e73796e635f61727469666163747394888c1273796e635f6f6e5f636865636b706f696e74948875622e\"\n  },\n  \"_orig_experiment_path\": \"/Users/kuba/ijcai_ski/ray_results/test\",\n  \"_orig_experiment_dir_name\": \"test\",\n  \"_local_experiment_path\": \"/Users/kuba/ijcai_ski/ray_results/test\",\n  \"_remote_experiment_path\": null,\n  \"config\": {\n    \"Gradient_LR\": 0.03324625058228744,\n    \"Gradient_Momentum\": 0.9858176539990473,\n    \"Gradient_Steps\": 310,\n    \"Threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308000000000000000094869452942e\"\n    },\n    \"merge_threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308110000000000000094869452942e\"\n    },\n    \"number_of_points\": 100,\n    \"outliers_multiplier\": 4.65895142737396\n  },\n  \"_Trial__unresolved_config\": {\n    \"Gradient_LR\": 0.03324625058228744,\n    \"Gradient_Momentum\": 0.9858176539990473,\n    \"Gradient_Steps\": 310,\n    \"Threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308000000000000000094869452942e\"\n    },\n    \"merge_threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308110000000000000094869452942e\"\n    },\n    \"number_of_points\": 100,\n    \"outliers_multiplier\": 4.65895142737396\n  },\n  \"evaluated_params\": {\n    \"Gradient_LR\": 0.03324625058228744,\n    \"Gradient_Momentum\": 0.9858176539990473,\n    \"Gradient_Steps\": 310,\n    \"Threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308000000000000000094869452942e\"\n    },\n    \"merge_threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308110000000000000094869452942e\"\n    },\n    \"number_of_points\": 100,\n    \"outliers_multiplier\": 4.65895142737396\n  },\n  \"experiment_tag\": \"33_Gradient_LR=0.0332,Gradient_Momentum=0.9858,Gradient_Steps=310,Threshold=0,merge_threshold=17,number_of_points=100,outliers_multiplier=4.6590\",\n  \"location\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059546000000000000008c197261792e74756e652e6578706572696d656e742e747269616c948c095f4c6f636174696f6e9493942981947d94288c08686f73746e616d65944e8c03706964944e75622e\"\n  },\n  \"stopping_criterion\": {},\n  \"_setup_default_resource\": true,\n  \"_default_placement_group_factory\": \"800595aa000000000000008c237261792e74756e652e657865637574696f6e2e706c6163656d656e745f67726f757073948c15506c6163656d656e7447726f7570466163746f72799493942981947d94288c085f62756e646c6573945d947d948c0343505594473ff000000000000073618c155f686561645f62756e646c655f69735f656d70747994898c095f7374726174656779948c045041434b948c055f6172677394298c075f6b7761726773947d9475622e\",\n  \"placement_group_factory\": \"800595aa000000000000008c237261792e74756e652e657865637574696f6e2e706c6163656d656e745f67726f757073948c15506c6163656d656e7447726f7570466163746f72799493942981947d94288c085f62756e646c6573945d947d948c0343505594473ff000000000000073618c155f686561645f62756e646c655f69735f656d70747994898c095f7374726174656779948c045041434b948c055f6172677394298c075f6b7761726773947d9475622e\",\n  \"log_to_file\": [\n    null,\n    null\n  ],\n  \"max_failures\": 0,\n  \"_last_result\": {\n    \"score\": 0.9298143591230857,\n    \"time_this_iter_s\": 12.613407135009766,\n    \"done\": true,\n    \"training_iteration\": 1,\n    \"trial_id\": \"eaa141d8\",\n    \"date\": \"2025-01-23_02-40-44\",\n    \"timestamp\": 1737596444,\n    \"time_total_s\": 12.613407135009766,\n    \"pid\": 30510,\n    \"hostname\": \"MacBook-Pro.ds19.agh.edu.pl\",\n    \"node_ip\": \"127.0.0.1\",\n    \"config\": {\n      \"Gradient_LR\": 0.03324625058228744,\n      \"Gradient_Momentum\": 0.9858176539990473,\n      \"Gradient_Steps\": 310,\n      \"Threshold\": 0,\n      \"merge_threshold\": 17,\n      \"number_of_points\": 100,\n      \"outliers_multiplier\": 4.65895142737396\n    },\n    \"time_since_restore\": 12.613407135009766,\n    \"iterations_since_restore\": 1,\n    \"experiment_tag\": \"33_Gradient_LR=0.0332,Gradient_Momentum=0.9858,Gradient_Steps=310,Threshold=0,merge_threshold=17,number_of_points=100,outliers_multiplier=4.6590\"\n  },\n  \"_default_result_or_future\": null,\n  \"last_update_time\": 1737596444.352936,\n  \"metric_analysis\": {\n    \"score\": {\n      \"max\": 0.9298143591230857,\n      \"min\": 0.9298143591230857,\n      \"avg\": 0.9298143591230857,\n      \"last\": 0.9298143591230857,\n      \"last-5-avg\": 0.9298143591230857,\n      \"last-10-avg\": 0.9298143591230857\n    },\n    \"time_this_iter_s\": {\n      \"max\": 12.613407135009766,\n      \"min\": 12.613407135009766,\n      \"avg\": 12.613407135009766,\n      \"last\": 12.613407135009766,\n      \"last-5-avg\": 12.613407135009766,\n      \"last-10-avg\": 12.613407135009766\n    },\n    \"done\": {\n      \"max\": false,\n      \"min\": false,\n      \"avg\": false,\n      \"last\": false,\n      \"last-5-avg\": false,\n      \"last-10-avg\": false\n    },\n    \"training_iteration\": {\n      \"max\": 1,\n      \"min\": 1,\n      \"avg\": 1,\n      \"last\": 1,\n      \"last-5-avg\": 1,\n      \"last-10-avg\": 1\n    },\n    \"time_total_s\": {\n      \"max\": 12.613407135009766,\n      \"min\": 12.613407135009766,\n      \"avg\": 12.613407135009766,\n      \"last\": 12.613407135009766,\n      \"last-5-avg\": 12.613407135009766,\n      \"last-10-avg\": 12.613407135009766\n    },\n    \"time_since_restore\": {\n      \"max\": 12.613407135009766,\n      \"min\": 12.613407135009766,\n      \"avg\": 12.613407135009766,\n      \"last\": 12.613407135009766,\n      \"last-5-avg\": 12.613407135009766,\n      \"last-10-avg\": 12.613407135009766\n    },\n    \"iterations_since_restore\": {\n      \"max\": 1,\n      \"min\": 1,\n      \"avg\": 1,\n      \"last\": 1,\n      \"last-5-avg\": 1,\n      \"last-10-avg\": 1\n    }\n  },\n  \"n_steps\": [\n    5,\n    10\n  ],\n  \"metric_n_steps\": {\n    \"score\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059589000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452948c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02663894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308831df90a0ac1ed3f9486945294612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059589000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452948c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02663894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308831df90a0ac1ed3f9486945294612e\"\n      }\n    },\n    \"time_this_iter_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944740293a1080000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944740293a1080000000612e\"\n      }\n    },\n    \"done\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059522000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529489612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059522000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529489612e\"\n      }\n    },\n    \"training_iteration\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b01612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b01612e\"\n      }\n    },\n    \"time_total_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944740293a1080000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944740293a1080000000612e\"\n      }\n    },\n    \"time_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944740293a1080000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944740293a1080000000612e\"\n      }\n    },\n    \"iterations_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b01612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b01612e\"\n      }\n    }\n  },\n  \"export_formats\": [],\n  \"status\": \"TERMINATED\",\n  \"start_time\": 1737596431.733959,\n  \"relative_logdir\": \"lambda_eaa141d8_33_Gradient_LR=0.0332,Gradient_Momentum=0.9858,Gradient_Steps=310,Threshold=0,merge_threshold=17,number_of_points=_2025-01-23_02-40-29\",\n  \"runner\": null,\n  \"last_debug\": 0,\n  \"error_filename\": null,\n  \"pickled_error_filename\": null,\n  \"trial_name_creator\": null,\n  \"trial_dirname_creator\": null,\n  \"custom_trial_name\": null,\n  \"custom_dirname\": null,\n  \"experiment_dir_name\": \"test\",\n  \"saving_to\": null,\n  \"checkpoint_config\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"800595ff000000000000008c0e7261792e6169722e636f6e666967948c10436865636b706f696e74436f6e6669679493942981947d94288c0b6e756d5f746f5f6b656570944b038c1a636865636b706f696e745f73636f72655f617474726962757465948c12747261696e696e675f697465726174696f6e948c16636865636b706f696e745f73636f72655f6f72646572948c036d6178948c14636865636b706f696e745f6672657175656e6379944b008c11636865636b706f696e745f61745f656e64944e8c1a5f636865636b706f696e745f6b6565705f616c6c5f72616e6b7394898c1f5f636865636b706f696e745f75706c6f61645f66726f6d5f776f726b657273948975622e\"\n  },\n  \"checkpoint_manager\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"800595d2020000000000008c257261792e74756e652e657865637574696f6e2e636865636b706f696e745f6d616e61676572948c125f436865636b706f696e744d616e616765729493942981947d94288c145f636865636b706f696e745f7374726174656779948c0e7261792e6169722e636f6e666967948c10436865636b706f696e74436f6e6669679493942981947d94288c0b6e756d5f746f5f6b656570944b038c1a636865636b706f696e745f73636f72655f617474726962757465948c12747261696e696e675f697465726174696f6e948c16636865636b706f696e745f73636f72655f6f72646572948c036d6178948c14636865636b706f696e745f6672657175656e6379944b008c11636865636b706f696e745f61745f656e64944e8c1a5f636865636b706f696e745f6b6565705f616c6c5f72616e6b7394898c1f5f636865636b706f696e745f75706c6f61645f66726f6d5f776f726b657273948975628c155f6c61746573745f636865636b706f696e745f6964944b008c1a5f746f705f7065727369737465645f636865636b706f696e7473945d948c1a5f626573745f7065727369737465645f636865636b706f696e74944e8c1c5f6c61746573745f7065727369737465645f636865636b706f696e74944e8c195f6c61746573745f6d656d6f72795f636865636b706f696e74948c247261792e6169722e5f696e7465726e616c2e636865636b706f696e745f6d616e61676572948c125f547261636b6564436865636b706f696e749493942981947d94288c0b6469725f6f725f64617461944e8c026964944affffffff8c0c73746f726167655f6d6f646594681a8c11436865636b706f696e7453746f726167659493944b01859452948c0472616e6b944b008c076d657472696373947d948c076e6f64655f6970944e8c155f6d6f76655f696e73746561645f6f665f636f7079948875628c185f636865636b706f696e74735f746f5f636c65616e5f7570948f9475622e\"\n  },\n  \"restore_path\": null,\n  \"restoring_from\": null,\n  \"num_failures\": 0,\n  \"num_restore_failures\": 0,\n  \"results\": \"80054e2e\",\n  \"best_result\": \"80054e2e\",\n  \"param_config\": \"80054e2e\",\n  \"extra_arg\": \"80054e2e\",\n  \"_state_json\": null,\n  \"_state_valid\": false,\n  \"_resources\": \"80054e2e\"\n}",
    "{\n  \"stub\": false,\n  \"trainable_name\": \"lambda\",\n  \"trial_id\": \"23a22c85\",\n  \"sync_config\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059593000000000000008c0f7261792e74756e652e73796e636572948c0a53796e63436f6e6669679493942981947d94288c0a75706c6f61645f646972944e8c0673796e636572948c046175746f948c0b73796e635f706572696f64944d2c018c0c73796e635f74696d656f7574944d08078c0e73796e635f61727469666163747394888c1273796e635f6f6e5f636865636b706f696e74948875622e\"\n  },\n  \"_orig_experiment_path\": \"/Users/kuba/ijcai_ski/ray_results/test\",\n  \"_orig_experiment_dir_name\": \"test\",\n  \"_local_experiment_path\": \"/Users/kuba/ijcai_ski/ray_results/test\",\n  \"_remote_experiment_path\": null,\n  \"config\": {\n    \"Gradient_LR\": 0.015608669544562945,\n    \"Gradient_Momentum\": 0.9778050072888251,\n    \"Gradient_Steps\": 130,\n    \"Threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308020000000000000094869452942e\"\n    },\n    \"merge_threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b0074946243080e0000000000000094869452942e\"\n    },\n    \"number_of_points\": 900,\n    \"outliers_multiplier\": 4.797488038430615\n  },\n  \"_Trial__unresolved_config\": {\n    \"Gradient_LR\": 0.015608669544562945,\n    \"Gradient_Momentum\": 0.9778050072888251,\n    \"Gradient_Steps\": 130,\n    \"Threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308020000000000000094869452942e\"\n    },\n    \"merge_threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b0074946243080e0000000000000094869452942e\"\n    },\n    \"number_of_points\": 900,\n    \"outliers_multiplier\": 4.797488038430615\n  },\n  \"evaluated_params\": {\n    \"Gradient_LR\": 0.015608669544562945,\n    \"Gradient_Momentum\": 0.9778050072888251,\n    \"Gradient_Steps\": 130,\n    \"Threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308020000000000000094869452942e\"\n    },\n    \"merge_threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b0074946243080e0000000000000094869452942e\"\n    },\n    \"number_of_points\": 900,\n    \"outliers_multiplier\": 4.797488038430615\n  },\n  \"experiment_tag\": \"35_Gradient_LR=0.0156,Gradient_Momentum=0.9778,Gradient_Steps=130,Threshold=2,merge_threshold=14,number_of_points=900,outliers_multiplier=4.7975\",\n  \"location\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059546000000000000008c197261792e74756e652e6578706572696d656e742e747269616c948c095f4c6f636174696f6e9493942981947d94288c08686f73746e616d65944e8c03706964944e75622e\"\n  },\n  \"stopping_criterion\": {},\n  \"_setup_default_resource\": true,\n  \"_default_placement_group_factory\": \"800595aa000000000000008c237261792e74756e652e657865637574696f6e2e706c6163656d656e745f67726f757073948c15506c6163656d656e7447726f7570466163746f72799493942981947d94288c085f62756e646c6573945d947d948c0343505594473ff000000000000073618c155f686561645f62756e646c655f69735f656d70747994898c095f7374726174656779948c045041434b948c055f6172677394298c075f6b7761726773947d9475622e\",\n  \"placement_group_factory\": \"800595aa000000000000008c237261792e74756e652e657865637574696f6e2e706c6163656d656e745f67726f757073948c15506c6163656d656e7447726f7570466163746f72799493942981947d94288c085f62756e646c6573945d947d948c0343505594473ff000000000000073618c155f686561645f62756e646c655f69735f656d70747994898c095f7374726174656779948c045041434b948c055f6172677394298c075f6b7761726773947d9475622e\",\n  \"log_to_file\": [\n    null,\n    null\n  ],\n  \"max_failures\": 0,\n  \"_last_result\": {\n    \"score\": 0.9054196768835598,\n    \"time_this_iter_s\": 10.682409048080444,\n    \"done\": true,\n    \"training_iteration\": 1,\n    \"trial_id\": \"23a22c85\",\n    \"date\": \"2025-01-23_02-40-44\",\n    \"timestamp\": 1737596444,\n    \"time_total_s\": 10.682409048080444,\n    \"pid\": 30798,\n    \"hostname\": \"MacBook-Pro.ds19.agh.edu.pl\",\n    \"node_ip\": \"127.0.0.1\",\n    \"config\": {\n      \"Gradient_LR\": 0.015608669544562945,\n      \"Gradient_Momentum\": 0.9778050072888251,\n      \"Gradient_Steps\": 130,\n      \"Threshold\": 2,\n      \"merge_threshold\": 14,\n      \"number_of_points\": 900,\n      \"outliers_multiplier\": 4.797488038430615\n    },\n    \"time_since_restore\": 10.682409048080444,\n    \"iterations_since_restore\": 1,\n    \"experiment_tag\": \"35_Gradient_LR=0.0156,Gradient_Momentum=0.9778,Gradient_Steps=130,Threshold=2,merge_threshold=14,number_of_points=900,outliers_multiplier=4.7975\"\n  },\n  \"_default_result_or_future\": null,\n  \"last_update_time\": 1737596444.706065,\n  \"metric_analysis\": {\n    \"score\": {\n      \"max\": 0.9054196768835598,\n      \"min\": 0.9054196768835598,\n      \"avg\": 0.9054196768835598,\n      \"last\": 0.9054196768835598,\n      \"last-5-avg\": 0.9054196768835598,\n      \"last-10-avg\": 0.9054196768835598\n    },\n    \"time_this_iter_s\": {\n      \"max\": 10.682409048080444,\n      \"min\": 10.682409048080444,\n      \"avg\": 10.682409048080444,\n      \"last\": 10.682409048080444,\n      \"last-5-avg\": 10.682409048080444,\n      \"last-10-avg\": 10.682409048080444\n    },\n    \"done\": {\n      \"max\": false,\n      \"min\": false,\n      \"avg\": false,\n      \"last\": false,\n      \"last-5-avg\": false,\n      \"last-10-avg\": false\n    },\n    \"training_iteration\": {\n      \"max\": 1,\n      \"min\": 1,\n      \"avg\": 1,\n      \"last\": 1,\n      \"last-5-avg\": 1,\n      \"last-10-avg\": 1\n    },\n    \"time_total_s\": {\n      \"max\": 10.682409048080444,\n      \"min\": 10.682409048080444,\n      \"avg\": 10.682409048080444,\n      \"last\": 10.682409048080444,\n      \"last-5-avg\": 10.682409048080444,\n      \"last-10-avg\": 10.682409048080444\n    },\n    \"time_since_restore\": {\n      \"max\": 10.682409048080444,\n      \"min\": 10.682409048080444,\n      \"avg\": 10.682409048080444,\n      \"last\": 10.682409048080444,\n      \"last-5-avg\": 10.682409048080444,\n      \"last-10-avg\": 10.682409048080444\n    },\n    \"iterations_since_restore\": {\n      \"max\": 1,\n      \"min\": 1,\n      \"avg\": 1,\n      \"last\": 1,\n      \"last-5-avg\": 1,\n      \"last-10-avg\": 1\n    }\n  },\n  \"n_steps\": [\n    5,\n    10\n  ],\n  \"metric_n_steps\": {\n    \"score\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059589000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452948c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02663894898887945294284b038c013c944e4e4e4affffffff4affffffff4b00749462430836d5abaf32f9ec3f9486945294612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059589000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452948c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02663894898887945294284b038c013c944e4e4e4affffffff4affffffff4b00749462430836d5abaf32f9ec3f9486945294612e\"\n      }\n    },\n    \"time_this_iter_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944740255d64b8000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944740255d64b8000000612e\"\n      }\n    },\n    \"done\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059522000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529489612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059522000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529489612e\"\n      }\n    },\n    \"training_iteration\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b01612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b01612e\"\n      }\n    },\n    \"time_total_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944740255d64b8000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944740255d64b8000000612e\"\n      }\n    },\n    \"time_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944740255d64b8000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944740255d64b8000000612e\"\n      }\n    },\n    \"iterations_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b01612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b01612e\"\n      }\n    }\n  },\n  \"export_formats\": [],\n  \"status\": \"TERMINATED\",\n  \"start_time\": 1737596434.0116212,\n  \"relative_logdir\": \"lambda_23a22c85_35_Gradient_LR=0.0156,Gradient_Momentum=0.9778,Gradient_Steps=130,Threshold=2,merge_threshold=14,number_of_points=_2025-01-23_02-40-32\",\n  \"runner\": null,\n  \"last_debug\": 0,\n  \"error_filename\": null,\n  \"pickled_error_filename\": null,\n  \"trial_name_creator\": null,\n  \"trial_dirname_creator\": null,\n  \"custom_trial_name\": null,\n  \"custom_dirname\": null,\n  \"experiment_dir_name\": \"test\",\n  \"saving_to\": null,\n  \"checkpoint_config\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"800595ff000000000000008c0e7261792e6169722e636f6e666967948c10436865636b706f696e74436f6e6669679493942981947d94288c0b6e756d5f746f5f6b656570944b038c1a636865636b706f696e745f73636f72655f617474726962757465948c12747261696e696e675f697465726174696f6e948c16636865636b706f696e745f73636f72655f6f72646572948c036d6178948c14636865636b706f696e745f6672657175656e6379944b008c11636865636b706f696e745f61745f656e64944e8c1a5f636865636b706f696e745f6b6565705f616c6c5f72616e6b7394898c1f5f636865636b706f696e745f75706c6f61645f66726f6d5f776f726b657273948975622e\"\n  },\n  \"checkpoint_manager\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"800595d2020000000000008c257261792e74756e652e657865637574696f6e2e636865636b706f696e745f6d616e61676572948c125f436865636b706f696e744d616e616765729493942981947d94288c145f636865636b706f696e745f7374726174656779948c0e7261792e6169722e636f6e666967948c10436865636b706f696e74436f6e6669679493942981947d94288c0b6e756d5f746f5f6b656570944b038c1a636865636b706f696e745f73636f72655f617474726962757465948c12747261696e696e675f697465726174696f6e948c16636865636b706f696e745f73636f72655f6f72646572948c036d6178948c14636865636b706f696e745f6672657175656e6379944b008c11636865636b706f696e745f61745f656e64944e8c1a5f636865636b706f696e745f6b6565705f616c6c5f72616e6b7394898c1f5f636865636b706f696e745f75706c6f61645f66726f6d5f776f726b657273948975628c155f6c61746573745f636865636b706f696e745f6964944b008c1a5f746f705f7065727369737465645f636865636b706f696e7473945d948c1a5f626573745f7065727369737465645f636865636b706f696e74944e8c1c5f6c61746573745f7065727369737465645f636865636b706f696e74944e8c195f6c61746573745f6d656d6f72795f636865636b706f696e74948c247261792e6169722e5f696e7465726e616c2e636865636b706f696e745f6d616e61676572948c125f547261636b6564436865636b706f696e749493942981947d94288c0b6469725f6f725f64617461944e8c026964944affffffff8c0c73746f726167655f6d6f646594681a8c11436865636b706f696e7453746f726167659493944b01859452948c0472616e6b944b008c076d657472696373947d948c076e6f64655f6970944e8c155f6d6f76655f696e73746561645f6f665f636f7079948875628c185f636865636b706f696e74735f746f5f636c65616e5f7570948f9475622e\"\n  },\n  \"restore_path\": null,\n  \"restoring_from\": null,\n  \"num_failures\": 0,\n  \"num_restore_failures\": 0,\n  \"results\": \"80054e2e\",\n  \"best_result\": \"80054e2e\",\n  \"param_config\": \"80054e2e\",\n  \"extra_arg\": \"80054e2e\",\n  \"_state_json\": null,\n  \"_state_valid\": false,\n  \"_resources\": \"80054e2e\"\n}",
    "{\n  \"stub\": false,\n  \"trainable_name\": \"lambda\",\n  \"trial_id\": \"9f483cf7\",\n  \"sync_config\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059593000000000000008c0f7261792e74756e652e73796e636572948c0a53796e63436f6e6669679493942981947d94288c0a75706c6f61645f646972944e8c0673796e636572948c046175746f948c0b73796e635f706572696f64944d2c018c0c73796e635f74696d656f7574944d08078c0e73796e635f61727469666163747394888c1273796e635f6f6e5f636865636b706f696e74948875622e\"\n  },\n  \"_orig_experiment_path\": \"/Users/kuba/ijcai_ski/ray_results/test\",\n  \"_orig_experiment_dir_name\": \"test\",\n  \"_local_experiment_path\": \"/Users/kuba/ijcai_ski/ray_results/test\",\n  \"_remote_experiment_path\": null,\n  \"config\": {\n    \"Gradient_LR\": 0.02004207366074894,\n    \"Gradient_Momentum\": 0.9108778380236051,\n    \"Gradient_Steps\": 310,\n    \"Threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308000000000000000094869452942e\"\n    },\n    \"merge_threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308110000000000000094869452942e\"\n    },\n    \"number_of_points\": 100,\n    \"outliers_multiplier\": 4.959462078009578\n  },\n  \"_Trial__unresolved_config\": {\n    \"Gradient_LR\": 0.02004207366074894,\n    \"Gradient_Momentum\": 0.9108778380236051,\n    \"Gradient_Steps\": 310,\n    \"Threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308000000000000000094869452942e\"\n    },\n    \"merge_threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308110000000000000094869452942e\"\n    },\n    \"number_of_points\": 100,\n    \"outliers_multiplier\": 4.959462078009578\n  },\n  \"evaluated_params\": {\n    \"Gradient_LR\": 0.02004207366074894,\n    \"Gradient_Momentum\": 0.9108778380236051,\n    \"Gradient_Steps\": 310,\n    \"Threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308000000000000000094869452942e\"\n    },\n    \"merge_threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308110000000000000094869452942e\"\n    },\n    \"number_of_points\": 100,\n    \"outliers_multiplier\": 4.959462078009578\n  },\n  \"experiment_tag\": \"42_Gradient_LR=0.0200,Gradient_Momentum=0.9109,Gradient_Steps=310,Threshold=0,merge_threshold=17,number_of_points=100,outliers_multiplier=4.9595\",\n  \"location\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059546000000000000008c197261792e74756e652e6578706572696d656e742e747269616c948c095f4c6f636174696f6e9493942981947d94288c08686f73746e616d65944e8c03706964944e75622e\"\n  },\n  \"stopping_criterion\": {},\n  \"_setup_default_resource\": true,\n  \"_default_placement_group_factory\": \"800595aa000000000000008c237261792e74756e652e657865637574696f6e2e706c6163656d656e745f67726f757073948c15506c6163656d656e7447726f7570466163746f72799493942981947d94288c085f62756e646c6573945d947d948c0343505594473ff000000000000073618c155f686561645f62756e646c655f69735f656d70747994898c095f7374726174656779948c045041434b948c055f6172677394298c075f6b7761726773947d9475622e\",\n  \"placement_group_factory\": \"800595aa000000000000008c237261792e74756e652e657865637574696f6e2e706c6163656d656e745f67726f757073948c15506c6163656d656e7447726f7570466163746f72799493942981947d94288c085f62756e646c6573945d947d948c0343505594473ff000000000000073618c155f686561645f62756e646c655f69735f656d70747994898c095f7374726174656779948c045041434b948c055f6172677394298c075f6b7761726773947d9475622e\",\n  \"log_to_file\": [\n    null,\n    null\n  ],\n  \"max_failures\": 0,\n  \"_last_result\": {\n    \"score\": 0.9265362491737397,\n    \"time_this_iter_s\": 12.815465211868286,\n    \"done\": true,\n    \"training_iteration\": 1,\n    \"trial_id\": \"9f483cf7\",\n    \"date\": \"2025-01-23_02-40-57\",\n    \"timestamp\": 1737596457,\n    \"time_total_s\": 12.815465211868286,\n    \"pid\": 30798,\n    \"hostname\": \"MacBook-Pro.ds19.agh.edu.pl\",\n    \"node_ip\": \"127.0.0.1\",\n    \"config\": {\n      \"Gradient_LR\": 0.02004207366074894,\n      \"Gradient_Momentum\": 0.9108778380236051,\n      \"Gradient_Steps\": 310,\n      \"Threshold\": 0,\n      \"merge_threshold\": 17,\n      \"number_of_points\": 100,\n      \"outliers_multiplier\": 4.959462078009578\n    },\n    \"time_since_restore\": 12.815465211868286,\n    \"iterations_since_restore\": 1,\n    \"experiment_tag\": \"42_Gradient_LR=0.0200,Gradient_Momentum=0.9109,Gradient_Steps=310,Threshold=0,merge_threshold=17,number_of_points=100,outliers_multiplier=4.9595\"\n  },\n  \"_default_result_or_future\": null,\n  \"last_update_time\": 1737596457.570749,\n  \"metric_analysis\": {\n    \"score\": {\n      \"max\": 0.9265362491737397,\n      \"min\": 0.9265362491737397,\n      \"avg\": 0.9265362491737397,\n      \"last\": 0.9265362491737397,\n      \"last-5-avg\": 0.9265362491737397,\n      \"last-10-avg\": 0.9265362491737397\n    },\n    \"time_this_iter_s\": {\n      \"max\": 12.815465211868286,\n      \"min\": 12.815465211868286,\n      \"avg\": 12.815465211868286,\n      \"last\": 12.815465211868286,\n      \"last-5-avg\": 12.815465211868286,\n      \"last-10-avg\": 12.815465211868286\n    },\n    \"done\": {\n      \"max\": false,\n      \"min\": false,\n      \"avg\": false,\n      \"last\": false,\n      \"last-5-avg\": false,\n      \"last-10-avg\": false\n    },\n    \"training_iteration\": {\n      \"max\": 1,\n      \"min\": 1,\n      \"avg\": 1,\n      \"last\": 1,\n      \"last-5-avg\": 1,\n      \"last-10-avg\": 1\n    },\n    \"time_total_s\": {\n      \"max\": 12.815465211868286,\n      \"min\": 12.815465211868286,\n      \"avg\": 12.815465211868286,\n      \"last\": 12.815465211868286,\n      \"last-5-avg\": 12.815465211868286,\n      \"last-10-avg\": 12.815465211868286\n    },\n    \"time_since_restore\": {\n      \"max\": 12.815465211868286,\n      \"min\": 12.815465211868286,\n      \"avg\": 12.815465211868286,\n      \"last\": 12.815465211868286,\n      \"last-5-avg\": 12.815465211868286,\n      \"last-10-avg\": 12.815465211868286\n    },\n    \"iterations_since_restore\": {\n      \"max\": 1,\n      \"min\": 1,\n      \"avg\": 1,\n      \"last\": 1,\n      \"last-5-avg\": 1,\n      \"last-10-avg\": 1\n    }\n  },\n  \"n_steps\": [\n    5,\n    10\n  ],\n  \"metric_n_steps\": {\n    \"score\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059589000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452948c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02663894898887945294284b038c013c944e4e4e4affffffff4affffffff4b0074946243089e4f18592fa6ed3f9486945294612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059589000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452948c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02663894898887945294284b038c013c944e4e4e4affffffff4affffffff4b0074946243089e4f18592fa6ed3f9486945294612e\"\n      }\n    },\n    \"time_this_iter_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294474029a184a8000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294474029a184a8000000612e\"\n      }\n    },\n    \"done\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059522000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529489612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059522000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529489612e\"\n      }\n    },\n    \"training_iteration\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b01612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b01612e\"\n      }\n    },\n    \"time_total_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294474029a184a8000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294474029a184a8000000612e\"\n      }\n    },\n    \"time_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294474029a184a8000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294474029a184a8000000612e\"\n      }\n    },\n    \"iterations_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b01612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b01612e\"\n      }\n    }\n  },\n  \"export_formats\": [],\n  \"status\": \"TERMINATED\",\n  \"start_time\": 1737596444.72738,\n  \"relative_logdir\": \"lambda_9f483cf7_42_Gradient_LR=0.0200,Gradient_Momentum=0.9109,Gradient_Steps=310,Threshold=0,merge_threshold=17,number_of_points=_2025-01-23_02-40-44\",\n  \"runner\": null,\n  \"last_debug\": 0,\n  \"error_filename\": null,\n  \"pickled_error_filename\": null,\n  \"trial_name_creator\": null,\n  \"trial_dirname_creator\": null,\n  \"custom_trial_name\": null,\n  \"custom_dirname\": null,\n  \"experiment_dir_name\": \"test\",\n  \"saving_to\": null,\n  \"checkpoint_config\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"800595ff000000000000008c0e7261792e6169722e636f6e666967948c10436865636b706f696e74436f6e6669679493942981947d94288c0b6e756d5f746f5f6b656570944b038c1a636865636b706f696e745f73636f72655f617474726962757465948c12747261696e696e675f697465726174696f6e948c16636865636b706f696e745f73636f72655f6f72646572948c036d6178948c14636865636b706f696e745f6672657175656e6379944b008c11636865636b706f696e745f61745f656e64944e8c1a5f636865636b706f696e745f6b6565705f616c6c5f72616e6b7394898c1f5f636865636b706f696e745f75706c6f61645f66726f6d5f776f726b657273948975622e\"\n  },\n  \"checkpoint_manager\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"800595d2020000000000008c257261792e74756e652e657865637574696f6e2e636865636b706f696e745f6d616e61676572948c125f436865636b706f696e744d616e616765729493942981947d94288c145f636865636b706f696e745f7374726174656779948c0e7261792e6169722e636f6e666967948c10436865636b706f696e74436f6e6669679493942981947d94288c0b6e756d5f746f5f6b656570944b038c1a636865636b706f696e745f73636f72655f617474726962757465948c12747261696e696e675f697465726174696f6e948c16636865636b706f696e745f73636f72655f6f72646572948c036d6178948c14636865636b706f696e745f6672657175656e6379944b008c11636865636b706f696e745f61745f656e64944e8c1a5f636865636b706f696e745f6b6565705f616c6c5f72616e6b7394898c1f5f636865636b706f696e745f75706c6f61645f66726f6d5f776f726b657273948975628c155f6c61746573745f636865636b706f696e745f6964944b008c1a5f746f705f7065727369737465645f636865636b706f696e7473945d948c1a5f626573745f7065727369737465645f636865636b706f696e74944e8c1c5f6c61746573745f7065727369737465645f636865636b706f696e74944e8c195f6c61746573745f6d656d6f72795f636865636b706f696e74948c247261792e6169722e5f696e7465726e616c2e636865636b706f696e745f6d616e61676572948c125f547261636b6564436865636b706f696e749493942981947d94288c0b6469725f6f725f64617461944e8c026964944affffffff8c0c73746f726167655f6d6f646594681a8c11436865636b706f696e7453746f726167659493944b01859452948c0472616e6b944b008c076d657472696373947d948c076e6f64655f6970944e8c155f6d6f76655f696e73746561645f6f665f636f7079948875628c185f636865636b706f696e74735f746f5f636c65616e5f7570948f9475622e\"\n  },\n  \"restore_path\": null,\n  \"restoring_from\": null,\n  \"num_failures\": 0,\n  \"num_restore_failures\": 0,\n  \"results\": \"80054e2e\",\n  \"best_result\": \"80054e2e\",\n  \"param_config\": \"80054e2e\",\n  \"extra_arg\": \"80054e2e\",\n  \"_state_json\": null,\n  \"_state_valid\": false,\n  \"_resources\": \"80054e2e\"\n}",
    "{\n  \"stub\": false,\n  \"trainable_name\": \"lambda\",\n  \"trial_id\": \"7ed4d2d8\",\n  \"sync_config\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059593000000000000008c0f7261792e74756e652e73796e636572948c0a53796e63436f6e6669679493942981947d94288c0a75706c6f61645f646972944e8c0673796e636572948c046175746f948c0b73796e635f706572696f64944d2c018c0c73796e635f74696d656f7574944d08078c0e73796e635f61727469666163747394888c1273796e635f6f6e5f636865636b706f696e74948875622e\"\n  },\n  \"_orig_experiment_path\": \"/Users/kuba/ijcai_ski/ray_results/test\",\n  \"_orig_experiment_dir_name\": \"test\",\n  \"_local_experiment_path\": \"/Users/kuba/ijcai_ski/ray_results/test\",\n  \"_remote_experiment_path\": null,\n  \"config\": {\n    \"Gradient_LR\": 0.0628881668194244,\n    \"Gradient_Momentum\": 0.965196866173659,\n    \"Gradient_Steps\": 350,\n    \"Threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308000000000000000094869452942e\"\n    },\n    \"merge_threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308090000000000000094869452942e\"\n    },\n    \"number_of_points\": 200,\n    \"outliers_multiplier\": 2.7213592925582404\n  },\n  \"_Trial__unresolved_config\": {\n    \"Gradient_LR\": 0.0628881668194244,\n    \"Gradient_Momentum\": 0.965196866173659,\n    \"Gradient_Steps\": 350,\n    \"Threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308000000000000000094869452942e\"\n    },\n    \"merge_threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308090000000000000094869452942e\"\n    },\n    \"number_of_points\": 200,\n    \"outliers_multiplier\": 2.7213592925582404\n  },\n  \"evaluated_params\": {\n    \"Gradient_LR\": 0.0628881668194244,\n    \"Gradient_Momentum\": 0.965196866173659,\n    \"Gradient_Steps\": 350,\n    \"Threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308000000000000000094869452942e\"\n    },\n    \"merge_threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308090000000000000094869452942e\"\n    },\n    \"number_of_points\": 200,\n    \"outliers_multiplier\": 2.7213592925582404\n  },\n  \"experiment_tag\": \"40_Gradient_LR=0.0629,Gradient_Momentum=0.9652,Gradient_Steps=350,Threshold=0,merge_threshold=9,number_of_points=200,outliers_multiplier=2.7214\",\n  \"location\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059546000000000000008c197261792e74756e652e6578706572696d656e742e747269616c948c095f4c6f636174696f6e9493942981947d94288c08686f73746e616d65944e8c03706964944e75622e\"\n  },\n  \"stopping_criterion\": {},\n  \"_setup_default_resource\": true,\n  \"_default_placement_group_factory\": \"800595aa000000000000008c237261792e74756e652e657865637574696f6e2e706c6163656d656e745f67726f757073948c15506c6163656d656e7447726f7570466163746f72799493942981947d94288c085f62756e646c6573945d947d948c0343505594473ff000000000000073618c155f686561645f62756e646c655f69735f656d70747994898c095f7374726174656779948c045041434b948c055f6172677394298c075f6b7761726773947d9475622e\",\n  \"placement_group_factory\": \"800595aa000000000000008c237261792e74756e652e657865637574696f6e2e706c6163656d656e745f67726f757073948c15506c6163656d656e7447726f7570466163746f72799493942981947d94288c085f62756e646c6573945d947d948c0343505594473ff000000000000073618c155f686561645f62756e646c655f69735f656d70747994898c095f7374726174656779948c045041434b948c055f6172677394298c075f6b7761726773947d9475622e\",\n  \"log_to_file\": [\n    null,\n    null\n  ],\n  \"max_failures\": 0,\n  \"_last_result\": {\n    \"score\": 0.9352221545402267,\n    \"time_this_iter_s\": 14.830225706100464,\n    \"done\": true,\n    \"training_iteration\": 1,\n    \"trial_id\": \"7ed4d2d8\",\n    \"date\": \"2025-01-23_02-40-59\",\n    \"timestamp\": 1737596459,\n    \"time_total_s\": 14.830225706100464,\n    \"pid\": 30510,\n    \"hostname\": \"MacBook-Pro.ds19.agh.edu.pl\",\n    \"node_ip\": \"127.0.0.1\",\n    \"config\": {\n      \"Gradient_LR\": 0.0628881668194244,\n      \"Gradient_Momentum\": 0.965196866173659,\n      \"Gradient_Steps\": 350,\n      \"Threshold\": 0,\n      \"merge_threshold\": 9,\n      \"number_of_points\": 200,\n      \"outliers_multiplier\": 2.7213592925582404\n    },\n    \"time_since_restore\": 14.830225706100464,\n    \"iterations_since_restore\": 1,\n    \"experiment_tag\": \"40_Gradient_LR=0.0629,Gradient_Momentum=0.9652,Gradient_Steps=350,Threshold=0,merge_threshold=9,number_of_points=200,outliers_multiplier=2.7214\"\n  },\n  \"_default_result_or_future\": null,\n  \"last_update_time\": 1737596459.227387,\n  \"metric_analysis\": {\n    \"score\": {\n      \"max\": 0.9352221545402267,\n      \"min\": 0.9352221545402267,\n      \"avg\": 0.9352221545402267,\n      \"last\": 0.9352221545402267,\n      \"last-5-avg\": 0.9352221545402267,\n      \"last-10-avg\": 0.9352221545402267\n    },\n    \"time_this_iter_s\": {\n      \"max\": 14.830225706100464,\n      \"min\": 14.830225706100464,\n      \"avg\": 14.830225706100464,\n      \"last\": 14.830225706100464,\n      \"last-5-avg\": 14.830225706100464,\n      \"last-10-avg\": 14.830225706100464\n    },\n    \"done\": {\n      \"max\": false,\n      \"min\": false,\n      \"avg\": false,\n      \"last\": false,\n      \"last-5-avg\": false,\n      \"last-10-avg\": false\n    },\n    \"training_iteration\": {\n      \"max\": 1,\n      \"min\": 1,\n      \"avg\": 1,\n      \"last\": 1,\n      \"last-5-avg\": 1,\n      \"last-10-avg\": 1\n    },\n    \"time_total_s\": {\n      \"max\": 14.830225706100464,\n      \"min\": 14.830225706100464,\n      \"avg\": 14.830225706100464,\n      \"last\": 14.830225706100464,\n      \"last-5-avg\": 14.830225706100464,\n      \"last-10-avg\": 14.830225706100464\n    },\n    \"time_since_restore\": {\n      \"max\": 14.830225706100464,\n      \"min\": 14.830225706100464,\n      \"avg\": 14.830225706100464,\n      \"last\": 14.830225706100464,\n      \"last-5-avg\": 14.830225706100464,\n      \"last-10-avg\": 14.830225706100464\n    },\n    \"iterations_since_restore\": {\n      \"max\": 1,\n      \"min\": 1,\n      \"avg\": 1,\n      \"last\": 1,\n      \"last-5-avg\": 1,\n      \"last-10-avg\": 1\n    }\n  },\n  \"n_steps\": [\n    5,\n    10\n  ],\n  \"metric_n_steps\": {\n    \"score\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059589000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452948c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02663894898887945294284b038c013c944e4e4e4affffffff4affffffff4b0074946243087bd6070357eded3f9486945294612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059589000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452948c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02663894898887945294284b038c013c944e4e4e4affffffff4affffffff4b0074946243087bd6070357eded3f9486945294612e\"\n      }\n    },\n    \"time_this_iter_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529447402da91358000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529447402da91358000000612e\"\n      }\n    },\n    \"done\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059522000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529489612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059522000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529489612e\"\n      }\n    },\n    \"training_iteration\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b01612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b01612e\"\n      }\n    },\n    \"time_total_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529447402da91358000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529447402da91358000000612e\"\n      }\n    },\n    \"time_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529447402da91358000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529447402da91358000000612e\"\n      }\n    },\n    \"iterations_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b01612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b01612e\"\n      }\n    }\n  },\n  \"export_formats\": [],\n  \"status\": \"TERMINATED\",\n  \"start_time\": 1737596444.3629398,\n  \"relative_logdir\": \"lambda_7ed4d2d8_40_Gradient_LR=0.0629,Gradient_Momentum=0.9652,Gradient_Steps=350,Threshold=0,merge_threshold=9,number_of_points=2_2025-01-23_02-40-42\",\n  \"runner\": null,\n  \"last_debug\": 0,\n  \"error_filename\": null,\n  \"pickled_error_filename\": null,\n  \"trial_name_creator\": null,\n  \"trial_dirname_creator\": null,\n  \"custom_trial_name\": null,\n  \"custom_dirname\": null,\n  \"experiment_dir_name\": \"test\",\n  \"saving_to\": null,\n  \"checkpoint_config\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"800595ff000000000000008c0e7261792e6169722e636f6e666967948c10436865636b706f696e74436f6e6669679493942981947d94288c0b6e756d5f746f5f6b656570944b038c1a636865636b706f696e745f73636f72655f617474726962757465948c12747261696e696e675f697465726174696f6e948c16636865636b706f696e745f73636f72655f6f72646572948c036d6178948c14636865636b706f696e745f6672657175656e6379944b008c11636865636b706f696e745f61745f656e64944e8c1a5f636865636b706f696e745f6b6565705f616c6c5f72616e6b7394898c1f5f636865636b706f696e745f75706c6f61645f66726f6d5f776f726b657273948975622e\"\n  },\n  \"checkpoint_manager\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"800595d2020000000000008c257261792e74756e652e657865637574696f6e2e636865636b706f696e745f6d616e61676572948c125f436865636b706f696e744d616e616765729493942981947d94288c145f636865636b706f696e745f7374726174656779948c0e7261792e6169722e636f6e666967948c10436865636b706f696e74436f6e6669679493942981947d94288c0b6e756d5f746f5f6b656570944b038c1a636865636b706f696e745f73636f72655f617474726962757465948c12747261696e696e675f697465726174696f6e948c16636865636b706f696e745f73636f72655f6f72646572948c036d6178948c14636865636b706f696e745f6672657175656e6379944b008c11636865636b706f696e745f61745f656e64944e8c1a5f636865636b706f696e745f6b6565705f616c6c5f72616e6b7394898c1f5f636865636b706f696e745f75706c6f61645f66726f6d5f776f726b657273948975628c155f6c61746573745f636865636b706f696e745f6964944b008c1a5f746f705f7065727369737465645f636865636b706f696e7473945d948c1a5f626573745f7065727369737465645f636865636b706f696e74944e8c1c5f6c61746573745f7065727369737465645f636865636b706f696e74944e8c195f6c61746573745f6d656d6f72795f636865636b706f696e74948c247261792e6169722e5f696e7465726e616c2e636865636b706f696e745f6d616e61676572948c125f547261636b6564436865636b706f696e749493942981947d94288c0b6469725f6f725f64617461944e8c026964944affffffff8c0c73746f726167655f6d6f646594681a8c11436865636b706f696e7453746f726167659493944b01859452948c0472616e6b944b008c076d657472696373947d948c076e6f64655f6970944e8c155f6d6f76655f696e73746561645f6f665f636f7079948875628c185f636865636b706f696e74735f746f5f636c65616e5f7570948f9475622e\"\n  },\n  \"restore_path\": null,\n  \"restoring_from\": null,\n  \"num_failures\": 0,\n  \"num_restore_failures\": 0,\n  \"results\": \"80054e2e\",\n  \"best_result\": \"80054e2e\",\n  \"param_config\": \"80054e2e\",\n  \"extra_arg\": \"80054e2e\",\n  \"_state_json\": null,\n  \"_state_valid\": false,\n  \"_resources\": \"80054e2e\"\n}",
    "{\n  \"stub\": false,\n  \"trainable_name\": \"lambda\",\n  \"trial_id\": \"e8780771\",\n  \"sync_config\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059593000000000000008c0f7261792e74756e652e73796e636572948c0a53796e63436f6e6669679493942981947d94288c0a75706c6f61645f646972944e8c0673796e636572948c046175746f948c0b73796e635f706572696f64944d2c018c0c73796e635f74696d656f7574944d08078c0e73796e635f61727469666163747394888c1273796e635f6f6e5f636865636b706f696e74948875622e\"\n  },\n  \"_orig_experiment_path\": \"/Users/kuba/ijcai_ski/ray_results/test\",\n  \"_orig_experiment_dir_name\": \"test\",\n  \"_local_experiment_path\": \"/Users/kuba/ijcai_ski/ray_results/test\",\n  \"_remote_experiment_path\": null,\n  \"config\": {\n    \"Gradient_LR\": 0.0876024928268502,\n    \"Gradient_Momentum\": 0.9590159022620575,\n    \"Gradient_Steps\": 360,\n    \"Threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308030000000000000094869452942e\"\n    },\n    \"merge_threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b0074946243080d0000000000000094869452942e\"\n    },\n    \"number_of_points\": 1000,\n    \"outliers_multiplier\": 2.9159944161445974\n  },\n  \"_Trial__unresolved_config\": {\n    \"Gradient_LR\": 0.0876024928268502,\n    \"Gradient_Momentum\": 0.9590159022620575,\n    \"Gradient_Steps\": 360,\n    \"Threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308030000000000000094869452942e\"\n    },\n    \"merge_threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b0074946243080d0000000000000094869452942e\"\n    },\n    \"number_of_points\": 1000,\n    \"outliers_multiplier\": 2.9159944161445974\n  },\n  \"evaluated_params\": {\n    \"Gradient_LR\": 0.0876024928268502,\n    \"Gradient_Momentum\": 0.9590159022620575,\n    \"Gradient_Steps\": 360,\n    \"Threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308030000000000000094869452942e\"\n    },\n    \"merge_threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b0074946243080d0000000000000094869452942e\"\n    },\n    \"number_of_points\": 1000,\n    \"outliers_multiplier\": 2.9159944161445974\n  },\n  \"experiment_tag\": \"38_Gradient_LR=0.0876,Gradient_Momentum=0.9590,Gradient_Steps=360,Threshold=3,merge_threshold=13,number_of_points=1000,outliers_multiplier=2.9160\",\n  \"location\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059546000000000000008c197261792e74756e652e6578706572696d656e742e747269616c948c095f4c6f636174696f6e9493942981947d94288c08686f73746e616d65944e8c03706964944e75622e\"\n  },\n  \"stopping_criterion\": {},\n  \"_setup_default_resource\": true,\n  \"_default_placement_group_factory\": \"800595aa000000000000008c237261792e74756e652e657865637574696f6e2e706c6163656d656e745f67726f757073948c15506c6163656d656e7447726f7570466163746f72799493942981947d94288c085f62756e646c6573945d947d948c0343505594473ff000000000000073618c155f686561645f62756e646c655f69735f656d70747994898c095f7374726174656779948c045041434b948c055f6172677394298c075f6b7761726773947d9475622e\",\n  \"placement_group_factory\": \"800595aa000000000000008c237261792e74756e652e657865637574696f6e2e706c6163656d656e745f67726f757073948c15506c6163656d656e7447726f7570466163746f72799493942981947d94288c085f62756e646c6573945d947d948c0343505594473ff000000000000073618c155f686561645f62756e646c655f69735f656d70747994898c095f7374726174656779948c045041434b948c055f6172677394298c075f6b7761726773947d9475622e\",\n  \"log_to_file\": [\n    null,\n    null\n  ],\n  \"max_failures\": 0,\n  \"_last_result\": {\n    \"score\": 0.890562100533919,\n    \"time_this_iter_s\": 15.905536890029907,\n    \"done\": true,\n    \"training_iteration\": 1,\n    \"trial_id\": \"e8780771\",\n    \"date\": \"2025-01-23_02-40-55\",\n    \"timestamp\": 1737596455,\n    \"time_total_s\": 15.905536890029907,\n    \"pid\": 30590,\n    \"hostname\": \"MacBook-Pro.ds19.agh.edu.pl\",\n    \"node_ip\": \"127.0.0.1\",\n    \"config\": {\n      \"Gradient_LR\": 0.0876024928268502,\n      \"Gradient_Momentum\": 0.9590159022620575,\n      \"Gradient_Steps\": 360,\n      \"Threshold\": 3,\n      \"merge_threshold\": 13,\n      \"number_of_points\": 1000,\n      \"outliers_multiplier\": 2.9159944161445974\n    },\n    \"time_since_restore\": 15.905536890029907,\n    \"iterations_since_restore\": 1,\n    \"experiment_tag\": \"38_Gradient_LR=0.0876,Gradient_Momentum=0.9590,Gradient_Steps=360,Threshold=3,merge_threshold=13,number_of_points=1000,outliers_multiplier=2.9160\"\n  },\n  \"_default_result_or_future\": null,\n  \"last_update_time\": 1737596455.289477,\n  \"metric_analysis\": {\n    \"score\": {\n      \"max\": 0.890562100533919,\n      \"min\": 0.890562100533919,\n      \"avg\": 0.890562100533919,\n      \"last\": 0.890562100533919,\n      \"last-5-avg\": 0.890562100533919,\n      \"last-10-avg\": 0.890562100533919\n    },\n    \"time_this_iter_s\": {\n      \"max\": 15.905536890029907,\n      \"min\": 15.905536890029907,\n      \"avg\": 15.905536890029907,\n      \"last\": 15.905536890029907,\n      \"last-5-avg\": 15.905536890029907,\n      \"last-10-avg\": 15.905536890029907\n    },\n    \"done\": {\n      \"max\": false,\n      \"min\": false,\n      \"avg\": false,\n      \"last\": false,\n      \"last-5-avg\": false,\n      \"last-10-avg\": false\n    },\n    \"training_iteration\": {\n      \"max\": 1,\n      \"min\": 1,\n      \"avg\": 1,\n      \"last\": 1,\n      \"last-5-avg\": 1,\n      \"last-10-avg\": 1\n    },\n    \"time_total_s\": {\n      \"max\": 15.905536890029907,\n      \"min\": 15.905536890029907,\n      \"avg\": 15.905536890029907,\n      \"last\": 15.905536890029907,\n      \"last-5-avg\": 15.905536890029907,\n      \"last-10-avg\": 15.905536890029907\n    },\n    \"time_since_restore\": {\n      \"max\": 15.905536890029907,\n      \"min\": 15.905536890029907,\n      \"avg\": 15.905536890029907,\n      \"last\": 15.905536890029907,\n      \"last-5-avg\": 15.905536890029907,\n      \"last-10-avg\": 15.905536890029907\n    },\n    \"iterations_since_restore\": {\n      \"max\": 1,\n      \"min\": 1,\n      \"avg\": 1,\n      \"last\": 1,\n      \"last-5-avg\": 1,\n      \"last-10-avg\": 1\n    }\n  },\n  \"n_steps\": [\n    5,\n    10\n  ],\n  \"metric_n_steps\": {\n    \"score\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059589000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452948c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02663894898887945294284b038c013c944e4e4e4affffffff4affffffff4b00749462430838351b177c7fec3f9486945294612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059589000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452948c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02663894898887945294284b038c013c944e4e4e4affffffff4affffffff4b00749462430838351b177c7fec3f9486945294612e\"\n      }\n    },\n    \"time_this_iter_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529447402fcfa288000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529447402fcfa288000000612e\"\n      }\n    },\n    \"done\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059522000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529489612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059522000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529489612e\"\n      }\n    },\n    \"training_iteration\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b01612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b01612e\"\n      }\n    },\n    \"time_total_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529447402fcfa288000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529447402fcfa288000000612e\"\n      }\n    },\n    \"time_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529447402fcfa288000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529447402fcfa288000000612e\"\n      }\n    },\n    \"iterations_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b01612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b01612e\"\n      }\n    }\n  },\n  \"export_formats\": [],\n  \"status\": \"TERMINATED\",\n  \"start_time\": 1737596439.3738549,\n  \"relative_logdir\": \"lambda_e8780771_38_Gradient_LR=0.0876,Gradient_Momentum=0.9590,Gradient_Steps=360,Threshold=3,merge_threshold=13,number_of_points=_2025-01-23_02-40-37\",\n  \"runner\": null,\n  \"last_debug\": 0,\n  \"error_filename\": null,\n  \"pickled_error_filename\": null,\n  \"trial_name_creator\": null,\n  \"trial_dirname_creator\": null,\n  \"custom_trial_name\": null,\n  \"custom_dirname\": null,\n  \"experiment_dir_name\": \"test\",\n  \"saving_to\": null,\n  \"checkpoint_config\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"800595ff000000000000008c0e7261792e6169722e636f6e666967948c10436865636b706f696e74436f6e6669679493942981947d94288c0b6e756d5f746f5f6b656570944b038c1a636865636b706f696e745f73636f72655f617474726962757465948c12747261696e696e675f697465726174696f6e948c16636865636b706f696e745f73636f72655f6f72646572948c036d6178948c14636865636b706f696e745f6672657175656e6379944b008c11636865636b706f696e745f61745f656e64944e8c1a5f636865636b706f696e745f6b6565705f616c6c5f72616e6b7394898c1f5f636865636b706f696e745f75706c6f61645f66726f6d5f776f726b657273948975622e\"\n  },\n  \"checkpoint_manager\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"800595d2020000000000008c257261792e74756e652e657865637574696f6e2e636865636b706f696e745f6d616e61676572948c125f436865636b706f696e744d616e616765729493942981947d94288c145f636865636b706f696e745f7374726174656779948c0e7261792e6169722e636f6e666967948c10436865636b706f696e74436f6e6669679493942981947d94288c0b6e756d5f746f5f6b656570944b038c1a636865636b706f696e745f73636f72655f617474726962757465948c12747261696e696e675f697465726174696f6e948c16636865636b706f696e745f73636f72655f6f72646572948c036d6178948c14636865636b706f696e745f6672657175656e6379944b008c11636865636b706f696e745f61745f656e64944e8c1a5f636865636b706f696e745f6b6565705f616c6c5f72616e6b7394898c1f5f636865636b706f696e745f75706c6f61645f66726f6d5f776f726b657273948975628c155f6c61746573745f636865636b706f696e745f6964944b008c1a5f746f705f7065727369737465645f636865636b706f696e7473945d948c1a5f626573745f7065727369737465645f636865636b706f696e74944e8c1c5f6c61746573745f7065727369737465645f636865636b706f696e74944e8c195f6c61746573745f6d656d6f72795f636865636b706f696e74948c247261792e6169722e5f696e7465726e616c2e636865636b706f696e745f6d616e61676572948c125f547261636b6564436865636b706f696e749493942981947d94288c0b6469725f6f725f64617461944e8c026964944affffffff8c0c73746f726167655f6d6f646594681a8c11436865636b706f696e7453746f726167659493944b01859452948c0472616e6b944b008c076d657472696373947d948c076e6f64655f6970944e8c155f6d6f76655f696e73746561645f6f665f636f7079948875628c185f636865636b706f696e74735f746f5f636c65616e5f7570948f9475622e\"\n  },\n  \"restore_path\": null,\n  \"restoring_from\": null,\n  \"num_failures\": 0,\n  \"num_restore_failures\": 0,\n  \"results\": \"80054e2e\",\n  \"best_result\": \"80054e2e\",\n  \"param_config\": \"80054e2e\",\n  \"extra_arg\": \"80054e2e\",\n  \"_state_json\": null,\n  \"_state_valid\": false,\n  \"_resources\": \"80054e2e\"\n}",
    "{\n  \"stub\": false,\n  \"trainable_name\": \"lambda\",\n  \"trial_id\": \"d471b1bb\",\n  \"sync_config\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059593000000000000008c0f7261792e74756e652e73796e636572948c0a53796e63436f6e6669679493942981947d94288c0a75706c6f61645f646972944e8c0673796e636572948c046175746f948c0b73796e635f706572696f64944d2c018c0c73796e635f74696d656f7574944d08078c0e73796e635f61727469666163747394888c1273796e635f6f6e5f636865636b706f696e74948875622e\"\n  },\n  \"_orig_experiment_path\": \"/Users/kuba/ijcai_ski/ray_results/test\",\n  \"_orig_experiment_dir_name\": \"test\",\n  \"_local_experiment_path\": \"/Users/kuba/ijcai_ski/ray_results/test\",\n  \"_remote_experiment_path\": null,\n  \"config\": {\n    \"Gradient_LR\": 0.037703677667892066,\n    \"Gradient_Momentum\": 0.9321562346628722,\n    \"Gradient_Steps\": 160,\n    \"Threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b0074946243080d0000000000000094869452942e\"\n    },\n    \"merge_threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308070000000000000094869452942e\"\n    },\n    \"number_of_points\": 700,\n    \"outliers_multiplier\": 3.33260327395788\n  },\n  \"_Trial__unresolved_config\": {\n    \"Gradient_LR\": 0.037703677667892066,\n    \"Gradient_Momentum\": 0.9321562346628722,\n    \"Gradient_Steps\": 160,\n    \"Threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b0074946243080d0000000000000094869452942e\"\n    },\n    \"merge_threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308070000000000000094869452942e\"\n    },\n    \"number_of_points\": 700,\n    \"outliers_multiplier\": 3.33260327395788\n  },\n  \"evaluated_params\": {\n    \"Gradient_LR\": 0.037703677667892066,\n    \"Gradient_Momentum\": 0.9321562346628722,\n    \"Gradient_Steps\": 160,\n    \"Threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b0074946243080d0000000000000094869452942e\"\n    },\n    \"merge_threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308070000000000000094869452942e\"\n    },\n    \"number_of_points\": 700,\n    \"outliers_multiplier\": 3.33260327395788\n  },\n  \"experiment_tag\": \"39_Gradient_LR=0.0377,Gradient_Momentum=0.9322,Gradient_Steps=160,Threshold=13,merge_threshold=7,number_of_points=700,outliers_multiplier=3.3326\",\n  \"location\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059546000000000000008c197261792e74756e652e6578706572696d656e742e747269616c948c095f4c6f636174696f6e9493942981947d94288c08686f73746e616d65944e8c03706964944e75622e\"\n  },\n  \"stopping_criterion\": {},\n  \"_setup_default_resource\": true,\n  \"_default_placement_group_factory\": \"800595aa000000000000008c237261792e74756e652e657865637574696f6e2e706c6163656d656e745f67726f757073948c15506c6163656d656e7447726f7570466163746f72799493942981947d94288c085f62756e646c6573945d947d948c0343505594473ff000000000000073618c155f686561645f62756e646c655f69735f656d70747994898c095f7374726174656779948c045041434b948c055f6172677394298c075f6b7761726773947d9475622e\",\n  \"placement_group_factory\": \"800595aa000000000000008c237261792e74756e652e657865637574696f6e2e706c6163656d656e745f67726f757073948c15506c6163656d656e7447726f7570466163746f72799493942981947d94288c085f62756e646c6573945d947d948c0343505594473ff000000000000073618c155f686561645f62756e646c655f69735f656d70747994898c095f7374726174656779948c045041434b948c055f6172677394298c075f6b7761726773947d9475622e\",\n  \"log_to_file\": [\n    null,\n    null\n  ],\n  \"max_failures\": 0,\n  \"_last_result\": {\n    \"score\": 0.8633840499301442,\n    \"time_this_iter_s\": 10.503724098205566,\n    \"done\": true,\n    \"training_iteration\": 1,\n    \"trial_id\": \"d471b1bb\",\n    \"date\": \"2025-01-23_02-40-53\",\n    \"timestamp\": 1737596453,\n    \"time_total_s\": 10.503724098205566,\n    \"pid\": 30487,\n    \"hostname\": \"MacBook-Pro.ds19.agh.edu.pl\",\n    \"node_ip\": \"127.0.0.1\",\n    \"config\": {\n      \"Gradient_LR\": 0.037703677667892066,\n      \"Gradient_Momentum\": 0.9321562346628722,\n      \"Gradient_Steps\": 160,\n      \"Threshold\": 13,\n      \"merge_threshold\": 7,\n      \"number_of_points\": 700,\n      \"outliers_multiplier\": 3.33260327395788\n    },\n    \"time_since_restore\": 10.503724098205566,\n    \"iterations_since_restore\": 1,\n    \"experiment_tag\": \"39_Gradient_LR=0.0377,Gradient_Momentum=0.9322,Gradient_Steps=160,Threshold=13,merge_threshold=7,number_of_points=700,outliers_multiplier=3.3326\"\n  },\n  \"_default_result_or_future\": null,\n  \"last_update_time\": 1737596453.226051,\n  \"metric_analysis\": {\n    \"score\": {\n      \"max\": 0.8633840499301442,\n      \"min\": 0.8633840499301442,\n      \"avg\": 0.8633840499301442,\n      \"last\": 0.8633840499301442,\n      \"last-5-avg\": 0.8633840499301442,\n      \"last-10-avg\": 0.8633840499301442\n    },\n    \"time_this_iter_s\": {\n      \"max\": 10.503724098205566,\n      \"min\": 10.503724098205566,\n      \"avg\": 10.503724098205566,\n      \"last\": 10.503724098205566,\n      \"last-5-avg\": 10.503724098205566,\n      \"last-10-avg\": 10.503724098205566\n    },\n    \"done\": {\n      \"max\": false,\n      \"min\": false,\n      \"avg\": false,\n      \"last\": false,\n      \"last-5-avg\": false,\n      \"last-10-avg\": false\n    },\n    \"training_iteration\": {\n      \"max\": 1,\n      \"min\": 1,\n      \"avg\": 1,\n      \"last\": 1,\n      \"last-5-avg\": 1,\n      \"last-10-avg\": 1\n    },\n    \"time_total_s\": {\n      \"max\": 10.503724098205566,\n      \"min\": 10.503724098205566,\n      \"avg\": 10.503724098205566,\n      \"last\": 10.503724098205566,\n      \"last-5-avg\": 10.503724098205566,\n      \"last-10-avg\": 10.503724098205566\n    },\n    \"time_since_restore\": {\n      \"max\": 10.503724098205566,\n      \"min\": 10.503724098205566,\n      \"avg\": 10.503724098205566,\n      \"last\": 10.503724098205566,\n      \"last-5-avg\": 10.503724098205566,\n      \"last-10-avg\": 10.503724098205566\n    },\n    \"iterations_since_restore\": {\n      \"max\": 1,\n      \"min\": 1,\n      \"avg\": 1,\n      \"last\": 1,\n      \"last-5-avg\": 1,\n      \"last-10-avg\": 1\n    }\n  },\n  \"n_steps\": [\n    5,\n    10\n  ],\n  \"metric_n_steps\": {\n    \"score\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059589000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452948c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02663894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308e6d04a96d7a0eb3f9486945294612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059589000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452948c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02663894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308e6d04a96d7a0eb3f9486945294612e\"\n      }\n    },\n    \"time_this_iter_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529447402501e820000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529447402501e820000000612e\"\n      }\n    },\n    \"done\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059522000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529489612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059522000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529489612e\"\n      }\n    },\n    \"training_iteration\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b01612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b01612e\"\n      }\n    },\n    \"time_total_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529447402501e820000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529447402501e820000000612e\"\n      }\n    },\n    \"time_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529447402501e820000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529447402501e820000000612e\"\n      }\n    },\n    \"iterations_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b01612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b01612e\"\n      }\n    }\n  },\n  \"export_formats\": [],\n  \"status\": \"TERMINATED\",\n  \"start_time\": 1737596442.7123258,\n  \"relative_logdir\": \"lambda_d471b1bb_39_Gradient_LR=0.0377,Gradient_Momentum=0.9322,Gradient_Steps=160,Threshold=13,merge_threshold=7,number_of_points=_2025-01-23_02-40-39\",\n  \"runner\": null,\n  \"last_debug\": 0,\n  \"error_filename\": null,\n  \"pickled_error_filename\": null,\n  \"trial_name_creator\": null,\n  \"trial_dirname_creator\": null,\n  \"custom_trial_name\": null,\n  \"custom_dirname\": null,\n  \"experiment_dir_name\": \"test\",\n  \"saving_to\": null,\n  \"checkpoint_config\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"800595ff000000000000008c0e7261792e6169722e636f6e666967948c10436865636b706f696e74436f6e6669679493942981947d94288c0b6e756d5f746f5f6b656570944b038c1a636865636b706f696e745f73636f72655f617474726962757465948c12747261696e696e675f697465726174696f6e948c16636865636b706f696e745f73636f72655f6f72646572948c036d6178948c14636865636b706f696e745f6672657175656e6379944b008c11636865636b706f696e745f61745f656e64944e8c1a5f636865636b706f696e745f6b6565705f616c6c5f72616e6b7394898c1f5f636865636b706f696e745f75706c6f61645f66726f6d5f776f726b657273948975622e\"\n  },\n  \"checkpoint_manager\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"800595d2020000000000008c257261792e74756e652e657865637574696f6e2e636865636b706f696e745f6d616e61676572948c125f436865636b706f696e744d616e616765729493942981947d94288c145f636865636b706f696e745f7374726174656779948c0e7261792e6169722e636f6e666967948c10436865636b706f696e74436f6e6669679493942981947d94288c0b6e756d5f746f5f6b656570944b038c1a636865636b706f696e745f73636f72655f617474726962757465948c12747261696e696e675f697465726174696f6e948c16636865636b706f696e745f73636f72655f6f72646572948c036d6178948c14636865636b706f696e745f6672657175656e6379944b008c11636865636b706f696e745f61745f656e64944e8c1a5f636865636b706f696e745f6b6565705f616c6c5f72616e6b7394898c1f5f636865636b706f696e745f75706c6f61645f66726f6d5f776f726b657273948975628c155f6c61746573745f636865636b706f696e745f6964944b008c1a5f746f705f7065727369737465645f636865636b706f696e7473945d948c1a5f626573745f7065727369737465645f636865636b706f696e74944e8c1c5f6c61746573745f7065727369737465645f636865636b706f696e74944e8c195f6c61746573745f6d656d6f72795f636865636b706f696e74948c247261792e6169722e5f696e7465726e616c2e636865636b706f696e745f6d616e61676572948c125f547261636b6564436865636b706f696e749493942981947d94288c0b6469725f6f725f64617461944e8c026964944affffffff8c0c73746f726167655f6d6f646594681a8c11436865636b706f696e7453746f726167659493944b01859452948c0472616e6b944b008c076d657472696373947d948c076e6f64655f6970944e8c155f6d6f76655f696e73746561645f6f665f636f7079948875628c185f636865636b706f696e74735f746f5f636c65616e5f7570948f9475622e\"\n  },\n  \"restore_path\": null,\n  \"restoring_from\": null,\n  \"num_failures\": 0,\n  \"num_restore_failures\": 0,\n  \"results\": \"80054e2e\",\n  \"best_result\": \"80054e2e\",\n  \"param_config\": \"80054e2e\",\n  \"extra_arg\": \"80054e2e\",\n  \"_state_json\": null,\n  \"_state_valid\": false,\n  \"_resources\": \"80054e2e\"\n}",
    "{\n  \"stub\": false,\n  \"trainable_name\": \"lambda\",\n  \"trial_id\": \"8ad0c71a\",\n  \"sync_config\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059593000000000000008c0f7261792e74756e652e73796e636572948c0a53796e63436f6e6669679493942981947d94288c0a75706c6f61645f646972944e8c0673796e636572948c046175746f948c0b73796e635f706572696f64944d2c018c0c73796e635f74696d656f7574944d08078c0e73796e635f61727469666163747394888c1273796e635f6f6e5f636865636b706f696e74948875622e\"\n  },\n  \"_orig_experiment_path\": \"/Users/kuba/ijcai_ski/ray_results/test\",\n  \"_orig_experiment_dir_name\": \"test\",\n  \"_local_experiment_path\": \"/Users/kuba/ijcai_ski/ray_results/test\",\n  \"_remote_experiment_path\": null,\n  \"config\": {\n    \"Gradient_LR\": 0.025150359888513853,\n    \"Gradient_Momentum\": 0.9952439771814127,\n    \"Gradient_Steps\": 340,\n    \"Threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b0074946243080b0000000000000094869452942e\"\n    },\n    \"merge_threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308080000000000000094869452942e\"\n    },\n    \"number_of_points\": 800,\n    \"outliers_multiplier\": 3.6205710824505273\n  },\n  \"_Trial__unresolved_config\": {\n    \"Gradient_LR\": 0.025150359888513853,\n    \"Gradient_Momentum\": 0.9952439771814127,\n    \"Gradient_Steps\": 340,\n    \"Threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b0074946243080b0000000000000094869452942e\"\n    },\n    \"merge_threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308080000000000000094869452942e\"\n    },\n    \"number_of_points\": 800,\n    \"outliers_multiplier\": 3.6205710824505273\n  },\n  \"evaluated_params\": {\n    \"Gradient_LR\": 0.025150359888513853,\n    \"Gradient_Momentum\": 0.9952439771814127,\n    \"Gradient_Steps\": 340,\n    \"Threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b0074946243080b0000000000000094869452942e\"\n    },\n    \"merge_threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308080000000000000094869452942e\"\n    },\n    \"number_of_points\": 800,\n    \"outliers_multiplier\": 3.6205710824505273\n  },\n  \"experiment_tag\": \"37_Gradient_LR=0.0252,Gradient_Momentum=0.9952,Gradient_Steps=340,Threshold=11,merge_threshold=8,number_of_points=800,outliers_multiplier=3.6206\",\n  \"location\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059546000000000000008c197261792e74756e652e6578706572696d656e742e747269616c948c095f4c6f636174696f6e9493942981947d94288c08686f73746e616d65944e8c03706964944e75622e\"\n  },\n  \"stopping_criterion\": {},\n  \"_setup_default_resource\": true,\n  \"_default_placement_group_factory\": \"800595aa000000000000008c237261792e74756e652e657865637574696f6e2e706c6163656d656e745f67726f757073948c15506c6163656d656e7447726f7570466163746f72799493942981947d94288c085f62756e646c6573945d947d948c0343505594473ff000000000000073618c155f686561645f62756e646c655f69735f656d70747994898c095f7374726174656779948c045041434b948c055f6172677394298c075f6b7761726773947d9475622e\",\n  \"placement_group_factory\": \"800595aa000000000000008c237261792e74756e652e657865637574696f6e2e706c6163656d656e745f67726f757073948c15506c6163656d656e7447726f7570466163746f72799493942981947d94288c085f62756e646c6573945d947d948c0343505594473ff000000000000073618c155f686561645f62756e646c655f69735f656d70747994898c095f7374726174656779948c045041434b948c055f6172677394298c075f6b7761726773947d9475622e\",\n  \"log_to_file\": [\n    null,\n    null\n  ],\n  \"max_failures\": 0,\n  \"_last_result\": {\n    \"score\": 0.8532166930142543,\n    \"time_this_iter_s\": 13.784097671508789,\n    \"done\": true,\n    \"training_iteration\": 1,\n    \"trial_id\": \"8ad0c71a\",\n    \"date\": \"2025-01-23_02-40-51\",\n    \"timestamp\": 1737596451,\n    \"time_total_s\": 13.784097671508789,\n    \"pid\": 30619,\n    \"hostname\": \"MacBook-Pro.ds19.agh.edu.pl\",\n    \"node_ip\": \"127.0.0.1\",\n    \"config\": {\n      \"Gradient_LR\": 0.025150359888513853,\n      \"Gradient_Momentum\": 0.9952439771814127,\n      \"Gradient_Steps\": 340,\n      \"Threshold\": 11,\n      \"merge_threshold\": 8,\n      \"number_of_points\": 800,\n      \"outliers_multiplier\": 3.6205710824505273\n    },\n    \"time_since_restore\": 13.784097671508789,\n    \"iterations_since_restore\": 1,\n    \"experiment_tag\": \"37_Gradient_LR=0.0252,Gradient_Momentum=0.9952,Gradient_Steps=340,Threshold=11,merge_threshold=8,number_of_points=800,outliers_multiplier=3.6206\"\n  },\n  \"_default_result_or_future\": null,\n  \"last_update_time\": 1737596451.6416261,\n  \"metric_analysis\": {\n    \"score\": {\n      \"max\": 0.8532166930142543,\n      \"min\": 0.8532166930142543,\n      \"avg\": 0.8532166930142543,\n      \"last\": 0.8532166930142543,\n      \"last-5-avg\": 0.8532166930142543,\n      \"last-10-avg\": 0.8532166930142543\n    },\n    \"time_this_iter_s\": {\n      \"max\": 13.784097671508789,\n      \"min\": 13.784097671508789,\n      \"avg\": 13.784097671508789,\n      \"last\": 13.784097671508789,\n      \"last-5-avg\": 13.784097671508789,\n      \"last-10-avg\": 13.784097671508789\n    },\n    \"done\": {\n      \"max\": false,\n      \"min\": false,\n      \"avg\": false,\n      \"last\": false,\n      \"last-5-avg\": false,\n      \"last-10-avg\": false\n    },\n    \"training_iteration\": {\n      \"max\": 1,\n      \"min\": 1,\n      \"avg\": 1,\n      \"last\": 1,\n      \"last-5-avg\": 1,\n      \"last-10-avg\": 1\n    },\n    \"time_total_s\": {\n      \"max\": 13.784097671508789,\n      \"min\": 13.784097671508789,\n      \"avg\": 13.784097671508789,\n      \"last\": 13.784097671508789,\n      \"last-5-avg\": 13.784097671508789,\n      \"last-10-avg\": 13.784097671508789\n    },\n    \"time_since_restore\": {\n      \"max\": 13.784097671508789,\n      \"min\": 13.784097671508789,\n      \"avg\": 13.784097671508789,\n      \"last\": 13.784097671508789,\n      \"last-5-avg\": 13.784097671508789,\n      \"last-10-avg\": 13.784097671508789\n    },\n    \"iterations_since_restore\": {\n      \"max\": 1,\n      \"min\": 1,\n      \"avg\": 1,\n      \"last\": 1,\n      \"last-5-avg\": 1,\n      \"last-10-avg\": 1\n    }\n  },\n  \"n_steps\": [\n    5,\n    10\n  ],\n  \"metric_n_steps\": {\n    \"score\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059589000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452948c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02663894898887945294284b038c013c944e4e4e4affffffff4affffffff4b00749462430845b81c188d4deb3f9486945294612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059589000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452948c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02663894898887945294284b038c013c944e4e4e4affffffff4affffffff4b00749462430845b81c188d4deb3f9486945294612e\"\n      }\n    },\n    \"time_this_iter_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529447402b917540000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529447402b917540000000612e\"\n      }\n    },\n    \"done\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059522000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529489612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059522000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529489612e\"\n      }\n    },\n    \"training_iteration\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b01612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b01612e\"\n      }\n    },\n    \"time_total_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529447402b917540000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529447402b917540000000612e\"\n      }\n    },\n    \"time_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529447402b917540000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529447402b917540000000612e\"\n      }\n    },\n    \"iterations_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b01612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b01612e\"\n      }\n    }\n  },\n  \"export_formats\": [],\n  \"status\": \"TERMINATED\",\n  \"start_time\": 1737596437.848924,\n  \"relative_logdir\": \"lambda_8ad0c71a_37_Gradient_LR=0.0252,Gradient_Momentum=0.9952,Gradient_Steps=340,Threshold=11,merge_threshold=8,number_of_points=_2025-01-23_02-40-37\",\n  \"runner\": null,\n  \"last_debug\": 0,\n  \"error_filename\": null,\n  \"pickled_error_filename\": null,\n  \"trial_name_creator\": null,\n  \"trial_dirname_creator\": null,\n  \"custom_trial_name\": null,\n  \"custom_dirname\": null,\n  \"experiment_dir_name\": \"test\",\n  \"saving_to\": null,\n  \"checkpoint_config\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"800595ff000000000000008c0e7261792e6169722e636f6e666967948c10436865636b706f696e74436f6e6669679493942981947d94288c0b6e756d5f746f5f6b656570944b038c1a636865636b706f696e745f73636f72655f617474726962757465948c12747261696e696e675f697465726174696f6e948c16636865636b706f696e745f73636f72655f6f72646572948c036d6178948c14636865636b706f696e745f6672657175656e6379944b008c11636865636b706f696e745f61745f656e64944e8c1a5f636865636b706f696e745f6b6565705f616c6c5f72616e6b7394898c1f5f636865636b706f696e745f75706c6f61645f66726f6d5f776f726b657273948975622e\"\n  },\n  \"checkpoint_manager\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"800595d2020000000000008c257261792e74756e652e657865637574696f6e2e636865636b706f696e745f6d616e61676572948c125f436865636b706f696e744d616e616765729493942981947d94288c145f636865636b706f696e745f7374726174656779948c0e7261792e6169722e636f6e666967948c10436865636b706f696e74436f6e6669679493942981947d94288c0b6e756d5f746f5f6b656570944b038c1a636865636b706f696e745f73636f72655f617474726962757465948c12747261696e696e675f697465726174696f6e948c16636865636b706f696e745f73636f72655f6f72646572948c036d6178948c14636865636b706f696e745f6672657175656e6379944b008c11636865636b706f696e745f61745f656e64944e8c1a5f636865636b706f696e745f6b6565705f616c6c5f72616e6b7394898c1f5f636865636b706f696e745f75706c6f61645f66726f6d5f776f726b657273948975628c155f6c61746573745f636865636b706f696e745f6964944b008c1a5f746f705f7065727369737465645f636865636b706f696e7473945d948c1a5f626573745f7065727369737465645f636865636b706f696e74944e8c1c5f6c61746573745f7065727369737465645f636865636b706f696e74944e8c195f6c61746573745f6d656d6f72795f636865636b706f696e74948c247261792e6169722e5f696e7465726e616c2e636865636b706f696e745f6d616e61676572948c125f547261636b6564436865636b706f696e749493942981947d94288c0b6469725f6f725f64617461944e8c026964944affffffff8c0c73746f726167655f6d6f646594681a8c11436865636b706f696e7453746f726167659493944b01859452948c0472616e6b944b008c076d657472696373947d948c076e6f64655f6970944e8c155f6d6f76655f696e73746561645f6f665f636f7079948875628c185f636865636b706f696e74735f746f5f636c65616e5f7570948f9475622e\"\n  },\n  \"restore_path\": null,\n  \"restoring_from\": null,\n  \"num_failures\": 0,\n  \"num_restore_failures\": 0,\n  \"results\": \"80054e2e\",\n  \"best_result\": \"80054e2e\",\n  \"param_config\": \"80054e2e\",\n  \"extra_arg\": \"80054e2e\",\n  \"_state_json\": null,\n  \"_state_valid\": false,\n  \"_resources\": \"80054e2e\"\n}",
    "{\n  \"stub\": false,\n  \"trainable_name\": \"lambda\",\n  \"trial_id\": \"119b9542\",\n  \"sync_config\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059593000000000000008c0f7261792e74756e652e73796e636572948c0a53796e63436f6e6669679493942981947d94288c0a75706c6f61645f646972944e8c0673796e636572948c046175746f948c0b73796e635f706572696f64944d2c018c0c73796e635f74696d656f7574944d08078c0e73796e635f61727469666163747394888c1273796e635f6f6e5f636865636b706f696e74948875622e\"\n  },\n  \"_orig_experiment_path\": \"/Users/kuba/ijcai_ski/ray_results/test\",\n  \"_orig_experiment_dir_name\": \"test\",\n  \"_local_experiment_path\": \"/Users/kuba/ijcai_ski/ray_results/test\",\n  \"_remote_experiment_path\": null,\n  \"config\": {\n    \"Gradient_LR\": 0.03242841175084241,\n    \"Gradient_Momentum\": 0.9849635797366073,\n    \"Gradient_Steps\": 310,\n    \"Threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308000000000000000094869452942e\"\n    },\n    \"merge_threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308110000000000000094869452942e\"\n    },\n    \"number_of_points\": 100,\n    \"outliers_multiplier\": 4.639712353328051\n  },\n  \"_Trial__unresolved_config\": {\n    \"Gradient_LR\": 0.03242841175084241,\n    \"Gradient_Momentum\": 0.9849635797366073,\n    \"Gradient_Steps\": 310,\n    \"Threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308000000000000000094869452942e\"\n    },\n    \"merge_threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308110000000000000094869452942e\"\n    },\n    \"number_of_points\": 100,\n    \"outliers_multiplier\": 4.639712353328051\n  },\n  \"evaluated_params\": {\n    \"Gradient_LR\": 0.03242841175084241,\n    \"Gradient_Momentum\": 0.9849635797366073,\n    \"Gradient_Steps\": 310,\n    \"Threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308000000000000000094869452942e\"\n    },\n    \"merge_threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308110000000000000094869452942e\"\n    },\n    \"number_of_points\": 100,\n    \"outliers_multiplier\": 4.639712353328051\n  },\n  \"experiment_tag\": \"41_Gradient_LR=0.0324,Gradient_Momentum=0.9850,Gradient_Steps=310,Threshold=0,merge_threshold=17,number_of_points=100,outliers_multiplier=4.6397\",\n  \"location\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059546000000000000008c197261792e74756e652e6578706572696d656e742e747269616c948c095f4c6f636174696f6e9493942981947d94288c08686f73746e616d65944e8c03706964944e75622e\"\n  },\n  \"stopping_criterion\": {},\n  \"_setup_default_resource\": true,\n  \"_default_placement_group_factory\": \"800595aa000000000000008c237261792e74756e652e657865637574696f6e2e706c6163656d656e745f67726f757073948c15506c6163656d656e7447726f7570466163746f72799493942981947d94288c085f62756e646c6573945d947d948c0343505594473ff000000000000073618c155f686561645f62756e646c655f69735f656d70747994898c095f7374726174656779948c045041434b948c055f6172677394298c075f6b7761726773947d9475622e\",\n  \"placement_group_factory\": \"800595aa000000000000008c237261792e74756e652e657865637574696f6e2e706c6163656d656e745f67726f757073948c15506c6163656d656e7447726f7570466163746f72799493942981947d94288c085f62756e646c6573945d947d948c0343505594473ff000000000000073618c155f686561645f62756e646c655f69735f656d70747994898c095f7374726174656779948c045041434b948c055f6172677394298c075f6b7761726773947d9475622e\",\n  \"log_to_file\": [\n    null,\n    null\n  ],\n  \"max_failures\": 0,\n  \"_last_result\": {\n    \"score\": 0.9324356345067774,\n    \"time_this_iter_s\": 14.142532110214233,\n    \"done\": true,\n    \"training_iteration\": 1,\n    \"trial_id\": \"119b9542\",\n    \"date\": \"2025-01-23_02-40-58\",\n    \"timestamp\": 1737596458,\n    \"time_total_s\": 14.142532110214233,\n    \"pid\": 30566,\n    \"hostname\": \"MacBook-Pro.ds19.agh.edu.pl\",\n    \"node_ip\": \"127.0.0.1\",\n    \"config\": {\n      \"Gradient_LR\": 0.03242841175084241,\n      \"Gradient_Momentum\": 0.9849635797366073,\n      \"Gradient_Steps\": 310,\n      \"Threshold\": 0,\n      \"merge_threshold\": 17,\n      \"number_of_points\": 100,\n      \"outliers_multiplier\": 4.639712353328051\n    },\n    \"time_since_restore\": 14.142532110214233,\n    \"iterations_since_restore\": 1,\n    \"experiment_tag\": \"41_Gradient_LR=0.0324,Gradient_Momentum=0.9850,Gradient_Steps=310,Threshold=0,merge_threshold=17,number_of_points=100,outliers_multiplier=4.6397\"\n  },\n  \"_default_result_or_future\": null,\n  \"last_update_time\": 1737596458.572041,\n  \"metric_analysis\": {\n    \"score\": {\n      \"max\": 0.9324356345067774,\n      \"min\": 0.9324356345067774,\n      \"avg\": 0.9324356345067774,\n      \"last\": 0.9324356345067774,\n      \"last-5-avg\": 0.9324356345067774,\n      \"last-10-avg\": 0.9324356345067774\n    },\n    \"time_this_iter_s\": {\n      \"max\": 14.142532110214233,\n      \"min\": 14.142532110214233,\n      \"avg\": 14.142532110214233,\n      \"last\": 14.142532110214233,\n      \"last-5-avg\": 14.142532110214233,\n      \"last-10-avg\": 14.142532110214233\n    },\n    \"done\": {\n      \"max\": false,\n      \"min\": false,\n      \"avg\": false,\n      \"last\": false,\n      \"last-5-avg\": false,\n      \"last-10-avg\": false\n    },\n    \"training_iteration\": {\n      \"max\": 1,\n      \"min\": 1,\n      \"avg\": 1,\n      \"last\": 1,\n      \"last-5-avg\": 1,\n      \"last-10-avg\": 1\n    },\n    \"time_total_s\": {\n      \"max\": 14.142532110214233,\n      \"min\": 14.142532110214233,\n      \"avg\": 14.142532110214233,\n      \"last\": 14.142532110214233,\n      \"last-5-avg\": 14.142532110214233,\n      \"last-10-avg\": 14.142532110214233\n    },\n    \"time_since_restore\": {\n      \"max\": 14.142532110214233,\n      \"min\": 14.142532110214233,\n      \"avg\": 14.142532110214233,\n      \"last\": 14.142532110214233,\n      \"last-5-avg\": 14.142532110214233,\n      \"last-10-avg\": 14.142532110214233\n    },\n    \"iterations_since_restore\": {\n      \"max\": 1,\n      \"min\": 1,\n      \"avg\": 1,\n      \"last\": 1,\n      \"last-5-avg\": 1,\n      \"last-10-avg\": 1\n    }\n  },\n  \"n_steps\": [\n    5,\n    10\n  ],\n  \"metric_n_steps\": {\n    \"score\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059589000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452948c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02663894898887945294284b038c013c944e4e4e4affffffff4affffffff4b0074946243089d9c7a4183d6ed3f9486945294612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059589000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452948c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02663894898887945294284b038c013c944e4e4e4affffffff4affffffff4b0074946243089d9c7a4183d6ed3f9486945294612e\"\n      }\n    },\n    \"time_this_iter_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529447402c48f9f8000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529447402c48f9f8000000612e\"\n      }\n    },\n    \"done\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059522000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529489612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059522000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529489612e\"\n      }\n    },\n    \"training_iteration\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b01612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b01612e\"\n      }\n    },\n    \"time_total_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529447402c48f9f8000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529447402c48f9f8000000612e\"\n      }\n    },\n    \"time_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529447402c48f9f8000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529447402c48f9f8000000612e\"\n      }\n    },\n    \"iterations_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b01612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b01612e\"\n      }\n    }\n  },\n  \"export_formats\": [],\n  \"status\": \"TERMINATED\",\n  \"start_time\": 1737596444.4239948,\n  \"relative_logdir\": \"lambda_119b9542_41_Gradient_LR=0.0324,Gradient_Momentum=0.9850,Gradient_Steps=310,Threshold=0,merge_threshold=17,number_of_points=_2025-01-23_02-40-44\",\n  \"runner\": null,\n  \"last_debug\": 0,\n  \"error_filename\": null,\n  \"pickled_error_filename\": null,\n  \"trial_name_creator\": null,\n  \"trial_dirname_creator\": null,\n  \"custom_trial_name\": null,\n  \"custom_dirname\": null,\n  \"experiment_dir_name\": \"test\",\n  \"saving_to\": null,\n  \"checkpoint_config\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"800595ff000000000000008c0e7261792e6169722e636f6e666967948c10436865636b706f696e74436f6e6669679493942981947d94288c0b6e756d5f746f5f6b656570944b038c1a636865636b706f696e745f73636f72655f617474726962757465948c12747261696e696e675f697465726174696f6e948c16636865636b706f696e745f73636f72655f6f72646572948c036d6178948c14636865636b706f696e745f6672657175656e6379944b008c11636865636b706f696e745f61745f656e64944e8c1a5f636865636b706f696e745f6b6565705f616c6c5f72616e6b7394898c1f5f636865636b706f696e745f75706c6f61645f66726f6d5f776f726b657273948975622e\"\n  },\n  \"checkpoint_manager\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"800595d2020000000000008c257261792e74756e652e657865637574696f6e2e636865636b706f696e745f6d616e61676572948c125f436865636b706f696e744d616e616765729493942981947d94288c145f636865636b706f696e745f7374726174656779948c0e7261792e6169722e636f6e666967948c10436865636b706f696e74436f6e6669679493942981947d94288c0b6e756d5f746f5f6b656570944b038c1a636865636b706f696e745f73636f72655f617474726962757465948c12747261696e696e675f697465726174696f6e948c16636865636b706f696e745f73636f72655f6f72646572948c036d6178948c14636865636b706f696e745f6672657175656e6379944b008c11636865636b706f696e745f61745f656e64944e8c1a5f636865636b706f696e745f6b6565705f616c6c5f72616e6b7394898c1f5f636865636b706f696e745f75706c6f61645f66726f6d5f776f726b657273948975628c155f6c61746573745f636865636b706f696e745f6964944b008c1a5f746f705f7065727369737465645f636865636b706f696e7473945d948c1a5f626573745f7065727369737465645f636865636b706f696e74944e8c1c5f6c61746573745f7065727369737465645f636865636b706f696e74944e8c195f6c61746573745f6d656d6f72795f636865636b706f696e74948c247261792e6169722e5f696e7465726e616c2e636865636b706f696e745f6d616e61676572948c125f547261636b6564436865636b706f696e749493942981947d94288c0b6469725f6f725f64617461944e8c026964944affffffff8c0c73746f726167655f6d6f646594681a8c11436865636b706f696e7453746f726167659493944b01859452948c0472616e6b944b008c076d657472696373947d948c076e6f64655f6970944e8c155f6d6f76655f696e73746561645f6f665f636f7079948875628c185f636865636b706f696e74735f746f5f636c65616e5f7570948f9475622e\"\n  },\n  \"restore_path\": null,\n  \"restoring_from\": null,\n  \"num_failures\": 0,\n  \"num_restore_failures\": 0,\n  \"results\": \"80054e2e\",\n  \"best_result\": \"80054e2e\",\n  \"param_config\": \"80054e2e\",\n  \"extra_arg\": \"80054e2e\",\n  \"_state_json\": null,\n  \"_state_valid\": false,\n  \"_resources\": \"80054e2e\"\n}",
    "{\n  \"stub\": false,\n  \"trainable_name\": \"lambda\",\n  \"trial_id\": \"57b03138\",\n  \"sync_config\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059593000000000000008c0f7261792e74756e652e73796e636572948c0a53796e63436f6e6669679493942981947d94288c0a75706c6f61645f646972944e8c0673796e636572948c046175746f948c0b73796e635f706572696f64944d2c018c0c73796e635f74696d656f7574944d08078c0e73796e635f61727469666163747394888c1273796e635f6f6e5f636865636b706f696e74948875622e\"\n  },\n  \"_orig_experiment_path\": \"/Users/kuba/ijcai_ski/ray_results/test\",\n  \"_orig_experiment_dir_name\": \"test\",\n  \"_local_experiment_path\": \"/Users/kuba/ijcai_ski/ray_results/test\",\n  \"_remote_experiment_path\": null,\n  \"config\": {\n    \"Gradient_LR\": 0.01769997410158375,\n    \"Gradient_Momentum\": 0.9483993914317915,\n    \"Gradient_Steps\": 120,\n    \"Threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308050000000000000094869452942e\"\n    },\n    \"merge_threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308110000000000000094869452942e\"\n    },\n    \"number_of_points\": 100,\n    \"outliers_multiplier\": 4.709236057098326\n  },\n  \"_Trial__unresolved_config\": {\n    \"Gradient_LR\": 0.01769997410158375,\n    \"Gradient_Momentum\": 0.9483993914317915,\n    \"Gradient_Steps\": 120,\n    \"Threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308050000000000000094869452942e\"\n    },\n    \"merge_threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308110000000000000094869452942e\"\n    },\n    \"number_of_points\": 100,\n    \"outliers_multiplier\": 4.709236057098326\n  },\n  \"evaluated_params\": {\n    \"Gradient_LR\": 0.01769997410158375,\n    \"Gradient_Momentum\": 0.9483993914317915,\n    \"Gradient_Steps\": 120,\n    \"Threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308050000000000000094869452942e\"\n    },\n    \"merge_threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308110000000000000094869452942e\"\n    },\n    \"number_of_points\": 100,\n    \"outliers_multiplier\": 4.709236057098326\n  },\n  \"experiment_tag\": \"43_Gradient_LR=0.0177,Gradient_Momentum=0.9484,Gradient_Steps=120,Threshold=5,merge_threshold=17,number_of_points=100,outliers_multiplier=4.7092\",\n  \"location\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059546000000000000008c197261792e74756e652e6578706572696d656e742e747269616c948c095f4c6f636174696f6e9493942981947d94288c08686f73746e616d65944e8c03706964944e75622e\"\n  },\n  \"stopping_criterion\": {},\n  \"_setup_default_resource\": true,\n  \"_default_placement_group_factory\": \"800595aa000000000000008c237261792e74756e652e657865637574696f6e2e706c6163656d656e745f67726f757073948c15506c6163656d656e7447726f7570466163746f72799493942981947d94288c085f62756e646c6573945d947d948c0343505594473ff000000000000073618c155f686561645f62756e646c655f69735f656d70747994898c095f7374726174656779948c045041434b948c055f6172677394298c075f6b7761726773947d9475622e\",\n  \"placement_group_factory\": \"800595aa000000000000008c237261792e74756e652e657865637574696f6e2e706c6163656d656e745f67726f757073948c15506c6163656d656e7447726f7570466163746f72799493942981947d94288c085f62756e646c6573945d947d948c0343505594473ff000000000000073618c155f686561645f62756e646c655f69735f656d70747994898c095f7374726174656779948c045041434b948c055f6172677394298c075f6b7761726773947d9475622e\",\n  \"log_to_file\": [\n    null,\n    null\n  ],\n  \"max_failures\": 0,\n  \"_last_result\": {\n    \"score\": 0.8967923384894003,\n    \"time_this_iter_s\": 9.71113133430481,\n    \"done\": true,\n    \"training_iteration\": 1,\n    \"trial_id\": \"57b03138\",\n    \"date\": \"2025-01-23_02-40-59\",\n    \"timestamp\": 1737596459,\n    \"time_total_s\": 9.71113133430481,\n    \"pid\": 30525,\n    \"hostname\": \"MacBook-Pro.ds19.agh.edu.pl\",\n    \"node_ip\": \"127.0.0.1\",\n    \"config\": {\n      \"Gradient_LR\": 0.01769997410158375,\n      \"Gradient_Momentum\": 0.9483993914317915,\n      \"Gradient_Steps\": 120,\n      \"Threshold\": 5,\n      \"merge_threshold\": 17,\n      \"number_of_points\": 100,\n      \"outliers_multiplier\": 4.709236057098326\n    },\n    \"time_since_restore\": 9.71113133430481,\n    \"iterations_since_restore\": 1,\n    \"experiment_tag\": \"43_Gradient_LR=0.0177,Gradient_Momentum=0.9484,Gradient_Steps=120,Threshold=5,merge_threshold=17,number_of_points=100,outliers_multiplier=4.7092\"\n  },\n  \"_default_result_or_future\": null,\n  \"last_update_time\": 1737596459.503554,\n  \"metric_analysis\": {\n    \"score\": {\n      \"max\": 0.8967923384894003,\n      \"min\": 0.8967923384894003,\n      \"avg\": 0.8967923384894003,\n      \"last\": 0.8967923384894003,\n      \"last-5-avg\": 0.8967923384894003,\n      \"last-10-avg\": 0.8967923384894003\n    },\n    \"time_this_iter_s\": {\n      \"max\": 9.71113133430481,\n      \"min\": 9.71113133430481,\n      \"avg\": 9.71113133430481,\n      \"last\": 9.71113133430481,\n      \"last-5-avg\": 9.71113133430481,\n      \"last-10-avg\": 9.71113133430481\n    },\n    \"done\": {\n      \"max\": false,\n      \"min\": false,\n      \"avg\": false,\n      \"last\": false,\n      \"last-5-avg\": false,\n      \"last-10-avg\": false\n    },\n    \"training_iteration\": {\n      \"max\": 1,\n      \"min\": 1,\n      \"avg\": 1,\n      \"last\": 1,\n      \"last-5-avg\": 1,\n      \"last-10-avg\": 1\n    },\n    \"time_total_s\": {\n      \"max\": 9.71113133430481,\n      \"min\": 9.71113133430481,\n      \"avg\": 9.71113133430481,\n      \"last\": 9.71113133430481,\n      \"last-5-avg\": 9.71113133430481,\n      \"last-10-avg\": 9.71113133430481\n    },\n    \"time_since_restore\": {\n      \"max\": 9.71113133430481,\n      \"min\": 9.71113133430481,\n      \"avg\": 9.71113133430481,\n      \"last\": 9.71113133430481,\n      \"last-5-avg\": 9.71113133430481,\n      \"last-10-avg\": 9.71113133430481\n    },\n    \"iterations_since_restore\": {\n      \"max\": 1,\n      \"min\": 1,\n      \"avg\": 1,\n      \"last\": 1,\n      \"last-5-avg\": 1,\n      \"last-10-avg\": 1\n    }\n  },\n  \"n_steps\": [\n    5,\n    10\n  ],\n  \"metric_n_steps\": {\n    \"score\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059589000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452948c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02663894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308d6b0a3d885b2ec3f9486945294612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059589000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452948c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02663894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308d6b0a3d885b2ec3f9486945294612e\"\n      }\n    },\n    \"time_this_iter_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944740236c1968000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944740236c1968000000612e\"\n      }\n    },\n    \"done\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059522000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529489612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059522000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529489612e\"\n      }\n    },\n    \"training_iteration\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b01612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b01612e\"\n      }\n    },\n    \"time_total_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944740236c1968000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944740236c1968000000612e\"\n      }\n    },\n    \"time_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944740236c1968000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944740236c1968000000612e\"\n      }\n    },\n    \"iterations_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b01612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b01612e\"\n      }\n    }\n  },\n  \"export_formats\": [],\n  \"status\": \"TERMINATED\",\n  \"start_time\": 1737596449.786908,\n  \"relative_logdir\": \"lambda_57b03138_43_Gradient_LR=0.0177,Gradient_Momentum=0.9484,Gradient_Steps=120,Threshold=5,merge_threshold=17,number_of_points=_2025-01-23_02-40-44\",\n  \"runner\": null,\n  \"last_debug\": 0,\n  \"error_filename\": null,\n  \"pickled_error_filename\": null,\n  \"trial_name_creator\": null,\n  \"trial_dirname_creator\": null,\n  \"custom_trial_name\": null,\n  \"custom_dirname\": null,\n  \"experiment_dir_name\": \"test\",\n  \"saving_to\": null,\n  \"checkpoint_config\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"800595ff000000000000008c0e7261792e6169722e636f6e666967948c10436865636b706f696e74436f6e6669679493942981947d94288c0b6e756d5f746f5f6b656570944b038c1a636865636b706f696e745f73636f72655f617474726962757465948c12747261696e696e675f697465726174696f6e948c16636865636b706f696e745f73636f72655f6f72646572948c036d6178948c14636865636b706f696e745f6672657175656e6379944b008c11636865636b706f696e745f61745f656e64944e8c1a5f636865636b706f696e745f6b6565705f616c6c5f72616e6b7394898c1f5f636865636b706f696e745f75706c6f61645f66726f6d5f776f726b657273948975622e\"\n  },\n  \"checkpoint_manager\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"800595d2020000000000008c257261792e74756e652e657865637574696f6e2e636865636b706f696e745f6d616e61676572948c125f436865636b706f696e744d616e616765729493942981947d94288c145f636865636b706f696e745f7374726174656779948c0e7261792e6169722e636f6e666967948c10436865636b706f696e74436f6e6669679493942981947d94288c0b6e756d5f746f5f6b656570944b038c1a636865636b706f696e745f73636f72655f617474726962757465948c12747261696e696e675f697465726174696f6e948c16636865636b706f696e745f73636f72655f6f72646572948c036d6178948c14636865636b706f696e745f6672657175656e6379944b008c11636865636b706f696e745f61745f656e64944e8c1a5f636865636b706f696e745f6b6565705f616c6c5f72616e6b7394898c1f5f636865636b706f696e745f75706c6f61645f66726f6d5f776f726b657273948975628c155f6c61746573745f636865636b706f696e745f6964944b008c1a5f746f705f7065727369737465645f636865636b706f696e7473945d948c1a5f626573745f7065727369737465645f636865636b706f696e74944e8c1c5f6c61746573745f7065727369737465645f636865636b706f696e74944e8c195f6c61746573745f6d656d6f72795f636865636b706f696e74948c247261792e6169722e5f696e7465726e616c2e636865636b706f696e745f6d616e61676572948c125f547261636b6564436865636b706f696e749493942981947d94288c0b6469725f6f725f64617461944e8c026964944affffffff8c0c73746f726167655f6d6f646594681a8c11436865636b706f696e7453746f726167659493944b01859452948c0472616e6b944b008c076d657472696373947d948c076e6f64655f6970944e8c155f6d6f76655f696e73746561645f6f665f636f7079948875628c185f636865636b706f696e74735f746f5f636c65616e5f7570948f9475622e\"\n  },\n  \"restore_path\": null,\n  \"restoring_from\": null,\n  \"num_failures\": 0,\n  \"num_restore_failures\": 0,\n  \"results\": \"80054e2e\",\n  \"best_result\": \"80054e2e\",\n  \"param_config\": \"80054e2e\",\n  \"extra_arg\": \"80054e2e\",\n  \"_state_json\": null,\n  \"_state_valid\": false,\n  \"_resources\": \"80054e2e\"\n}",
    "{\n  \"stub\": false,\n  \"trainable_name\": \"lambda\",\n  \"trial_id\": \"2cfc1a93\",\n  \"sync_config\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059593000000000000008c0f7261792e74756e652e73796e636572948c0a53796e63436f6e6669679493942981947d94288c0a75706c6f61645f646972944e8c0673796e636572948c046175746f948c0b73796e635f706572696f64944d2c018c0c73796e635f74696d656f7574944d08078c0e73796e635f61727469666163747394888c1273796e635f6f6e5f636865636b706f696e74948875622e\"\n  },\n  \"_orig_experiment_path\": \"/Users/kuba/ijcai_ski/ray_results/test\",\n  \"_orig_experiment_dir_name\": \"test\",\n  \"_local_experiment_path\": \"/Users/kuba/ijcai_ski/ray_results/test\",\n  \"_remote_experiment_path\": null,\n  \"config\": {\n    \"Gradient_LR\": 0.02985416464431841,\n    \"Gradient_Momentum\": 0.9748166563657911,\n    \"Gradient_Steps\": 400,\n    \"Threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308060000000000000094869452942e\"\n    },\n    \"merge_threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308060000000000000094869452942e\"\n    },\n    \"number_of_points\": 300,\n    \"outliers_multiplier\": 4.505958987322922\n  },\n  \"_Trial__unresolved_config\": {\n    \"Gradient_LR\": 0.02985416464431841,\n    \"Gradient_Momentum\": 0.9748166563657911,\n    \"Gradient_Steps\": 400,\n    \"Threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308060000000000000094869452942e\"\n    },\n    \"merge_threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308060000000000000094869452942e\"\n    },\n    \"number_of_points\": 300,\n    \"outliers_multiplier\": 4.505958987322922\n  },\n  \"evaluated_params\": {\n    \"Gradient_LR\": 0.02985416464431841,\n    \"Gradient_Momentum\": 0.9748166563657911,\n    \"Gradient_Steps\": 400,\n    \"Threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308060000000000000094869452942e\"\n    },\n    \"merge_threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308060000000000000094869452942e\"\n    },\n    \"number_of_points\": 300,\n    \"outliers_multiplier\": 4.505958987322922\n  },\n  \"experiment_tag\": \"45_Gradient_LR=0.0299,Gradient_Momentum=0.9748,Gradient_Steps=400,Threshold=6,merge_threshold=6,number_of_points=300,outliers_multiplier=4.5060\",\n  \"location\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059546000000000000008c197261792e74756e652e6578706572696d656e742e747269616c948c095f4c6f636174696f6e9493942981947d94288c08686f73746e616d65944e8c03706964944e75622e\"\n  },\n  \"stopping_criterion\": {},\n  \"_setup_default_resource\": true,\n  \"_default_placement_group_factory\": \"800595aa000000000000008c237261792e74756e652e657865637574696f6e2e706c6163656d656e745f67726f757073948c15506c6163656d656e7447726f7570466163746f72799493942981947d94288c085f62756e646c6573945d947d948c0343505594473ff000000000000073618c155f686561645f62756e646c655f69735f656d70747994898c095f7374726174656779948c045041434b948c055f6172677394298c075f6b7761726773947d9475622e\",\n  \"placement_group_factory\": \"800595aa000000000000008c237261792e74756e652e657865637574696f6e2e706c6163656d656e745f67726f757073948c15506c6163656d656e7447726f7570466163746f72799493942981947d94288c085f62756e646c6573945d947d948c0343505594473ff000000000000073618c155f686561645f62756e646c655f69735f656d70747994898c095f7374726174656779948c045041434b948c055f6172677394298c075f6b7761726773947d9475622e\",\n  \"log_to_file\": [\n    null,\n    null\n  ],\n  \"max_failures\": 0,\n  \"_last_result\": {\n    \"score\": 0.8783791895291868,\n    \"time_this_iter_s\": 13.313441038131714,\n    \"done\": true,\n    \"training_iteration\": 1,\n    \"trial_id\": \"2cfc1a93\",\n    \"date\": \"2025-01-23_02-41-06\",\n    \"timestamp\": 1737596466,\n    \"time_total_s\": 13.313441038131714,\n    \"pid\": 30487,\n    \"hostname\": \"MacBook-Pro.ds19.agh.edu.pl\",\n    \"node_ip\": \"127.0.0.1\",\n    \"config\": {\n      \"Gradient_LR\": 0.02985416464431841,\n      \"Gradient_Momentum\": 0.9748166563657911,\n      \"Gradient_Steps\": 400,\n      \"Threshold\": 6,\n      \"merge_threshold\": 6,\n      \"number_of_points\": 300,\n      \"outliers_multiplier\": 4.505958987322922\n    },\n    \"time_since_restore\": 13.313441038131714,\n    \"iterations_since_restore\": 1,\n    \"experiment_tag\": \"45_Gradient_LR=0.0299,Gradient_Momentum=0.9748,Gradient_Steps=400,Threshold=6,merge_threshold=6,number_of_points=300,outliers_multiplier=4.5060\"\n  },\n  \"_default_result_or_future\": null,\n  \"last_update_time\": 1737596466.604461,\n  \"metric_analysis\": {\n    \"score\": {\n      \"max\": 0.8783791895291868,\n      \"min\": 0.8783791895291868,\n      \"avg\": 0.8783791895291868,\n      \"last\": 0.8783791895291868,\n      \"last-5-avg\": 0.8783791895291868,\n      \"last-10-avg\": 0.8783791895291868\n    },\n    \"time_this_iter_s\": {\n      \"max\": 13.313441038131714,\n      \"min\": 13.313441038131714,\n      \"avg\": 13.313441038131714,\n      \"last\": 13.313441038131714,\n      \"last-5-avg\": 13.313441038131714,\n      \"last-10-avg\": 13.313441038131714\n    },\n    \"done\": {\n      \"max\": false,\n      \"min\": false,\n      \"avg\": false,\n      \"last\": false,\n      \"last-5-avg\": false,\n      \"last-10-avg\": false\n    },\n    \"training_iteration\": {\n      \"max\": 1,\n      \"min\": 1,\n      \"avg\": 1,\n      \"last\": 1,\n      \"last-5-avg\": 1,\n      \"last-10-avg\": 1\n    },\n    \"time_total_s\": {\n      \"max\": 13.313441038131714,\n      \"min\": 13.313441038131714,\n      \"avg\": 13.313441038131714,\n      \"last\": 13.313441038131714,\n      \"last-5-avg\": 13.313441038131714,\n      \"last-10-avg\": 13.313441038131714\n    },\n    \"time_since_restore\": {\n      \"max\": 13.313441038131714,\n      \"min\": 13.313441038131714,\n      \"avg\": 13.313441038131714,\n      \"last\": 13.313441038131714,\n      \"last-5-avg\": 13.313441038131714,\n      \"last-10-avg\": 13.313441038131714\n    },\n    \"iterations_since_restore\": {\n      \"max\": 1,\n      \"min\": 1,\n      \"avg\": 1,\n      \"last\": 1,\n      \"last-5-avg\": 1,\n      \"last-10-avg\": 1\n    }\n  },\n  \"n_steps\": [\n    5,\n    10\n  ],\n  \"metric_n_steps\": {\n    \"score\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059589000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452948c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02663894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308987990acae1bec3f9486945294612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059589000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452948c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02663894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308987990acae1bec3f9486945294612e\"\n      }\n    },\n    \"time_this_iter_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529447402aa07b58000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529447402aa07b58000000612e\"\n      }\n    },\n    \"done\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059522000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529489612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059522000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529489612e\"\n      }\n    },\n    \"training_iteration\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b01612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b01612e\"\n      }\n    },\n    \"time_total_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529447402aa07b58000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529447402aa07b58000000612e\"\n      }\n    },\n    \"time_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529447402aa07b58000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529447402aa07b58000000612e\"\n      }\n    },\n    \"iterations_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b01612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b01612e\"\n      }\n    }\n  },\n  \"export_formats\": [],\n  \"status\": \"TERMINATED\",\n  \"start_time\": 1737596453.245114,\n  \"relative_logdir\": \"lambda_2cfc1a93_45_Gradient_LR=0.0299,Gradient_Momentum=0.9748,Gradient_Steps=400,Threshold=6,merge_threshold=6,number_of_points=3_2025-01-23_02-40-51\",\n  \"runner\": null,\n  \"last_debug\": 0,\n  \"error_filename\": null,\n  \"pickled_error_filename\": null,\n  \"trial_name_creator\": null,\n  \"trial_dirname_creator\": null,\n  \"custom_trial_name\": null,\n  \"custom_dirname\": null,\n  \"experiment_dir_name\": \"test\",\n  \"saving_to\": null,\n  \"checkpoint_config\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"800595ff000000000000008c0e7261792e6169722e636f6e666967948c10436865636b706f696e74436f6e6669679493942981947d94288c0b6e756d5f746f5f6b656570944b038c1a636865636b706f696e745f73636f72655f617474726962757465948c12747261696e696e675f697465726174696f6e948c16636865636b706f696e745f73636f72655f6f72646572948c036d6178948c14636865636b706f696e745f6672657175656e6379944b008c11636865636b706f696e745f61745f656e64944e8c1a5f636865636b706f696e745f6b6565705f616c6c5f72616e6b7394898c1f5f636865636b706f696e745f75706c6f61645f66726f6d5f776f726b657273948975622e\"\n  },\n  \"checkpoint_manager\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"800595d2020000000000008c257261792e74756e652e657865637574696f6e2e636865636b706f696e745f6d616e61676572948c125f436865636b706f696e744d616e616765729493942981947d94288c145f636865636b706f696e745f7374726174656779948c0e7261792e6169722e636f6e666967948c10436865636b706f696e74436f6e6669679493942981947d94288c0b6e756d5f746f5f6b656570944b038c1a636865636b706f696e745f73636f72655f617474726962757465948c12747261696e696e675f697465726174696f6e948c16636865636b706f696e745f73636f72655f6f72646572948c036d6178948c14636865636b706f696e745f6672657175656e6379944b008c11636865636b706f696e745f61745f656e64944e8c1a5f636865636b706f696e745f6b6565705f616c6c5f72616e6b7394898c1f5f636865636b706f696e745f75706c6f61645f66726f6d5f776f726b657273948975628c155f6c61746573745f636865636b706f696e745f6964944b008c1a5f746f705f7065727369737465645f636865636b706f696e7473945d948c1a5f626573745f7065727369737465645f636865636b706f696e74944e8c1c5f6c61746573745f7065727369737465645f636865636b706f696e74944e8c195f6c61746573745f6d656d6f72795f636865636b706f696e74948c247261792e6169722e5f696e7465726e616c2e636865636b706f696e745f6d616e61676572948c125f547261636b6564436865636b706f696e749493942981947d94288c0b6469725f6f725f64617461944e8c026964944affffffff8c0c73746f726167655f6d6f646594681a8c11436865636b706f696e7453746f726167659493944b01859452948c0472616e6b944b008c076d657472696373947d948c076e6f64655f6970944e8c155f6d6f76655f696e73746561645f6f665f636f7079948875628c185f636865636b706f696e74735f746f5f636c65616e5f7570948f9475622e\"\n  },\n  \"restore_path\": null,\n  \"restoring_from\": null,\n  \"num_failures\": 0,\n  \"num_restore_failures\": 0,\n  \"results\": \"80054e2e\",\n  \"best_result\": \"80054e2e\",\n  \"param_config\": \"80054e2e\",\n  \"extra_arg\": \"80054e2e\",\n  \"_state_json\": null,\n  \"_state_valid\": false,\n  \"_resources\": \"80054e2e\"\n}",
    "{\n  \"stub\": false,\n  \"trainable_name\": \"lambda\",\n  \"trial_id\": \"707f2de1\",\n  \"sync_config\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059593000000000000008c0f7261792e74756e652e73796e636572948c0a53796e63436f6e6669679493942981947d94288c0a75706c6f61645f646972944e8c0673796e636572948c046175746f948c0b73796e635f706572696f64944d2c018c0c73796e635f74696d656f7574944d08078c0e73796e635f61727469666163747394888c1273796e635f6f6e5f636865636b706f696e74948875622e\"\n  },\n  \"_orig_experiment_path\": \"/Users/kuba/ijcai_ski/ray_results/test\",\n  \"_orig_experiment_dir_name\": \"test\",\n  \"_local_experiment_path\": \"/Users/kuba/ijcai_ski/ray_results/test\",\n  \"_remote_experiment_path\": null,\n  \"config\": {\n    \"Gradient_LR\": 0.012481732045105961,\n    \"Gradient_Momentum\": 0.9921682926291174,\n    \"Gradient_Steps\": 140,\n    \"Threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308090000000000000094869452942e\"\n    },\n    \"merge_threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308100000000000000094869452942e\"\n    },\n    \"number_of_points\": 100,\n    \"outliers_multiplier\": 4.395892248820274\n  },\n  \"_Trial__unresolved_config\": {\n    \"Gradient_LR\": 0.012481732045105961,\n    \"Gradient_Momentum\": 0.9921682926291174,\n    \"Gradient_Steps\": 140,\n    \"Threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308090000000000000094869452942e\"\n    },\n    \"merge_threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308100000000000000094869452942e\"\n    },\n    \"number_of_points\": 100,\n    \"outliers_multiplier\": 4.395892248820274\n  },\n  \"evaluated_params\": {\n    \"Gradient_LR\": 0.012481732045105961,\n    \"Gradient_Momentum\": 0.9921682926291174,\n    \"Gradient_Steps\": 140,\n    \"Threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308090000000000000094869452942e\"\n    },\n    \"merge_threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308100000000000000094869452942e\"\n    },\n    \"number_of_points\": 100,\n    \"outliers_multiplier\": 4.395892248820274\n  },\n  \"experiment_tag\": \"44_Gradient_LR=0.0125,Gradient_Momentum=0.9922,Gradient_Steps=140,Threshold=9,merge_threshold=16,number_of_points=100,outliers_multiplier=4.3959\",\n  \"location\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059546000000000000008c197261792e74756e652e6578706572696d656e742e747269616c948c095f4c6f636174696f6e9493942981947d94288c08686f73746e616d65944e8c03706964944e75622e\"\n  },\n  \"stopping_criterion\": {},\n  \"_setup_default_resource\": true,\n  \"_default_placement_group_factory\": \"800595aa000000000000008c237261792e74756e652e657865637574696f6e2e706c6163656d656e745f67726f757073948c15506c6163656d656e7447726f7570466163746f72799493942981947d94288c085f62756e646c6573945d947d948c0343505594473ff000000000000073618c155f686561645f62756e646c655f69735f656d70747994898c095f7374726174656779948c045041434b948c055f6172677394298c075f6b7761726773947d9475622e\",\n  \"placement_group_factory\": \"800595aa000000000000008c237261792e74756e652e657865637574696f6e2e706c6163656d656e745f67726f757073948c15506c6163656d656e7447726f7570466163746f72799493942981947d94288c085f62756e646c6573945d947d948c0343505594473ff000000000000073618c155f686561645f62756e646c655f69735f656d70747994898c095f7374726174656779948c045041434b948c055f6172677394298c075f6b7761726773947d9475622e\",\n  \"log_to_file\": [\n    null,\n    null\n  ],\n  \"max_failures\": 0,\n  \"_last_result\": {\n    \"score\": 0.7277472012832125,\n    \"time_this_iter_s\": 9.662177085876465,\n    \"done\": true,\n    \"training_iteration\": 1,\n    \"trial_id\": \"707f2de1\",\n    \"date\": \"2025-01-23_02-41-01\",\n    \"timestamp\": 1737596461,\n    \"time_total_s\": 9.662177085876465,\n    \"pid\": 30619,\n    \"hostname\": \"MacBook-Pro.ds19.agh.edu.pl\",\n    \"node_ip\": \"127.0.0.1\",\n    \"config\": {\n      \"Gradient_LR\": 0.012481732045105961,\n      \"Gradient_Momentum\": 0.9921682926291174,\n      \"Gradient_Steps\": 140,\n      \"Threshold\": 9,\n      \"merge_threshold\": 16,\n      \"number_of_points\": 100,\n      \"outliers_multiplier\": 4.395892248820274\n    },\n    \"time_since_restore\": 9.662177085876465,\n    \"iterations_since_restore\": 1,\n    \"experiment_tag\": \"44_Gradient_LR=0.0125,Gradient_Momentum=0.9922,Gradient_Steps=140,Threshold=9,merge_threshold=16,number_of_points=100,outliers_multiplier=4.3959\"\n  },\n  \"_default_result_or_future\": null,\n  \"last_update_time\": 1737596461.3187158,\n  \"metric_analysis\": {\n    \"score\": {\n      \"max\": 0.7277472012832125,\n      \"min\": 0.7277472012832125,\n      \"avg\": 0.7277472012832125,\n      \"last\": 0.7277472012832125,\n      \"last-5-avg\": 0.7277472012832125,\n      \"last-10-avg\": 0.7277472012832125\n    },\n    \"time_this_iter_s\": {\n      \"max\": 9.662177085876465,\n      \"min\": 9.662177085876465,\n      \"avg\": 9.662177085876465,\n      \"last\": 9.662177085876465,\n      \"last-5-avg\": 9.662177085876465,\n      \"last-10-avg\": 9.662177085876465\n    },\n    \"done\": {\n      \"max\": false,\n      \"min\": false,\n      \"avg\": false,\n      \"last\": false,\n      \"last-5-avg\": false,\n      \"last-10-avg\": false\n    },\n    \"training_iteration\": {\n      \"max\": 1,\n      \"min\": 1,\n      \"avg\": 1,\n      \"last\": 1,\n      \"last-5-avg\": 1,\n      \"last-10-avg\": 1\n    },\n    \"time_total_s\": {\n      \"max\": 9.662177085876465,\n      \"min\": 9.662177085876465,\n      \"avg\": 9.662177085876465,\n      \"last\": 9.662177085876465,\n      \"last-5-avg\": 9.662177085876465,\n      \"last-10-avg\": 9.662177085876465\n    },\n    \"time_since_restore\": {\n      \"max\": 9.662177085876465,\n      \"min\": 9.662177085876465,\n      \"avg\": 9.662177085876465,\n      \"last\": 9.662177085876465,\n      \"last-5-avg\": 9.662177085876465,\n      \"last-10-avg\": 9.662177085876465\n    },\n    \"iterations_since_restore\": {\n      \"max\": 1,\n      \"min\": 1,\n      \"avg\": 1,\n      \"last\": 1,\n      \"last-5-avg\": 1,\n      \"last-10-avg\": 1\n    }\n  },\n  \"n_steps\": [\n    5,\n    10\n  ],\n  \"metric_n_steps\": {\n    \"score\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059589000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452948c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02663894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308aa8aa87fb449e73f9486945294612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059589000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452948c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02663894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308aa8aa87fb449e73f9486945294612e\"\n      }\n    },\n    \"time_this_iter_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944740235308e0000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944740235308e0000000612e\"\n      }\n    },\n    \"done\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059522000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529489612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059522000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529489612e\"\n      }\n    },\n    \"training_iteration\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b01612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b01612e\"\n      }\n    },\n    \"time_total_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944740235308e0000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944740235308e0000000612e\"\n      }\n    },\n    \"time_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944740235308e0000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944740235308e0000000612e\"\n      }\n    },\n    \"iterations_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b01612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b01612e\"\n      }\n    }\n  },\n  \"export_formats\": [],\n  \"status\": \"TERMINATED\",\n  \"start_time\": 1737596451.650798,\n  \"relative_logdir\": \"lambda_707f2de1_44_Gradient_LR=0.0125,Gradient_Momentum=0.9922,Gradient_Steps=140,Threshold=9,merge_threshold=16,number_of_points=_2025-01-23_02-40-49\",\n  \"runner\": null,\n  \"last_debug\": 0,\n  \"error_filename\": null,\n  \"pickled_error_filename\": null,\n  \"trial_name_creator\": null,\n  \"trial_dirname_creator\": null,\n  \"custom_trial_name\": null,\n  \"custom_dirname\": null,\n  \"experiment_dir_name\": \"test\",\n  \"saving_to\": null,\n  \"checkpoint_config\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"800595ff000000000000008c0e7261792e6169722e636f6e666967948c10436865636b706f696e74436f6e6669679493942981947d94288c0b6e756d5f746f5f6b656570944b038c1a636865636b706f696e745f73636f72655f617474726962757465948c12747261696e696e675f697465726174696f6e948c16636865636b706f696e745f73636f72655f6f72646572948c036d6178948c14636865636b706f696e745f6672657175656e6379944b008c11636865636b706f696e745f61745f656e64944e8c1a5f636865636b706f696e745f6b6565705f616c6c5f72616e6b7394898c1f5f636865636b706f696e745f75706c6f61645f66726f6d5f776f726b657273948975622e\"\n  },\n  \"checkpoint_manager\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"800595d2020000000000008c257261792e74756e652e657865637574696f6e2e636865636b706f696e745f6d616e61676572948c125f436865636b706f696e744d616e616765729493942981947d94288c145f636865636b706f696e745f7374726174656779948c0e7261792e6169722e636f6e666967948c10436865636b706f696e74436f6e6669679493942981947d94288c0b6e756d5f746f5f6b656570944b038c1a636865636b706f696e745f73636f72655f617474726962757465948c12747261696e696e675f697465726174696f6e948c16636865636b706f696e745f73636f72655f6f72646572948c036d6178948c14636865636b706f696e745f6672657175656e6379944b008c11636865636b706f696e745f61745f656e64944e8c1a5f636865636b706f696e745f6b6565705f616c6c5f72616e6b7394898c1f5f636865636b706f696e745f75706c6f61645f66726f6d5f776f726b657273948975628c155f6c61746573745f636865636b706f696e745f6964944b008c1a5f746f705f7065727369737465645f636865636b706f696e7473945d948c1a5f626573745f7065727369737465645f636865636b706f696e74944e8c1c5f6c61746573745f7065727369737465645f636865636b706f696e74944e8c195f6c61746573745f6d656d6f72795f636865636b706f696e74948c247261792e6169722e5f696e7465726e616c2e636865636b706f696e745f6d616e61676572948c125f547261636b6564436865636b706f696e749493942981947d94288c0b6469725f6f725f64617461944e8c026964944affffffff8c0c73746f726167655f6d6f646594681a8c11436865636b706f696e7453746f726167659493944b01859452948c0472616e6b944b008c076d657472696373947d948c076e6f64655f6970944e8c155f6d6f76655f696e73746561645f6f665f636f7079948875628c185f636865636b706f696e74735f746f5f636c65616e5f7570948f9475622e\"\n  },\n  \"restore_path\": null,\n  \"restoring_from\": null,\n  \"num_failures\": 0,\n  \"num_restore_failures\": 0,\n  \"results\": \"80054e2e\",\n  \"best_result\": \"80054e2e\",\n  \"param_config\": \"80054e2e\",\n  \"extra_arg\": \"80054e2e\",\n  \"_state_json\": null,\n  \"_state_valid\": false,\n  \"_resources\": \"80054e2e\"\n}",
    "{\n  \"stub\": false,\n  \"trainable_name\": \"lambda\",\n  \"trial_id\": \"607264ab\",\n  \"sync_config\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059593000000000000008c0f7261792e74756e652e73796e636572948c0a53796e63436f6e6669679493942981947d94288c0a75706c6f61645f646972944e8c0673796e636572948c046175746f948c0b73796e635f706572696f64944d2c018c0c73796e635f74696d656f7574944d08078c0e73796e635f61727469666163747394888c1273796e635f6f6e5f636865636b706f696e74948875622e\"\n  },\n  \"_orig_experiment_path\": \"/Users/kuba/ijcai_ski/ray_results/test\",\n  \"_orig_experiment_dir_name\": \"test\",\n  \"_local_experiment_path\": \"/Users/kuba/ijcai_ski/ray_results/test\",\n  \"_remote_experiment_path\": null,\n  \"config\": {\n    \"Gradient_LR\": 0.010034921237293357,\n    \"Gradient_Momentum\": 0.9830610984409517,\n    \"Gradient_Steps\": 470,\n    \"Threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308120000000000000094869452942e\"\n    },\n    \"merge_threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b0074946243080b0000000000000094869452942e\"\n    },\n    \"number_of_points\": 400,\n    \"outliers_multiplier\": 3.9302261386124484\n  },\n  \"_Trial__unresolved_config\": {\n    \"Gradient_LR\": 0.010034921237293357,\n    \"Gradient_Momentum\": 0.9830610984409517,\n    \"Gradient_Steps\": 470,\n    \"Threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308120000000000000094869452942e\"\n    },\n    \"merge_threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b0074946243080b0000000000000094869452942e\"\n    },\n    \"number_of_points\": 400,\n    \"outliers_multiplier\": 3.9302261386124484\n  },\n  \"evaluated_params\": {\n    \"Gradient_LR\": 0.010034921237293357,\n    \"Gradient_Momentum\": 0.9830610984409517,\n    \"Gradient_Steps\": 470,\n    \"Threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308120000000000000094869452942e\"\n    },\n    \"merge_threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b0074946243080b0000000000000094869452942e\"\n    },\n    \"number_of_points\": 400,\n    \"outliers_multiplier\": 3.9302261386124484\n  },\n  \"experiment_tag\": \"46_Gradient_LR=0.0100,Gradient_Momentum=0.9831,Gradient_Steps=470,Threshold=18,merge_threshold=11,number_of_points=400,outliers_multiplier=3.9302\",\n  \"location\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059546000000000000008c197261792e74756e652e6578706572696d656e742e747269616c948c095f4c6f636174696f6e9493942981947d94288c08686f73746e616d65944e8c03706964944e75622e\"\n  },\n  \"stopping_criterion\": {},\n  \"_setup_default_resource\": true,\n  \"_default_placement_group_factory\": \"800595aa000000000000008c237261792e74756e652e657865637574696f6e2e706c6163656d656e745f67726f757073948c15506c6163656d656e7447726f7570466163746f72799493942981947d94288c085f62756e646c6573945d947d948c0343505594473ff000000000000073618c155f686561645f62756e646c655f69735f656d70747994898c095f7374726174656779948c045041434b948c055f6172677394298c075f6b7761726773947d9475622e\",\n  \"placement_group_factory\": \"800595aa000000000000008c237261792e74756e652e657865637574696f6e2e706c6163656d656e745f67726f757073948c15506c6163656d656e7447726f7570466163746f72799493942981947d94288c085f62756e646c6573945d947d948c0343505594473ff000000000000073618c155f686561645f62756e646c655f69735f656d70747994898c095f7374726174656779948c045041434b948c055f6172677394298c075f6b7761726773947d9475622e\",\n  \"log_to_file\": [\n    null,\n    null\n  ],\n  \"max_failures\": 0,\n  \"_last_result\": {\n    \"score\": 0.8318844387375609,\n    \"time_this_iter_s\": 13.952874183654785,\n    \"done\": true,\n    \"training_iteration\": 1,\n    \"trial_id\": \"607264ab\",\n    \"date\": \"2025-01-23_02-41-07\",\n    \"timestamp\": 1737596467,\n    \"time_total_s\": 13.952874183654785,\n    \"pid\": 30685,\n    \"hostname\": \"MacBook-Pro.ds19.agh.edu.pl\",\n    \"node_ip\": \"127.0.0.1\",\n    \"config\": {\n      \"Gradient_LR\": 0.010034921237293357,\n      \"Gradient_Momentum\": 0.9830610984409517,\n      \"Gradient_Steps\": 470,\n      \"Threshold\": 18,\n      \"merge_threshold\": 11,\n      \"number_of_points\": 400,\n      \"outliers_multiplier\": 3.9302261386124484\n    },\n    \"time_since_restore\": 13.952874183654785,\n    \"iterations_since_restore\": 1,\n    \"experiment_tag\": \"46_Gradient_LR=0.0100,Gradient_Momentum=0.9831,Gradient_Steps=470,Threshold=18,merge_threshold=11,number_of_points=400,outliers_multiplier=3.9302\"\n  },\n  \"_default_result_or_future\": null,\n  \"last_update_time\": 1737596467.5648482,\n  \"metric_analysis\": {\n    \"score\": {\n      \"max\": 0.8318844387375609,\n      \"min\": 0.8318844387375609,\n      \"avg\": 0.8318844387375609,\n      \"last\": 0.8318844387375609,\n      \"last-5-avg\": 0.8318844387375609,\n      \"last-10-avg\": 0.8318844387375609\n    },\n    \"time_this_iter_s\": {\n      \"max\": 13.952874183654785,\n      \"min\": 13.952874183654785,\n      \"avg\": 13.952874183654785,\n      \"last\": 13.952874183654785,\n      \"last-5-avg\": 13.952874183654785,\n      \"last-10-avg\": 13.952874183654785\n    },\n    \"done\": {\n      \"max\": false,\n      \"min\": false,\n      \"avg\": false,\n      \"last\": false,\n      \"last-5-avg\": false,\n      \"last-10-avg\": false\n    },\n    \"training_iteration\": {\n      \"max\": 1,\n      \"min\": 1,\n      \"avg\": 1,\n      \"last\": 1,\n      \"last-5-avg\": 1,\n      \"last-10-avg\": 1\n    },\n    \"time_total_s\": {\n      \"max\": 13.952874183654785,\n      \"min\": 13.952874183654785,\n      \"avg\": 13.952874183654785,\n      \"last\": 13.952874183654785,\n      \"last-5-avg\": 13.952874183654785,\n      \"last-10-avg\": 13.952874183654785\n    },\n    \"time_since_restore\": {\n      \"max\": 13.952874183654785,\n      \"min\": 13.952874183654785,\n      \"avg\": 13.952874183654785,\n      \"last\": 13.952874183654785,\n      \"last-5-avg\": 13.952874183654785,\n      \"last-10-avg\": 13.952874183654785\n    },\n    \"iterations_since_restore\": {\n      \"max\": 1,\n      \"min\": 1,\n      \"avg\": 1,\n      \"last\": 1,\n      \"last-5-avg\": 1,\n      \"last-10-avg\": 1\n    }\n  },\n  \"n_steps\": [\n    5,\n    10\n  ],\n  \"metric_n_steps\": {\n    \"score\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059589000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452948c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02663894898887945294284b038c013c944e4e4e4affffffff4affffffff4b00749462430883bb4d1dcc9eea3f9486945294612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059589000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452948c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02663894898887945294284b038c013c944e4e4e4affffffff4affffffff4b00749462430883bb4d1dcc9eea3f9486945294612e\"\n      }\n    },\n    \"time_this_iter_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529447402be7df20000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529447402be7df20000000612e\"\n      }\n    },\n    \"done\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059522000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529489612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059522000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529489612e\"\n      }\n    },\n    \"training_iteration\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b01612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b01612e\"\n      }\n    },\n    \"time_total_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529447402be7df20000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529447402be7df20000000612e\"\n      }\n    },\n    \"time_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529447402be7df20000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529447402be7df20000000612e\"\n      }\n    },\n    \"iterations_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b01612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b01612e\"\n      }\n    }\n  },\n  \"export_formats\": [],\n  \"status\": \"TERMINATED\",\n  \"start_time\": 1737596453.607104,\n  \"relative_logdir\": \"lambda_607264ab_46_Gradient_LR=0.0100,Gradient_Momentum=0.9831,Gradient_Steps=470,Threshold=18,merge_threshold=11,number_of_points_2025-01-23_02-40-53\",\n  \"runner\": null,\n  \"last_debug\": 0,\n  \"error_filename\": null,\n  \"pickled_error_filename\": null,\n  \"trial_name_creator\": null,\n  \"trial_dirname_creator\": null,\n  \"custom_trial_name\": null,\n  \"custom_dirname\": null,\n  \"experiment_dir_name\": \"test\",\n  \"saving_to\": null,\n  \"checkpoint_config\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"800595ff000000000000008c0e7261792e6169722e636f6e666967948c10436865636b706f696e74436f6e6669679493942981947d94288c0b6e756d5f746f5f6b656570944b038c1a636865636b706f696e745f73636f72655f617474726962757465948c12747261696e696e675f697465726174696f6e948c16636865636b706f696e745f73636f72655f6f72646572948c036d6178948c14636865636b706f696e745f6672657175656e6379944b008c11636865636b706f696e745f61745f656e64944e8c1a5f636865636b706f696e745f6b6565705f616c6c5f72616e6b7394898c1f5f636865636b706f696e745f75706c6f61645f66726f6d5f776f726b657273948975622e\"\n  },\n  \"checkpoint_manager\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"800595d2020000000000008c257261792e74756e652e657865637574696f6e2e636865636b706f696e745f6d616e61676572948c125f436865636b706f696e744d616e616765729493942981947d94288c145f636865636b706f696e745f7374726174656779948c0e7261792e6169722e636f6e666967948c10436865636b706f696e74436f6e6669679493942981947d94288c0b6e756d5f746f5f6b656570944b038c1a636865636b706f696e745f73636f72655f617474726962757465948c12747261696e696e675f697465726174696f6e948c16636865636b706f696e745f73636f72655f6f72646572948c036d6178948c14636865636b706f696e745f6672657175656e6379944b008c11636865636b706f696e745f61745f656e64944e8c1a5f636865636b706f696e745f6b6565705f616c6c5f72616e6b7394898c1f5f636865636b706f696e745f75706c6f61645f66726f6d5f776f726b657273948975628c155f6c61746573745f636865636b706f696e745f6964944b008c1a5f746f705f7065727369737465645f636865636b706f696e7473945d948c1a5f626573745f7065727369737465645f636865636b706f696e74944e8c1c5f6c61746573745f7065727369737465645f636865636b706f696e74944e8c195f6c61746573745f6d656d6f72795f636865636b706f696e74948c247261792e6169722e5f696e7465726e616c2e636865636b706f696e745f6d616e61676572948c125f547261636b6564436865636b706f696e749493942981947d94288c0b6469725f6f725f64617461944e8c026964944affffffff8c0c73746f726167655f6d6f646594681a8c11436865636b706f696e7453746f726167659493944b01859452948c0472616e6b944b008c076d657472696373947d948c076e6f64655f6970944e8c155f6d6f76655f696e73746561645f6f665f636f7079948875628c185f636865636b706f696e74735f746f5f636c65616e5f7570948f9475622e\"\n  },\n  \"restore_path\": null,\n  \"restoring_from\": null,\n  \"num_failures\": 0,\n  \"num_restore_failures\": 0,\n  \"results\": \"80054e2e\",\n  \"best_result\": \"80054e2e\",\n  \"param_config\": \"80054e2e\",\n  \"extra_arg\": \"80054e2e\",\n  \"_state_json\": null,\n  \"_state_valid\": false,\n  \"_resources\": \"80054e2e\"\n}",
    "{\n  \"stub\": false,\n  \"trainable_name\": \"lambda\",\n  \"trial_id\": \"36ec64c0\",\n  \"sync_config\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059593000000000000008c0f7261792e74756e652e73796e636572948c0a53796e63436f6e6669679493942981947d94288c0a75706c6f61645f646972944e8c0673796e636572948c046175746f948c0b73796e635f706572696f64944d2c018c0c73796e635f74696d656f7574944d08078c0e73796e635f61727469666163747394888c1273796e635f6f6e5f636865636b706f696e74948875622e\"\n  },\n  \"_orig_experiment_path\": \"/Users/kuba/ijcai_ski/ray_results/test\",\n  \"_orig_experiment_dir_name\": \"test\",\n  \"_local_experiment_path\": \"/Users/kuba/ijcai_ski/ray_results/test\",\n  \"_remote_experiment_path\": null,\n  \"config\": {\n    \"Gradient_LR\": 0.014008783986335855,\n    \"Gradient_Momentum\": 0.915885280450373,\n    \"Gradient_Steps\": 320,\n    \"Threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308040000000000000094869452942e\"\n    },\n    \"merge_threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308050000000000000094869452942e\"\n    },\n    \"number_of_points\": 100,\n    \"outliers_multiplier\": 4.106558037819749\n  },\n  \"_Trial__unresolved_config\": {\n    \"Gradient_LR\": 0.014008783986335855,\n    \"Gradient_Momentum\": 0.915885280450373,\n    \"Gradient_Steps\": 320,\n    \"Threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308040000000000000094869452942e\"\n    },\n    \"merge_threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308050000000000000094869452942e\"\n    },\n    \"number_of_points\": 100,\n    \"outliers_multiplier\": 4.106558037819749\n  },\n  \"evaluated_params\": {\n    \"Gradient_LR\": 0.014008783986335855,\n    \"Gradient_Momentum\": 0.915885280450373,\n    \"Gradient_Steps\": 320,\n    \"Threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308040000000000000094869452942e\"\n    },\n    \"merge_threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308050000000000000094869452942e\"\n    },\n    \"number_of_points\": 100,\n    \"outliers_multiplier\": 4.106558037819749\n  },\n  \"experiment_tag\": \"47_Gradient_LR=0.0140,Gradient_Momentum=0.9159,Gradient_Steps=320,Threshold=4,merge_threshold=5,number_of_points=100,outliers_multiplier=4.1066\",\n  \"location\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059546000000000000008c197261792e74756e652e6578706572696d656e742e747269616c948c095f4c6f636174696f6e9493942981947d94288c08686f73746e616d65944e8c03706964944e75622e\"\n  },\n  \"stopping_criterion\": {},\n  \"_setup_default_resource\": true,\n  \"_default_placement_group_factory\": \"800595aa000000000000008c237261792e74756e652e657865637574696f6e2e706c6163656d656e745f67726f757073948c15506c6163656d656e7447726f7570466163746f72799493942981947d94288c085f62756e646c6573945d947d948c0343505594473ff000000000000073618c155f686561645f62756e646c655f69735f656d70747994898c095f7374726174656779948c045041434b948c055f6172677394298c075f6b7761726773947d9475622e\",\n  \"placement_group_factory\": \"800595aa000000000000008c237261792e74756e652e657865637574696f6e2e706c6163656d656e745f67726f757073948c15506c6163656d656e7447726f7570466163746f72799493942981947d94288c085f62756e646c6573945d947d948c0343505594473ff000000000000073618c155f686561645f62756e646c655f69735f656d70747994898c095f7374726174656779948c045041434b948c055f6172677394298c075f6b7761726773947d9475622e\",\n  \"log_to_file\": [\n    null,\n    null\n  ],\n  \"max_failures\": 0,\n  \"_last_result\": {\n    \"score\": 0.8880407830298125,\n    \"time_this_iter_s\": 11.609307050704956,\n    \"done\": true,\n    \"training_iteration\": 1,\n    \"trial_id\": \"36ec64c0\",\n    \"date\": \"2025-01-23_02-41-06\",\n    \"timestamp\": 1737596466,\n    \"time_total_s\": 11.609307050704956,\n    \"pid\": 30590,\n    \"hostname\": \"MacBook-Pro.ds19.agh.edu.pl\",\n    \"node_ip\": \"127.0.0.1\",\n    \"config\": {\n      \"Gradient_LR\": 0.014008783986335855,\n      \"Gradient_Momentum\": 0.915885280450373,\n      \"Gradient_Steps\": 320,\n      \"Threshold\": 4,\n      \"merge_threshold\": 5,\n      \"number_of_points\": 100,\n      \"outliers_multiplier\": 4.106558037819749\n    },\n    \"time_since_restore\": 11.609307050704956,\n    \"iterations_since_restore\": 1,\n    \"experiment_tag\": \"47_Gradient_LR=0.0140,Gradient_Momentum=0.9159,Gradient_Steps=320,Threshold=4,merge_threshold=5,number_of_points=100,outliers_multiplier=4.1066\"\n  },\n  \"_default_result_or_future\": null,\n  \"last_update_time\": 1737596466.9404318,\n  \"metric_analysis\": {\n    \"score\": {\n      \"max\": 0.8880407830298125,\n      \"min\": 0.8880407830298125,\n      \"avg\": 0.8880407830298125,\n      \"last\": 0.8880407830298125,\n      \"last-5-avg\": 0.8880407830298125,\n      \"last-10-avg\": 0.8880407830298125\n    },\n    \"time_this_iter_s\": {\n      \"max\": 11.609307050704956,\n      \"min\": 11.609307050704956,\n      \"avg\": 11.609307050704956,\n      \"last\": 11.609307050704956,\n      \"last-5-avg\": 11.609307050704956,\n      \"last-10-avg\": 11.609307050704956\n    },\n    \"done\": {\n      \"max\": false,\n      \"min\": false,\n      \"avg\": false,\n      \"last\": false,\n      \"last-5-avg\": false,\n      \"last-10-avg\": false\n    },\n    \"training_iteration\": {\n      \"max\": 1,\n      \"min\": 1,\n      \"avg\": 1,\n      \"last\": 1,\n      \"last-5-avg\": 1,\n      \"last-10-avg\": 1\n    },\n    \"time_total_s\": {\n      \"max\": 11.609307050704956,\n      \"min\": 11.609307050704956,\n      \"avg\": 11.609307050704956,\n      \"last\": 11.609307050704956,\n      \"last-5-avg\": 11.609307050704956,\n      \"last-10-avg\": 11.609307050704956\n    },\n    \"time_since_restore\": {\n      \"max\": 11.609307050704956,\n      \"min\": 11.609307050704956,\n      \"avg\": 11.609307050704956,\n      \"last\": 11.609307050704956,\n      \"last-5-avg\": 11.609307050704956,\n      \"last-10-avg\": 11.609307050704956\n    },\n    \"iterations_since_restore\": {\n      \"max\": 1,\n      \"min\": 1,\n      \"avg\": 1,\n      \"last\": 1,\n      \"last-5-avg\": 1,\n      \"last-10-avg\": 1\n    }\n  },\n  \"n_steps\": [\n    5,\n    10\n  ],\n  \"metric_n_steps\": {\n    \"score\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059589000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452948c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02663894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308a6121481d46aec3f9486945294612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059589000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452948c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02663894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308a6121481d46aec3f9486945294612e\"\n      }\n    },\n    \"time_this_iter_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529447402737f718000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529447402737f718000000612e\"\n      }\n    },\n    \"done\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059522000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529489612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059522000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529489612e\"\n      }\n    },\n    \"training_iteration\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b01612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b01612e\"\n      }\n    },\n    \"time_total_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529447402737f718000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529447402737f718000000612e\"\n      }\n    },\n    \"time_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529447402737f718000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529447402737f718000000612e\"\n      }\n    },\n    \"iterations_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b01612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b01612e\"\n      }\n    }\n  },\n  \"export_formats\": [],\n  \"status\": \"TERMINATED\",\n  \"start_time\": 1737596455.31571,\n  \"relative_logdir\": \"lambda_36ec64c0_47_Gradient_LR=0.0140,Gradient_Momentum=0.9159,Gradient_Steps=320,Threshold=4,merge_threshold=5,number_of_points=1_2025-01-23_02-40-53\",\n  \"runner\": null,\n  \"last_debug\": 0,\n  \"error_filename\": null,\n  \"pickled_error_filename\": null,\n  \"trial_name_creator\": null,\n  \"trial_dirname_creator\": null,\n  \"custom_trial_name\": null,\n  \"custom_dirname\": null,\n  \"experiment_dir_name\": \"test\",\n  \"saving_to\": null,\n  \"checkpoint_config\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"800595ff000000000000008c0e7261792e6169722e636f6e666967948c10436865636b706f696e74436f6e6669679493942981947d94288c0b6e756d5f746f5f6b656570944b038c1a636865636b706f696e745f73636f72655f617474726962757465948c12747261696e696e675f697465726174696f6e948c16636865636b706f696e745f73636f72655f6f72646572948c036d6178948c14636865636b706f696e745f6672657175656e6379944b008c11636865636b706f696e745f61745f656e64944e8c1a5f636865636b706f696e745f6b6565705f616c6c5f72616e6b7394898c1f5f636865636b706f696e745f75706c6f61645f66726f6d5f776f726b657273948975622e\"\n  },\n  \"checkpoint_manager\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"800595d2020000000000008c257261792e74756e652e657865637574696f6e2e636865636b706f696e745f6d616e61676572948c125f436865636b706f696e744d616e616765729493942981947d94288c145f636865636b706f696e745f7374726174656779948c0e7261792e6169722e636f6e666967948c10436865636b706f696e74436f6e6669679493942981947d94288c0b6e756d5f746f5f6b656570944b038c1a636865636b706f696e745f73636f72655f617474726962757465948c12747261696e696e675f697465726174696f6e948c16636865636b706f696e745f73636f72655f6f72646572948c036d6178948c14636865636b706f696e745f6672657175656e6379944b008c11636865636b706f696e745f61745f656e64944e8c1a5f636865636b706f696e745f6b6565705f616c6c5f72616e6b7394898c1f5f636865636b706f696e745f75706c6f61645f66726f6d5f776f726b657273948975628c155f6c61746573745f636865636b706f696e745f6964944b008c1a5f746f705f7065727369737465645f636865636b706f696e7473945d948c1a5f626573745f7065727369737465645f636865636b706f696e74944e8c1c5f6c61746573745f7065727369737465645f636865636b706f696e74944e8c195f6c61746573745f6d656d6f72795f636865636b706f696e74948c247261792e6169722e5f696e7465726e616c2e636865636b706f696e745f6d616e61676572948c125f547261636b6564436865636b706f696e749493942981947d94288c0b6469725f6f725f64617461944e8c026964944affffffff8c0c73746f726167655f6d6f646594681a8c11436865636b706f696e7453746f726167659493944b01859452948c0472616e6b944b008c076d657472696373947d948c076e6f64655f6970944e8c155f6d6f76655f696e73746561645f6f665f636f7079948875628c185f636865636b706f696e74735f746f5f636c65616e5f7570948f9475622e\"\n  },\n  \"restore_path\": null,\n  \"restoring_from\": null,\n  \"num_failures\": 0,\n  \"num_restore_failures\": 0,\n  \"results\": \"80054e2e\",\n  \"best_result\": \"80054e2e\",\n  \"param_config\": \"80054e2e\",\n  \"extra_arg\": \"80054e2e\",\n  \"_state_json\": null,\n  \"_state_valid\": false,\n  \"_resources\": \"80054e2e\"\n}",
    "{\n  \"stub\": false,\n  \"trainable_name\": \"lambda\",\n  \"trial_id\": \"fe52331a\",\n  \"sync_config\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059593000000000000008c0f7261792e74756e652e73796e636572948c0a53796e63436f6e6669679493942981947d94288c0a75706c6f61645f646972944e8c0673796e636572948c046175746f948c0b73796e635f706572696f64944d2c018c0c73796e635f74696d656f7574944d08078c0e73796e635f61727469666163747394888c1273796e635f6f6e5f636865636b706f696e74948875622e\"\n  },\n  \"_orig_experiment_path\": \"/Users/kuba/ijcai_ski/ray_results/test\",\n  \"_orig_experiment_dir_name\": \"test\",\n  \"_local_experiment_path\": \"/Users/kuba/ijcai_ski/ray_results/test\",\n  \"_remote_experiment_path\": null,\n  \"config\": {\n    \"Gradient_LR\": 0.05408112908530112,\n    \"Gradient_Momentum\": 0.9065501048205011,\n    \"Gradient_Steps\": 250,\n    \"Threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308130000000000000094869452942e\"\n    },\n    \"merge_threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b0074946243080c0000000000000094869452942e\"\n    },\n    \"number_of_points\": 800,\n    \"outliers_multiplier\": 4.191016936035208\n  },\n  \"_Trial__unresolved_config\": {\n    \"Gradient_LR\": 0.05408112908530112,\n    \"Gradient_Momentum\": 0.9065501048205011,\n    \"Gradient_Steps\": 250,\n    \"Threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308130000000000000094869452942e\"\n    },\n    \"merge_threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b0074946243080c0000000000000094869452942e\"\n    },\n    \"number_of_points\": 800,\n    \"outliers_multiplier\": 4.191016936035208\n  },\n  \"evaluated_params\": {\n    \"Gradient_LR\": 0.05408112908530112,\n    \"Gradient_Momentum\": 0.9065501048205011,\n    \"Gradient_Steps\": 250,\n    \"Threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308130000000000000094869452942e\"\n    },\n    \"merge_threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b0074946243080c0000000000000094869452942e\"\n    },\n    \"number_of_points\": 800,\n    \"outliers_multiplier\": 4.191016936035208\n  },\n  \"experiment_tag\": \"49_Gradient_LR=0.0541,Gradient_Momentum=0.9066,Gradient_Steps=250,Threshold=19,merge_threshold=12,number_of_points=800,outliers_multiplier=4.1910\",\n  \"location\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059546000000000000008c197261792e74756e652e6578706572696d656e742e747269616c948c095f4c6f636174696f6e9493942981947d94288c08686f73746e616d65944e8c03706964944e75622e\"\n  },\n  \"stopping_criterion\": {},\n  \"_setup_default_resource\": true,\n  \"_default_placement_group_factory\": \"800595aa000000000000008c237261792e74756e652e657865637574696f6e2e706c6163656d656e745f67726f757073948c15506c6163656d656e7447726f7570466163746f72799493942981947d94288c085f62756e646c6573945d947d948c0343505594473ff000000000000073618c155f686561645f62756e646c655f69735f656d70747994898c095f7374726174656779948c045041434b948c055f6172677394298c075f6b7761726773947d9475622e\",\n  \"placement_group_factory\": \"800595aa000000000000008c237261792e74756e652e657865637574696f6e2e706c6163656d656e745f67726f757073948c15506c6163656d656e7447726f7570466163746f72799493942981947d94288c085f62756e646c6573945d947d948c0343505594473ff000000000000073618c155f686561645f62756e646c655f69735f656d70747994898c095f7374726174656779948c045041434b948c055f6172677394298c075f6b7761726773947d9475622e\",\n  \"log_to_file\": [\n    null,\n    null\n  ],\n  \"max_failures\": 0,\n  \"_last_result\": {\n    \"score\": 0.8265663628570203,\n    \"time_this_iter_s\": 9.824345111846924,\n    \"done\": true,\n    \"training_iteration\": 1,\n    \"trial_id\": \"fe52331a\",\n    \"date\": \"2025-01-23_02-41-08\",\n    \"timestamp\": 1737596468,\n    \"time_total_s\": 9.824345111846924,\n    \"pid\": 30566,\n    \"hostname\": \"MacBook-Pro.ds19.agh.edu.pl\",\n    \"node_ip\": \"127.0.0.1\",\n    \"config\": {\n      \"Gradient_LR\": 0.05408112908530112,\n      \"Gradient_Momentum\": 0.9065501048205011,\n      \"Gradient_Steps\": 250,\n      \"Threshold\": 19,\n      \"merge_threshold\": 12,\n      \"number_of_points\": 800,\n      \"outliers_multiplier\": 4.191016936035208\n    },\n    \"time_since_restore\": 9.824345111846924,\n    \"iterations_since_restore\": 1,\n    \"experiment_tag\": \"49_Gradient_LR=0.0541,Gradient_Momentum=0.9066,Gradient_Steps=250,Threshold=19,merge_threshold=12,number_of_points=800,outliers_multiplier=4.1910\"\n  },\n  \"_default_result_or_future\": null,\n  \"last_update_time\": 1737596468.4092631,\n  \"metric_analysis\": {\n    \"score\": {\n      \"max\": 0.8265663628570203,\n      \"min\": 0.8265663628570203,\n      \"avg\": 0.8265663628570203,\n      \"last\": 0.8265663628570203,\n      \"last-5-avg\": 0.8265663628570203,\n      \"last-10-avg\": 0.8265663628570203\n    },\n    \"time_this_iter_s\": {\n      \"max\": 9.824345111846924,\n      \"min\": 9.824345111846924,\n      \"avg\": 9.824345111846924,\n      \"last\": 9.824345111846924,\n      \"last-5-avg\": 9.824345111846924,\n      \"last-10-avg\": 9.824345111846924\n    },\n    \"done\": {\n      \"max\": false,\n      \"min\": false,\n      \"avg\": false,\n      \"last\": false,\n      \"last-5-avg\": false,\n      \"last-10-avg\": false\n    },\n    \"training_iteration\": {\n      \"max\": 1,\n      \"min\": 1,\n      \"avg\": 1,\n      \"last\": 1,\n      \"last-5-avg\": 1,\n      \"last-10-avg\": 1\n    },\n    \"time_total_s\": {\n      \"max\": 9.824345111846924,\n      \"min\": 9.824345111846924,\n      \"avg\": 9.824345111846924,\n      \"last\": 9.824345111846924,\n      \"last-5-avg\": 9.824345111846924,\n      \"last-10-avg\": 9.824345111846924\n    },\n    \"time_since_restore\": {\n      \"max\": 9.824345111846924,\n      \"min\": 9.824345111846924,\n      \"avg\": 9.824345111846924,\n      \"last\": 9.824345111846924,\n      \"last-5-avg\": 9.824345111846924,\n      \"last-10-avg\": 9.824345111846924\n    },\n    \"iterations_since_restore\": {\n      \"max\": 1,\n      \"min\": 1,\n      \"avg\": 1,\n      \"last\": 1,\n      \"last-5-avg\": 1,\n      \"last-10-avg\": 1\n    }\n  },\n  \"n_steps\": [\n    5,\n    10\n  ],\n  \"metric_n_steps\": {\n    \"score\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059589000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452948c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02663894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308ee390e4d3b73ea3f9486945294612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059589000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452948c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02663894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308ee390e4d3b73ea3f9486945294612e\"\n      }\n    },\n    \"time_this_iter_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294474023a61090000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294474023a61090000000612e\"\n      }\n    },\n    \"done\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059522000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529489612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059522000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529489612e\"\n      }\n    },\n    \"training_iteration\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b01612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b01612e\"\n      }\n    },\n    \"time_total_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294474023a61090000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294474023a61090000000612e\"\n      }\n    },\n    \"time_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294474023a61090000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294474023a61090000000612e\"\n      }\n    },\n    \"iterations_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b01612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b01612e\"\n      }\n    }\n  },\n  \"export_formats\": [],\n  \"status\": \"TERMINATED\",\n  \"start_time\": 1737596458.581464,\n  \"relative_logdir\": \"lambda_fe52331a_49_Gradient_LR=0.0541,Gradient_Momentum=0.9066,Gradient_Steps=250,Threshold=19,merge_threshold=12,number_of_points_2025-01-23_02-40-57\",\n  \"runner\": null,\n  \"last_debug\": 0,\n  \"error_filename\": null,\n  \"pickled_error_filename\": null,\n  \"trial_name_creator\": null,\n  \"trial_dirname_creator\": null,\n  \"custom_trial_name\": null,\n  \"custom_dirname\": null,\n  \"experiment_dir_name\": \"test\",\n  \"saving_to\": null,\n  \"checkpoint_config\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"800595ff000000000000008c0e7261792e6169722e636f6e666967948c10436865636b706f696e74436f6e6669679493942981947d94288c0b6e756d5f746f5f6b656570944b038c1a636865636b706f696e745f73636f72655f617474726962757465948c12747261696e696e675f697465726174696f6e948c16636865636b706f696e745f73636f72655f6f72646572948c036d6178948c14636865636b706f696e745f6672657175656e6379944b008c11636865636b706f696e745f61745f656e64944e8c1a5f636865636b706f696e745f6b6565705f616c6c5f72616e6b7394898c1f5f636865636b706f696e745f75706c6f61645f66726f6d5f776f726b657273948975622e\"\n  },\n  \"checkpoint_manager\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"800595d2020000000000008c257261792e74756e652e657865637574696f6e2e636865636b706f696e745f6d616e61676572948c125f436865636b706f696e744d616e616765729493942981947d94288c145f636865636b706f696e745f7374726174656779948c0e7261792e6169722e636f6e666967948c10436865636b706f696e74436f6e6669679493942981947d94288c0b6e756d5f746f5f6b656570944b038c1a636865636b706f696e745f73636f72655f617474726962757465948c12747261696e696e675f697465726174696f6e948c16636865636b706f696e745f73636f72655f6f72646572948c036d6178948c14636865636b706f696e745f6672657175656e6379944b008c11636865636b706f696e745f61745f656e64944e8c1a5f636865636b706f696e745f6b6565705f616c6c5f72616e6b7394898c1f5f636865636b706f696e745f75706c6f61645f66726f6d5f776f726b657273948975628c155f6c61746573745f636865636b706f696e745f6964944b008c1a5f746f705f7065727369737465645f636865636b706f696e7473945d948c1a5f626573745f7065727369737465645f636865636b706f696e74944e8c1c5f6c61746573745f7065727369737465645f636865636b706f696e74944e8c195f6c61746573745f6d656d6f72795f636865636b706f696e74948c247261792e6169722e5f696e7465726e616c2e636865636b706f696e745f6d616e61676572948c125f547261636b6564436865636b706f696e749493942981947d94288c0b6469725f6f725f64617461944e8c026964944affffffff8c0c73746f726167655f6d6f646594681a8c11436865636b706f696e7453746f726167659493944b01859452948c0472616e6b944b008c076d657472696373947d948c076e6f64655f6970944e8c155f6d6f76655f696e73746561645f6f665f636f7079948875628c185f636865636b706f696e74735f746f5f636c65616e5f7570948f9475622e\"\n  },\n  \"restore_path\": null,\n  \"restoring_from\": null,\n  \"num_failures\": 0,\n  \"num_restore_failures\": 0,\n  \"results\": \"80054e2e\",\n  \"best_result\": \"80054e2e\",\n  \"param_config\": \"80054e2e\",\n  \"extra_arg\": \"80054e2e\",\n  \"_state_json\": null,\n  \"_state_valid\": false,\n  \"_resources\": \"80054e2e\"\n}",
    "{\n  \"stub\": false,\n  \"trainable_name\": \"lambda\",\n  \"trial_id\": \"1b0ddc0d\",\n  \"sync_config\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059593000000000000008c0f7261792e74756e652e73796e636572948c0a53796e63436f6e6669679493942981947d94288c0a75706c6f61645f646972944e8c0673796e636572948c046175746f948c0b73796e635f706572696f64944d2c018c0c73796e635f74696d656f7574944d08078c0e73796e635f61727469666163747394888c1273796e635f6f6e5f636865636b706f696e74948875622e\"\n  },\n  \"_orig_experiment_path\": \"/Users/kuba/ijcai_ski/ray_results/test\",\n  \"_orig_experiment_dir_name\": \"test\",\n  \"_local_experiment_path\": \"/Users/kuba/ijcai_ski/ray_results/test\",\n  \"_remote_experiment_path\": null,\n  \"config\": {\n    \"Gradient_LR\": 0.02771934664219768,\n    \"Gradient_Momentum\": 0.9552297935543095,\n    \"Gradient_Steps\": 460,\n    \"Threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308000000000000000094869452942e\"\n    },\n    \"merge_threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b0074946243080d0000000000000094869452942e\"\n    },\n    \"number_of_points\": 700,\n    \"outliers_multiplier\": 4.481238294854446\n  },\n  \"_Trial__unresolved_config\": {\n    \"Gradient_LR\": 0.02771934664219768,\n    \"Gradient_Momentum\": 0.9552297935543095,\n    \"Gradient_Steps\": 460,\n    \"Threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308000000000000000094869452942e\"\n    },\n    \"merge_threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b0074946243080d0000000000000094869452942e\"\n    },\n    \"number_of_points\": 700,\n    \"outliers_multiplier\": 4.481238294854446\n  },\n  \"evaluated_params\": {\n    \"Gradient_LR\": 0.02771934664219768,\n    \"Gradient_Momentum\": 0.9552297935543095,\n    \"Gradient_Steps\": 460,\n    \"Threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308000000000000000094869452942e\"\n    },\n    \"merge_threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b0074946243080d0000000000000094869452942e\"\n    },\n    \"number_of_points\": 700,\n    \"outliers_multiplier\": 4.481238294854446\n  },\n  \"experiment_tag\": \"50_Gradient_LR=0.0277,Gradient_Momentum=0.9552,Gradient_Steps=460,Threshold=0,merge_threshold=13,number_of_points=700,outliers_multiplier=4.4812\",\n  \"location\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059546000000000000008c197261792e74756e652e6578706572696d656e742e747269616c948c095f4c6f636174696f6e9493942981947d94288c08686f73746e616d65944e8c03706964944e75622e\"\n  },\n  \"stopping_criterion\": {},\n  \"_setup_default_resource\": true,\n  \"_default_placement_group_factory\": \"800595aa000000000000008c237261792e74756e652e657865637574696f6e2e706c6163656d656e745f67726f757073948c15506c6163656d656e7447726f7570466163746f72799493942981947d94288c085f62756e646c6573945d947d948c0343505594473ff000000000000073618c155f686561645f62756e646c655f69735f656d70747994898c095f7374726174656779948c045041434b948c055f6172677394298c075f6b7761726773947d9475622e\",\n  \"placement_group_factory\": \"800595aa000000000000008c237261792e74756e652e657865637574696f6e2e706c6163656d656e745f67726f757073948c15506c6163656d656e7447726f7570466163746f72799493942981947d94288c085f62756e646c6573945d947d948c0343505594473ff000000000000073618c155f686561645f62756e646c655f69735f656d70747994898c095f7374726174656779948c045041434b948c055f6172677394298c075f6b7761726773947d9475622e\",\n  \"log_to_file\": [\n    null,\n    null\n  ],\n  \"max_failures\": 0,\n  \"_last_result\": {\n    \"score\": 0.9260562723817666,\n    \"time_this_iter_s\": 12.354358911514282,\n    \"done\": true,\n    \"training_iteration\": 1,\n    \"trial_id\": \"1b0ddc0d\",\n    \"date\": \"2025-01-23_02-41-11\",\n    \"timestamp\": 1737596471,\n    \"time_total_s\": 12.354358911514282,\n    \"pid\": 30510,\n    \"hostname\": \"MacBook-Pro.ds19.agh.edu.pl\",\n    \"node_ip\": \"127.0.0.1\",\n    \"config\": {\n      \"Gradient_LR\": 0.02771934664219768,\n      \"Gradient_Momentum\": 0.9552297935543095,\n      \"Gradient_Steps\": 460,\n      \"Threshold\": 0,\n      \"merge_threshold\": 13,\n      \"number_of_points\": 700,\n      \"outliers_multiplier\": 4.481238294854446\n    },\n    \"time_since_restore\": 12.354358911514282,\n    \"iterations_since_restore\": 1,\n    \"experiment_tag\": \"50_Gradient_LR=0.0277,Gradient_Momentum=0.9552,Gradient_Steps=460,Threshold=0,merge_threshold=13,number_of_points=700,outliers_multiplier=4.4812\"\n  },\n  \"_default_result_or_future\": null,\n  \"last_update_time\": 1737596471.602015,\n  \"metric_analysis\": {\n    \"score\": {\n      \"max\": 0.9260562723817666,\n      \"min\": 0.9260562723817666,\n      \"avg\": 0.9260562723817666,\n      \"last\": 0.9260562723817666,\n      \"last-5-avg\": 0.9260562723817666,\n      \"last-10-avg\": 0.9260562723817666\n    },\n    \"time_this_iter_s\": {\n      \"max\": 12.354358911514282,\n      \"min\": 12.354358911514282,\n      \"avg\": 12.354358911514282,\n      \"last\": 12.354358911514282,\n      \"last-5-avg\": 12.354358911514282,\n      \"last-10-avg\": 12.354358911514282\n    },\n    \"done\": {\n      \"max\": false,\n      \"min\": false,\n      \"avg\": false,\n      \"last\": false,\n      \"last-5-avg\": false,\n      \"last-10-avg\": false\n    },\n    \"training_iteration\": {\n      \"max\": 1,\n      \"min\": 1,\n      \"avg\": 1,\n      \"last\": 1,\n      \"last-5-avg\": 1,\n      \"last-10-avg\": 1\n    },\n    \"time_total_s\": {\n      \"max\": 12.354358911514282,\n      \"min\": 12.354358911514282,\n      \"avg\": 12.354358911514282,\n      \"last\": 12.354358911514282,\n      \"last-5-avg\": 12.354358911514282,\n      \"last-10-avg\": 12.354358911514282\n    },\n    \"time_since_restore\": {\n      \"max\": 12.354358911514282,\n      \"min\": 12.354358911514282,\n      \"avg\": 12.354358911514282,\n      \"last\": 12.354358911514282,\n      \"last-5-avg\": 12.354358911514282,\n      \"last-10-avg\": 12.354358911514282\n    },\n    \"iterations_since_restore\": {\n      \"max\": 1,\n      \"min\": 1,\n      \"avg\": 1,\n      \"last\": 1,\n      \"last-5-avg\": 1,\n      \"last-10-avg\": 1\n    }\n  },\n  \"n_steps\": [\n    5,\n    10\n  ],\n  \"metric_n_steps\": {\n    \"score\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059589000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452948c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02663894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308d55484c340a2ed3f9486945294612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059589000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452948c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02663894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308d55484c340a2ed3f9486945294612e\"\n      }\n    },\n    \"time_this_iter_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294474028b56e88000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294474028b56e88000000612e\"\n      }\n    },\n    \"done\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059522000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529489612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059522000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529489612e\"\n      }\n    },\n    \"training_iteration\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b01612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b01612e\"\n      }\n    },\n    \"time_total_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294474028b56e88000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294474028b56e88000000612e\"\n      }\n    },\n    \"time_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294474028b56e88000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294474028b56e88000000612e\"\n      }\n    },\n    \"iterations_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b01612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b01612e\"\n      }\n    }\n  },\n  \"export_formats\": [],\n  \"status\": \"TERMINATED\",\n  \"start_time\": 1737596459.242989,\n  \"relative_logdir\": \"lambda_1b0ddc0d_50_Gradient_LR=0.0277,Gradient_Momentum=0.9552,Gradient_Steps=460,Threshold=0,merge_threshold=13,number_of_points=_2025-01-23_02-40-58\",\n  \"runner\": null,\n  \"last_debug\": 0,\n  \"error_filename\": null,\n  \"pickled_error_filename\": null,\n  \"trial_name_creator\": null,\n  \"trial_dirname_creator\": null,\n  \"custom_trial_name\": null,\n  \"custom_dirname\": null,\n  \"experiment_dir_name\": \"test\",\n  \"saving_to\": null,\n  \"checkpoint_config\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"800595ff000000000000008c0e7261792e6169722e636f6e666967948c10436865636b706f696e74436f6e6669679493942981947d94288c0b6e756d5f746f5f6b656570944b038c1a636865636b706f696e745f73636f72655f617474726962757465948c12747261696e696e675f697465726174696f6e948c16636865636b706f696e745f73636f72655f6f72646572948c036d6178948c14636865636b706f696e745f6672657175656e6379944b008c11636865636b706f696e745f61745f656e64944e8c1a5f636865636b706f696e745f6b6565705f616c6c5f72616e6b7394898c1f5f636865636b706f696e745f75706c6f61645f66726f6d5f776f726b657273948975622e\"\n  },\n  \"checkpoint_manager\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"800595d2020000000000008c257261792e74756e652e657865637574696f6e2e636865636b706f696e745f6d616e61676572948c125f436865636b706f696e744d616e616765729493942981947d94288c145f636865636b706f696e745f7374726174656779948c0e7261792e6169722e636f6e666967948c10436865636b706f696e74436f6e6669679493942981947d94288c0b6e756d5f746f5f6b656570944b038c1a636865636b706f696e745f73636f72655f617474726962757465948c12747261696e696e675f697465726174696f6e948c16636865636b706f696e745f73636f72655f6f72646572948c036d6178948c14636865636b706f696e745f6672657175656e6379944b008c11636865636b706f696e745f61745f656e64944e8c1a5f636865636b706f696e745f6b6565705f616c6c5f72616e6b7394898c1f5f636865636b706f696e745f75706c6f61645f66726f6d5f776f726b657273948975628c155f6c61746573745f636865636b706f696e745f6964944b008c1a5f746f705f7065727369737465645f636865636b706f696e7473945d948c1a5f626573745f7065727369737465645f636865636b706f696e74944e8c1c5f6c61746573745f7065727369737465645f636865636b706f696e74944e8c195f6c61746573745f6d656d6f72795f636865636b706f696e74948c247261792e6169722e5f696e7465726e616c2e636865636b706f696e745f6d616e61676572948c125f547261636b6564436865636b706f696e749493942981947d94288c0b6469725f6f725f64617461944e8c026964944affffffff8c0c73746f726167655f6d6f646594681a8c11436865636b706f696e7453746f726167659493944b01859452948c0472616e6b944b008c076d657472696373947d948c076e6f64655f6970944e8c155f6d6f76655f696e73746561645f6f665f636f7079948875628c185f636865636b706f696e74735f746f5f636c65616e5f7570948f9475622e\"\n  },\n  \"restore_path\": null,\n  \"restoring_from\": null,\n  \"num_failures\": 0,\n  \"num_restore_failures\": 0,\n  \"results\": \"80054e2e\",\n  \"best_result\": \"80054e2e\",\n  \"param_config\": \"80054e2e\",\n  \"extra_arg\": \"80054e2e\",\n  \"_state_json\": null,\n  \"_state_valid\": false,\n  \"_resources\": \"80054e2e\"\n}",
    "{\n  \"stub\": false,\n  \"trainable_name\": \"lambda\",\n  \"trial_id\": \"3c94bc07\",\n  \"sync_config\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059593000000000000008c0f7261792e74756e652e73796e636572948c0a53796e63436f6e6669679493942981947d94288c0a75706c6f61645f646972944e8c0673796e636572948c046175746f948c0b73796e635f706572696f64944d2c018c0c73796e635f74696d656f7574944d08078c0e73796e635f61727469666163747394888c1273796e635f6f6e5f636865636b706f696e74948875622e\"\n  },\n  \"_orig_experiment_path\": \"/Users/kuba/ijcai_ski/ray_results/test\",\n  \"_orig_experiment_dir_name\": \"test\",\n  \"_local_experiment_path\": \"/Users/kuba/ijcai_ski/ray_results/test\",\n  \"_remote_experiment_path\": null,\n  \"config\": {\n    \"Gradient_LR\": 0.022717216966610707,\n    \"Gradient_Momentum\": 0.9671953048831726,\n    \"Gradient_Steps\": 150,\n    \"Threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308080000000000000094869452942e\"\n    },\n    \"merge_threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308110000000000000094869452942e\"\n    },\n    \"number_of_points\": 900,\n    \"outliers_multiplier\": 4.618044848084256\n  },\n  \"_Trial__unresolved_config\": {\n    \"Gradient_LR\": 0.022717216966610707,\n    \"Gradient_Momentum\": 0.9671953048831726,\n    \"Gradient_Steps\": 150,\n    \"Threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308080000000000000094869452942e\"\n    },\n    \"merge_threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308110000000000000094869452942e\"\n    },\n    \"number_of_points\": 900,\n    \"outliers_multiplier\": 4.618044848084256\n  },\n  \"evaluated_params\": {\n    \"Gradient_LR\": 0.022717216966610707,\n    \"Gradient_Momentum\": 0.9671953048831726,\n    \"Gradient_Steps\": 150,\n    \"Threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308080000000000000094869452942e\"\n    },\n    \"merge_threshold\": {\n      \"_type\": \"CLOUDPICKLE_FALLBACK\",\n      \"value\": \"80059569000000000000008c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02693894898887945294284b038c013c944e4e4e4affffffff4affffffff4b007494624308110000000000000094869452942e\"\n    },\n    \"number_of_points\": 900,\n    \"outliers_multiplier\": 4.618044848084256\n  },\n  \"experiment_tag\": \"48_Gradient_LR=0.0227,Gradient_Momentum=0.9672,Gradient_Steps=150,Threshold=8,merge_threshold=17,number_of_points=900,outliers_multiplier=4.6180\",\n  \"location\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059546000000000000008c197261792e74756e652e6578706572696d656e742e747269616c948c095f4c6f636174696f6e9493942981947d94288c08686f73746e616d65944e8c03706964944e75622e\"\n  },\n  \"stopping_criterion\": {},\n  \"_setup_default_resource\": true,\n  \"_default_placement_group_factory\": \"800595aa000000000000008c237261792e74756e652e657865637574696f6e2e706c6163656d656e745f67726f757073948c15506c6163656d656e7447726f7570466163746f72799493942981947d94288c085f62756e646c6573945d947d948c0343505594473ff000000000000073618c155f686561645f62756e646c655f69735f656d70747994898c095f7374726174656779948c045041434b948c055f6172677394298c075f6b7761726773947d9475622e\",\n  \"placement_group_factory\": \"800595aa000000000000008c237261792e74756e652e657865637574696f6e2e706c6163656d656e745f67726f757073948c15506c6163656d656e7447726f7570466163746f72799493942981947d94288c085f62756e646c6573945d947d948c0343505594473ff000000000000073618c155f686561645f62756e646c655f69735f656d70747994898c095f7374726174656779948c045041434b948c055f6172677394298c075f6b7761726773947d9475622e\",\n  \"log_to_file\": [\n    null,\n    null\n  ],\n  \"max_failures\": 0,\n  \"_last_result\": {\n    \"score\": 0.8849348404152184,\n    \"time_this_iter_s\": 9.772911787033081,\n    \"done\": true,\n    \"training_iteration\": 1,\n    \"trial_id\": \"3c94bc07\",\n    \"date\": \"2025-01-23_02-41-07\",\n    \"timestamp\": 1737596467,\n    \"time_total_s\": 9.772911787033081,\n    \"pid\": 30798,\n    \"hostname\": \"MacBook-Pro.ds19.agh.edu.pl\",\n    \"node_ip\": \"127.0.0.1\",\n    \"config\": {\n      \"Gradient_LR\": 0.022717216966610707,\n      \"Gradient_Momentum\": 0.9671953048831726,\n      \"Gradient_Steps\": 150,\n      \"Threshold\": 8,\n      \"merge_threshold\": 17,\n      \"number_of_points\": 900,\n      \"outliers_multiplier\": 4.618044848084256\n    },\n    \"time_since_restore\": 9.772911787033081,\n    \"iterations_since_restore\": 1,\n    \"experiment_tag\": \"48_Gradient_LR=0.0227,Gradient_Momentum=0.9672,Gradient_Steps=150,Threshold=8,merge_threshold=17,number_of_points=900,outliers_multiplier=4.6180\"\n  },\n  \"_default_result_or_future\": null,\n  \"last_update_time\": 1737596467.400455,\n  \"metric_analysis\": {\n    \"score\": {\n      \"max\": 0.8849348404152184,\n      \"min\": 0.8849348404152184,\n      \"avg\": 0.8849348404152184,\n      \"last\": 0.8849348404152184,\n      \"last-5-avg\": 0.8849348404152184,\n      \"last-10-avg\": 0.8849348404152184\n    },\n    \"time_this_iter_s\": {\n      \"max\": 9.772911787033081,\n      \"min\": 9.772911787033081,\n      \"avg\": 9.772911787033081,\n      \"last\": 9.772911787033081,\n      \"last-5-avg\": 9.772911787033081,\n      \"last-10-avg\": 9.772911787033081\n    },\n    \"done\": {\n      \"max\": false,\n      \"min\": false,\n      \"avg\": false,\n      \"last\": false,\n      \"last-5-avg\": false,\n      \"last-10-avg\": false\n    },\n    \"training_iteration\": {\n      \"max\": 1,\n      \"min\": 1,\n      \"avg\": 1,\n      \"last\": 1,\n      \"last-5-avg\": 1,\n      \"last-10-avg\": 1\n    },\n    \"time_total_s\": {\n      \"max\": 9.772911787033081,\n      \"min\": 9.772911787033081,\n      \"avg\": 9.772911787033081,\n      \"last\": 9.772911787033081,\n      \"last-5-avg\": 9.772911787033081,\n      \"last-10-avg\": 9.772911787033081\n    },\n    \"time_since_restore\": {\n      \"max\": 9.772911787033081,\n      \"min\": 9.772911787033081,\n      \"avg\": 9.772911787033081,\n      \"last\": 9.772911787033081,\n      \"last-5-avg\": 9.772911787033081,\n      \"last-10-avg\": 9.772911787033081\n    },\n    \"iterations_since_restore\": {\n      \"max\": 1,\n      \"min\": 1,\n      \"avg\": 1,\n      \"last\": 1,\n      \"last-5-avg\": 1,\n      \"last-10-avg\": 1\n    }\n  },\n  \"n_steps\": [\n    5,\n    10\n  ],\n  \"metric_n_steps\": {\n    \"score\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059589000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452948c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02663894898887945294284b038c013c944e4e4e4affffffff4affffffff4b0074946243083694d5de6251ec3f9486945294612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059589000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452948c156e756d70792e636f72652e6d756c74696172726179948c067363616c61729493948c056e756d7079948c0564747970659493948c02663894898887945294284b038c013c944e4e4e4affffffff4affffffff4b0074946243083694d5de6251ec3f9486945294612e\"\n      }\n    },\n    \"time_this_iter_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944740238bbb18000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944740238bbb18000000612e\"\n      }\n    },\n    \"done\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059522000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529489612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059522000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529489612e\"\n      }\n    },\n    \"training_iteration\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b01612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b01612e\"\n      }\n    },\n    \"time_total_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944740238bbb18000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944740238bbb18000000612e\"\n      }\n    },\n    \"time_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944740238bbb18000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944740238bbb18000000612e\"\n      }\n    },\n    \"iterations_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b01612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b01612e\"\n      }\n    }\n  },\n  \"export_formats\": [],\n  \"status\": \"TERMINATED\",\n  \"start_time\": 1737596457.608143,\n  \"relative_logdir\": \"lambda_3c94bc07_48_Gradient_LR=0.0227,Gradient_Momentum=0.9672,Gradient_Steps=150,Threshold=8,merge_threshold=17,number_of_points=_2025-01-23_02-40-55\",\n  \"runner\": null,\n  \"last_debug\": 0,\n  \"error_filename\": null,\n  \"pickled_error_filename\": null,\n  \"trial_name_creator\": null,\n  \"trial_dirname_creator\": null,\n  \"custom_trial_name\": null,\n  \"custom_dirname\": null,\n  \"experiment_dir_name\": \"test\",\n  \"saving_to\": null,\n  \"checkpoint_config\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"800595ff000000000000008c0e7261792e6169722e636f6e666967948c10436865636b706f696e74436f6e6669679493942981947d94288c0b6e756d5f746f5f6b656570944b038c1a636865636b706f696e745f73636f72655f617474726962757465948c12747261696e696e675f697465726174696f6e948c16636865636b706f696e745f73636f72655f6f72646572948c036d6178948c14636865636b706f696e745f6672657175656e6379944b008c11636865636b706f696e745f61745f656e64944e8c1a5f636865636b706f696e745f6b6565705f616c6c5f72616e6b7394898c1f5f636865636b706f696e745f75706c6f61645f66726f6d5f776f726b657273948975622e\"\n  },\n  \"checkpoint_manager\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"800595d2020000000000008c257261792e74756e652e657865637574696f6e2e636865636b706f696e745f6d616e61676572948c125f436865636b706f696e744d616e616765729493942981947d94288c145f636865636b706f696e745f7374726174656779948c0e7261792e6169722e636f6e666967948c10436865636b706f696e74436f6e6669679493942981947d94288c0b6e756d5f746f5f6b656570944b038c1a636865636b706f696e745f73636f72655f617474726962757465948c12747261696e696e675f697465726174696f6e948c16636865636b706f696e745f73636f72655f6f72646572948c036d6178948c14636865636b706f696e745f6672657175656e6379944b008c11636865636b706f696e745f61745f656e64944e8c1a5f636865636b706f696e745f6b6565705f616c6c5f72616e6b7394898c1f5f636865636b706f696e745f75706c6f61645f66726f6d5f776f726b657273948975628c155f6c61746573745f636865636b706f696e745f6964944b008c1a5f746f705f7065727369737465645f636865636b706f696e7473945d948c1a5f626573745f7065727369737465645f636865636b706f696e74944e8c1c5f6c61746573745f7065727369737465645f636865636b706f696e74944e8c195f6c61746573745f6d656d6f72795f636865636b706f696e74948c247261792e6169722e5f696e7465726e616c2e636865636b706f696e745f6d616e61676572948c125f547261636b6564436865636b706f696e749493942981947d94288c0b6469725f6f725f64617461944e8c026964944affffffff8c0c73746f726167655f6d6f646594681a8c11436865636b706f696e7453746f726167659493944b01859452948c0472616e6b944b008c076d657472696373947d948c076e6f64655f6970944e8c155f6d6f76655f696e73746561645f6f665f636f7079948875628c185f636865636b706f696e74735f746f5f636c65616e5f7570948f9475622e\"\n  },\n  \"restore_path\": null,\n  \"restoring_from\": null,\n  \"num_failures\": 0,\n  \"num_restore_failures\": 0,\n  \"results\": \"80054e2e\",\n  \"best_result\": \"80054e2e\",\n  \"param_config\": \"80054e2e\",\n  \"extra_arg\": \"80054e2e\",\n  \"_state_json\": null,\n  \"_state_valid\": false,\n  \"_resources\": \"80054e2e\"\n}"
  ],
  "runner_data": {
    "_earliest_stopping_actor": 221327.947622333,
    "_actor_cleanup_timeout": 600,
    "_actor_force_cleanup_timeout": 10,
    "_reuse_actors": true,
    "_chdir_to_trial_dir": true,
    "_buffer_length": 1,
    "_buffer_min_time_s": 0.0,
    "_buffer_max_time_s": 100.0,
    "_max_pending_trials": 1,
    "_metric": "score",
    "_total_time": 1424.6177163124084,
    "_iteration": 1065,
    "_has_errored": false,
    "_fail_fast": false,
    "_print_trial_errors": true,
    "_server_port": null,
    "_cached_trial_decisions": {},
    "_queued_trial_decisions": {},
    "_should_stop_experiment": false,
    "_stopper": {
      "_type": "CLOUDPICKLE_FALLBACK",
      "value": "8005952c000000000000008c157261792e74756e652e73746f707065722e6e6f6f70948c0b4e6f6f7053746f707065729493942981942e"
    },
    "_start_time": 1737596363.8525598,
    "_last_checkpoint_time": -Infinity,
    "_session_str": "2025-01-23_02-39-23",
    "_checkpoint_period": "auto",
    "_trial_checkpoint_config": {
      "_type": "CLOUDPICKLE_FALLBACK",
      "value": "800595eb000000000000008c0e7261792e6169722e636f6e666967948c10436865636b706f696e74436f6e6669679493942981947d94288c0b6e756d5f746f5f6b656570944b038c1a636865636b706f696e745f73636f72655f617474726962757465944e8c16636865636b706f696e745f73636f72655f6f72646572948c036d6178948c14636865636b706f696e745f6672657175656e6379944b008c11636865636b706f696e745f61745f656e64944e8c1a5f636865636b706f696e745f6b6565705f616c6c5f72616e6b7394898c1f5f636865636b706f696e745f75706c6f61645f66726f6d5f776f726b657273948975622e"
    },
    "_resumed": false,
    "launch_web_server": false
  },
  "stats": {
    "start_time": 1737596363.8525598,
    "timestamp": -Infinity
  }
}