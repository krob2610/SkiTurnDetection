{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35f7b835",
   "metadata": {},
   "source": [
    "# 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803c3906",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# notebook init\n",
    "import os\n",
    "from notebook_init import init_notebook_path\n",
    "\n",
    "# Get project root path\n",
    "project_root = init_notebook_path()\n",
    "\n",
    "# Set environment variable for Ray workers to find the app module\n",
    "os.environ[\"PYTHONPATH\"] = f\"{project_root}:{os.environ.get('PYTHONPATH', '')}\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72bc1dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Imports\n",
    "from typing import Any, Sequence, List, Tuple, Dict\n",
    "import numpy as np\n",
    "from numpy.typing import NDArray\n",
    "import pandas as pd\n",
    "from app.workflow.utils import read_files_recursive\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41ed91b",
   "metadata": {},
   "source": [
    "# 2. Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9c5da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_data = read_files_recursive(path = os.path.join(project_root, \"data\", \"turns\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1717bba0",
   "metadata": {},
   "source": [
    "# 3. Data preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c3419a",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_data[0].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99bf04a6",
   "metadata": {},
   "source": [
    "# 4. Behavior filling & cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493e814f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_behavior(df_):\n",
    "    df = df_.copy()\n",
    "    s = df['Behavior'].where(df['Behavior'].isin(['left', 'right']))\n",
    "    df['Behavior'] = s.ffill()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2bc14b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply fill_behavior and drop Status column\n",
    "labeled_data_filled = [\n",
    "    fill_behavior(df)\n",
    "    .drop(columns=['Status'], errors='ignore')\n",
    "    for df in labeled_data\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455fd5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_data_filled[0].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c223fab",
   "metadata": {},
   "source": [
    "# 5. Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c55b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# names of vector like columns\n",
    "VECTOR_COLS: List[str] = [\n",
    "    \"Accelerometer\",\"Gyroscope\",\"Gravity\",\n",
    "    \"TotalAcceleration\",\"Orientation\",\"Magnetometer\",\n",
    "    \"GyroscopeUncalibrated\",\"MagnetometerUncalibrated\"\n",
    "]\n",
    "DIM = {\"Orientation\": 7}\n",
    "\n",
    "\n",
    "def to_vec(x: str) -> NDArray[np.float64]:\n",
    "        return np.asarray(x, dtype=float)\n",
    "\n",
    "\n",
    "def expand_vec_col(df: pd.DataFrame, col: str) -> pd.DataFrame:\n",
    "    \"\"\"Split vector column into x y z and norm.\"\"\"\n",
    "    dim = DIM.get(col, 3)\n",
    "    vecs = df[col].apply(to_vec)\n",
    "    arr = np.vstack(vecs.apply(lambda v: np.pad(v[:dim], (0, max(0, dim - len(v))), constant_values=np.nan)))\n",
    "    out = pd.DataFrame(arr, columns=[f\"{col}_{i}\" for i in range(dim)], index=df.index)\n",
    "    out[f\"{col}_norm\"] = np.linalg.norm(arr, axis=1)\n",
    "    return out\n",
    "\n",
    "def build_feature_frame(df: pd.DataFrame) -> pd.DataFrame:\n",
    "\n",
    "    work = df.copy()\n",
    "    work = work[work[\"Behavior\"].isin([\"left\", \"right\"])]\n",
    "\n",
    "    parts= []\n",
    "    for c in VECTOR_COLS:\n",
    "        if c in work.columns:\n",
    "            parts.append(expand_vec_col(work, c))\n",
    "    if not parts:\n",
    "        raise ValueError(\"No known sensor columns found\")\n",
    "\n",
    "    X = pd.concat(parts, axis=1)\n",
    "    X[\"Behavior\"] = work[\"Behavior\"].astype(\"category\")\n",
    "    return X\n",
    "\n",
    "# list of feature frames aligned with labeled_data\n",
    "data_with_features = [build_feature_frame(df) for df in labeled_data_filled]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc7cac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_with_features[0].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f6602a",
   "metadata": {},
   "source": [
    "# 6. Train/Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3baccd4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_frames(\n",
    "    feature_frames: Sequence[pd.DataFrame],\n",
    "    test_size: float = 0.2,\n",
    "    random_state: int = 42,\n",
    ") -> Tuple[List[pd.DataFrame], List[pd.DataFrame]]:\n",
    "    rng = np.random.default_rng(random_state)\n",
    "    idx = np.arange(len(feature_frames))\n",
    "    rng.shuffle(idx)\n",
    "    cut = int(round(len(idx) * (1 - test_size)))\n",
    "    train_idx = idx[:cut]\n",
    "    test_idx = idx[cut:]\n",
    "    train_frames = [feature_frames[i] for i in train_idx]\n",
    "    test_frames = [feature_frames[i] for i in test_idx]\n",
    "    return train_frames, test_frames\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8d96a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split\n",
    "train_frames, test_frames = split_frames(data_with_features, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52d80c3",
   "metadata": {},
   "source": [
    "# 7. Modeling & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c8e035",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FakePredictor:\n",
    "    def __init__(self, random_state: int = 42):\n",
    "        self.rng = np.random.default_rng(random_state)\n",
    "\n",
    "    def fit(self, X: List[pd.DataFrame], y: List[pd.DataFrame]) -> \"FakePredictor\":\n",
    "        return self \n",
    "\n",
    "    def predict(self, n: int) -> np.ndarray:\n",
    "        return self.rng.integers(0, 2, size=n)\n",
    "\n",
    "def make_dataset_single(df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.Series]:\n",
    "    frame = df.copy()\n",
    "    X = frame.drop(columns=[\"Behavior\"])\n",
    "    y = frame[\"Behavior\"].map({\"left\": 0, \"right\": 1}).astype(\"Int64\")\n",
    "    mask = y.isin([0, 1])\n",
    "    X = X.loc[mask]\n",
    "    y = y.loc[mask].astype(int)\n",
    "    return X, y\n",
    "\n",
    "def train_fake(\n",
    "    train_frames: Sequence[pd.DataFrame],\n",
    "    random_state: int = 42,\n",
    ") -> FakePredictor:\n",
    "    # Split into feature and label lists per frame\n",
    "    X_list, y_list = zip(*(make_dataset_single(frame) for frame in train_frames))\n",
    "    # Fit on the lists of DataFrames/Series\n",
    "    clf = FakePredictor(random_state=random_state).fit(list(X_list), list(y_list))\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315113a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_fake(\n",
    "    clf: FakePredictor,\n",
    "    test_frames: Sequence[pd.DataFrame],\n",
    ") -> Dict[str, Any]:\n",
    "    total_cm = np.zeros((2, 2), dtype=int)\n",
    "    total_correct = 0\n",
    "    total_n = 0\n",
    "    per_frame = []\n",
    "\n",
    "    for i, f in enumerate(test_frames):\n",
    "        X, y = make_dataset_single(f)\n",
    "        n = len(y)\n",
    "        if n == 0:\n",
    "            per_frame.append({\"frame_index\": i, \"skipped\": True, \"reason\": \"no labels\"})\n",
    "            continue\n",
    "        y_pred = clf.predict(n)\n",
    "        acc = float(accuracy_score(y, y_pred))\n",
    "        cm = confusion_matrix(y, y_pred, labels=[0, 1])\n",
    "        total_cm += cm\n",
    "        total_correct += int((y_pred == y).sum())\n",
    "        total_n += n\n",
    "        per_frame.append({\"frame_index\": i, \"n\": n, \"accuracy\": acc})\n",
    "\n",
    "    macro_acc = float(np.mean([x[\"accuracy\"] for x in per_frame if not x.get(\"skipped\")])) if any(not x.get(\"skipped\") for x in per_frame) else float(\"nan\")\n",
    "    weighted_acc = float(total_correct / total_n) if total_n > 0 else float(\"nan\")\n",
    "\n",
    "    return {\n",
    "        \"macro_accuracy\": macro_acc,\n",
    "        \"weighted_accuracy\": weighted_acc,\n",
    "        \"confusion_matrix_sum\": total_cm,\n",
    "        \"tested_samples\": total_n,\n",
    "        \"per_frame\": per_frame,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e2de19",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = train_fake(train_frames, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3742d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = evaluate_fake(clf, test_frames)\n",
    "print(\"Macro accuracy:\", summary[\"macro_accuracy\"])\n",
    "print(\"Weighted accuracy:\", summary[\"weighted_accuracy\"])\n",
    "print(\"Summed confusion matrix [left=0 right=1]:\")\n",
    "print(summary[\"confusion_matrix_sum\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ski_env_git",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
